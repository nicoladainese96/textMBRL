{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a model of the environment\n",
    "\n",
    "What do we require of a such a model? <br>\n",
    "- Ability to **model stochasticity**, either through transition sampling or latent variable modelling  <br>\n",
    "- Ability to be **conditioned on text**\n",
    "\n",
    "What extra characteristics should we strive for?  <br>\n",
    "- **Learn a text-invariant representation**: we want to identify through the usage of the text which monster is the target, which is the distractor, which is the good item and which is the bad one. Once this representation is obtained, the complexity of learning the dynamic of the environment, a policy and a value function would be the same of doing that with fixed configuration of monsters, items and rules.\n",
    "- **Train it with online experience** to make it data efficient. It might be feasible to learn from other agents' experience in some cases, but models learned together with the agent itself always tend to be more accurate on the important parts of the state-action space (usual problem of distributional shift in RL).\n",
    "\n",
    "What tests can we do about it? <br>\n",
    "- Learning a text-invariant latent representation: are we able to train a classifier to identify in the map the locations of the target, distractor, yes item and no item? We should train it while keping the model fixed. <br>\n",
    "- Good stochastic model: are we able to model or sample all possible stochastic outcomes of a transition? How could we measure this?\n",
    "\n",
    "## Text-conditioned VQVAE\n",
    "One idea would be to have a text-conditioned VQVAE and then follow the training procedure of https://arxiv.org/abs/2106.04615 (train a transition model too). \n",
    "The problem with this is that it currently requires a dataset of experience from which to learn the following things: <br>\n",
    "- how to encode the most significant part of the state space\n",
    "- what is the distribution of the transitions in the environment for all relevant (state,action) pairs\n",
    "- what is the reward associated with the various transitions\n",
    "- what is a good prior for the agent's policy\n",
    "\n",
    "TODO:\n",
    "- Try to think of an online training procedure for the text-conditioned VQM - or use an initial random collection scheme, pre-train and then start the online training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
