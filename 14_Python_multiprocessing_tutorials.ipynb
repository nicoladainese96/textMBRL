{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "with Pool(5) as p:\n",
    "    print(p.map(f, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello bob\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def f(name):\n",
    "    print('hello', name)\n",
    "\n",
    "p = Process(target=f, args=('bob',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main line\n",
      "module name: __main__\n",
      "parent process: 11316\n",
      "process id: 50564\n",
      "function f\n",
      "module name: __main__\n",
      "parent process: 50564\n",
      "process id: 50612\n",
      "hello bob\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def info(title):\n",
    "    print(title)\n",
    "    print('module name:', __name__)\n",
    "    print('parent process:', os.getppid())\n",
    "    print('process id:', os.getpid())\n",
    "\n",
    "def f(name):\n",
    "    info('function f')\n",
    "    print('hello', name)\n",
    "\n",
    "\n",
    "info('main line')\n",
    "p = Process(target=f, args=('bob',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the notebook before executing next cell, otherwise you'll get an error 'context already set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def foo(q):\n",
    "    q.put('hello')\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#mp.set_start_method('spawn')\n",
    "mp.set_start_method('fork')\n",
    "q = mp.Queue()\n",
    "p = mp.Process(target=foo, args=(q,))\n",
    "p.start()\n",
    "print(q.get())\n",
    "p.join()\n",
    "\n",
    "# this is actually never ending !? works with 'fork', but not with 'spawn'\n",
    "# problem of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "ctx = mp.get_context('fork')\n",
    "q = ctx.Queue()\n",
    "p = ctx.Process(target=foo, args=(q,))\n",
    "p.start()\n",
    "print(q.get())\n",
    "p.join()\n",
    "\n",
    "# this is actually never ending !? works with 'fork', but not with 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def f(q):\n",
    "    q.put([42, None, 'hello'])\n",
    "\n",
    "##if __name__ == '__main__':\n",
    "q = Queue()\n",
    "p = Process(target=f, args=(q,))\n",
    "p.start()\n",
    "print(q.get())    # prints \"[42, None, 'hello']\"\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def f(conn):\n",
    "    conn.send([42, None, 'hello'])\n",
    "    conn.close()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "parent_conn, child_conn = Pipe()\n",
    "p = Process(target=f, args=(child_conn,))\n",
    "p.start()\n",
    "print(parent_conn.recv())   # prints \"[42, None, 'hello']\"\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world 1\n",
      "hello world 0\n",
      "hello world 3\n",
      "hello world 2\n",
      "hello world 4\n",
      "hello world 5\n",
      "hello world 6\n",
      "hello world 8\n",
      "hello world 7\n",
      "hello world 9\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Lock\n",
    "\n",
    "def f(l, i):\n",
    "    l.acquire()\n",
    "    try:\n",
    "        print('hello world', i)\n",
    "    finally:\n",
    "        l.release()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "lock = Lock()\n",
    "\n",
    "for num in range(10):\n",
    "    Process(target=f, args=(lock, num)).start()\n",
    "    \n",
    "# they're already/still mixed... probably works in a terminal -> yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Server process managers are more flexible than using shared memory objects because they can be made to support arbitrary object types. Also, a single manager can be shared by processes on different computers over a network. They are, however, slower than using shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bear in mind that a process that has put items in a queue will wait before terminating until all the buffered items are fed by the “feeder” thread to the underlying pipe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that whenever you use a queue you need to make sure that all items which have been put on the queue will eventually be removed before the process is joined. Otherwise you cannot be sure that processes which have put items on the queue will terminate. Remember also that non-daemonic processes will be joined automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Unix using the fork start method, a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
