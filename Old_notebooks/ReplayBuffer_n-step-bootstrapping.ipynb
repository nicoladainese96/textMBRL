{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer and n-steps bootstrapping\n",
    "\n",
    "Goal of this notebook is to test my old code for (at most) n-step value bootstrapping with (possibly) multiple episodes per sample in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For precisely n-steps and no terminal states, we want to compute this formula\n",
    "$$V^{(n)}(t) = \\sum_{k=0}^{n-1} \\gamma^{k} r_{t+k+1} + \\gamma^n V(s_{t+n})$$\n",
    "Where the notation for a state transition is $(s_t, a_t, r_{t+1}, s_{t+1})$.\n",
    "\n",
    "For n=3 it looks like this:\n",
    "$$V^{(3)}(t) = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 V(s_{t+3})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_V_trg_v0(n_steps, discount, rewards, done, bootstrap, states, value_net, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute m-steps value target, with m = min(n, steps-to-episode-end).\n",
    "    Formula (for precisely n-steps):\n",
    "        V^{(n)}(t) = \\sum_{k=0}^{n-1} gamma^k r_{t+k+1} + gamma^n * V(s_{t+n})\n",
    "    \"\"\"\n",
    "    n_step_rewards, episode_mask, n_steps_mask_b = compute_n_step_rewards(rewards, done, n_steps, discount)\n",
    "    done[bootstrap] = False \n",
    "    trg_states = states[:,1:]\n",
    "    new_states, Gamma_V, done = compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount)\n",
    "\n",
    "    new_states = torch.tensor(new_states).float().to(device).reshape((-1,)+states.shape[2:])\n",
    "    done = torch.LongTensor(done.astype(int)).to(device).reshape(-1)\n",
    "    n_step_rewards = torch.tensor(n_step_rewards).float().to(device).reshape(-1)\n",
    "    Gamma_V = torch.tensor(Gamma_V).float().to(device).reshape(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        V_pred = value_net(new_states).squeeze()\n",
    "        V_trg = (1-done)*Gamma_V*V_pred + n_step_rewards\n",
    "        V_trg = V_trg.squeeze()\n",
    "    return V_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_rewards(rewards, done, n_steps, discount):\n",
    "    \"\"\"\n",
    "    Computes n-steps discounted reward. \n",
    "    Note: the rewards considered are AT MOST n, but can be less for the last n-1 elements.\n",
    "    \"\"\"\n",
    "\n",
    "    B = done.shape[0]\n",
    "    T = done.shape[1]\n",
    "\n",
    "    # Compute episode mask (i-th row contains 1 if col j is in the same episode of col i, 0 otherwise)\n",
    "    episode_mask = [[] for _ in range(B)]\n",
    "    last = [-1 for _ in range(B)]\n",
    "    xs, ys = np.nonzero(done)\n",
    "\n",
    "    # Add done at the end of every batch to avoid exceptions -> not used in real target computations\n",
    "    xs = np.concatenate([xs, np.arange(B)])\n",
    "    ys = np.concatenate([ys, np.full(B, T-1)])\n",
    "    for x, y in zip(xs, ys):\n",
    "        m = [1 if (i > last[x] and i <= y) else 0 for i in range(T)]\n",
    "        for _ in range(y-last[x]):\n",
    "            episode_mask[x].append(m)\n",
    "        last[x] = y\n",
    "    episode_mask = np.array(episode_mask)\n",
    "\n",
    "    # Compute n-steps mask and repeat it B times\n",
    "    n_steps_mask = []\n",
    "    for i in range(T):\n",
    "        m = [1 if (j>=i and j<i+n_steps) else 0 for j in range(T)]\n",
    "        n_steps_mask.append(m)\n",
    "    n_steps_mask = np.array(n_steps_mask)\n",
    "    n_steps_mask_b = np.repeat(n_steps_mask[np.newaxis,...] , B, axis=0)\n",
    "\n",
    "    # Broadcast rewards to use multiplicative masks\n",
    "    rewards_repeated = np.repeat(rewards[:,np.newaxis,:], T, axis=1)\n",
    "\n",
    "    # Exponential discount factor\n",
    "    Gamma = np.array([discount**i for i in range(T)]).reshape(1,-1)\n",
    "    n_steps_r = (Gamma*rewards_repeated*episode_mask*n_steps_mask_b).sum(axis=2)/Gamma\n",
    "    return n_steps_r, episode_mask, n_steps_mask_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount):\n",
    "    \"\"\"\n",
    "    Computes n-steps target states (to be used by the critic as target values together with the\n",
    "    n-steps discounted reward). For last n-1 elements the target state is the last one available.\n",
    "    Adjusts also the `done` mask used for disabling the bootstrapping in the case of terminal states\n",
    "    and returns Gamma_V, that are the discount factors for the target state-values, since they are \n",
    "    n-steps away (except for the last n-1 states, whose discount is adjusted accordingly).\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    new_states, Gamma_V, done: arrays with first dimension = len(states)-1\n",
    "    \"\"\"\n",
    "\n",
    "    B = done.shape[0]\n",
    "    T = done.shape[1]\n",
    "    V_mask = episode_mask*n_steps_mask_b\n",
    "    b, x, y = np.nonzero(V_mask)\n",
    "    V_trg_index = [[] for _ in range(B)]\n",
    "    for b_i in range(B):\n",
    "        valid_x = (b==b_i)\n",
    "        for i in range(T):\n",
    "            matching_x = (x==i)\n",
    "            V_trg_index[b_i].append(y[valid_x*matching_x][-1])\n",
    "    V_trg_index = np.array(V_trg_index)\n",
    "\n",
    "    cols = np.array([], dtype=np.int)\n",
    "    rows = np.array([], dtype=np.int)\n",
    "    for i, v in enumerate(V_trg_index):\n",
    "        cols = np.concatenate([cols, v], axis=0)\n",
    "        row = np.full(V_trg_index.shape[1], i)\n",
    "        rows = np.concatenate([rows, row], axis=0)\n",
    "    ###\n",
    "    new_states = trg_states[rows, cols].reshape(trg_states.shape)\n",
    "    ###\n",
    "    pw = V_trg_index - np.arange(V_trg_index.shape[1]) + 1\n",
    "    Gamma_V = discount**pw\n",
    "    shifted_done = done[rows, cols].reshape(done.shape)\n",
    "    return new_states, Gamma_V, shifted_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.9\n",
    "n_steps = 3 # number of rewards to sum-up at most before adding the discounted value of the next state\n",
    "B = 2 # batch_size\n",
    "T = 10 # trajectory length for each sample in the batch\n",
    "\n",
    "rewards = np.array([\n",
    "    [0,1,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,1,0,0,0,1,0,0]\n",
    "], dtype=np.float)\n",
    "\n",
    "done = np.array([\n",
    "    [0,1,0,0,0,0,1,0,0,1],\n",
    "    [0,0,0,1,0,0,0,1,0,1]\n",
    "], dtype=np.bool)\n",
    "\n",
    "# Old notation that I was using\n",
    "bootstrap = np.array([\n",
    "    [0,0,0,0,0,0,0,0,0,1],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "], dtype=np.bool)\n",
    "# In our current notation we don't have a bootstrap variable and our done can be obtained by done[boostrap] = 0\n",
    "\n",
    "# Let's say we have a binary state and the value of the state is 0.5 x state (so either 0 or 0.5)\n",
    "states = np.zeros((B,T+1,1), dtype=np.float) # states are 1 step longer than te rest of the signals\n",
    "states[0,-1,0] = 1. # let's just keep it simple and check the value bootstrapping on a single state\n",
    "\n",
    "expected_n_step_rewards = np.array([\n",
    "    [0.9,1,0,0,0.81,0.9,1,0,0,0],\n",
    "    [0,0.81,0.9,1,0,0.81,0.9,1,0,0]\n",
    "], dtype=np.float)\n",
    "\n",
    "\n",
    "expected_v_trg = torch.tensor([\n",
    "    [0.9,1,0,0,0.81,0.9,1,0.3645,0.405,0.45],\n",
    "    [0,0.81,0.9,1,0,0.81,0.9,1,0,0]\n",
    "]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5*x\n",
    "    \n",
    "value_net = ValueNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "n_step_rewards, episode_mask, n_steps_mask_b = compute_n_step_rewards(rewards, done, n_steps, discount)\n",
    "assert np.allclose(n_step_rewards, expected_n_step_rewards), \"n-step-rewards do not match the expected values\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "       [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps_mask_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_states = states[:,1:]\n",
    "new_states, Gamma_V, shifted_done = compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_states # (at most) n_step away target state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81 , 0.9  , 0.729, 0.729, 0.729, 0.81 , 0.9  , 0.729, 0.81 ,\n",
       "        0.9  ],\n",
       "       [0.729, 0.729, 0.81 , 0.9  , 0.729, 0.729, 0.81 , 0.9  , 0.81 ,\n",
       "        0.9  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gamma_V # always discounted at least of a factor gamma, up to gamma**n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "         True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_done # whether after (at most) n_steps a terminal state has been reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target values (reshaped): \n",
      " tensor([[0.9000, 1.0000, 0.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.3645, 0.4050,\n",
      "         0.4500],\n",
      "        [0.0000, 0.8100, 0.9000, 1.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "Expected: \n",
      " tensor([[0.9000, 1.0000, 0.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.3645, 0.4050,\n",
      "         0.4500],\n",
      "        [0.0000, 0.8100, 0.9000, 1.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "v_trg = compute_n_step_V_trg_v0(n_steps, discount, rewards, done, bootstrap, states, value_net, device=\"cpu\")\n",
    "print(\"Target values (reshaped): \\n\", v_trg.reshape(B,T))\n",
    "print(\"Expected: \\n\", expected_v_trg)\n",
    "assert torch.allclose(v_trg.reshape(B,T), expected_v_trg), \"Wrong vtarget values\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing bootstrap signal\n",
    "\n",
    "How it was used: we had as a default done=True at the end of each trajectory, even though it might have just been truncated while the episode was still going on. The done=True helped to signal that we should not take more steps than to that timestep, but then needs to be turned off so that we know that the final step needs bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_done = np.array([\n",
    "    [0,1,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,1,0,0,0,1,0,0]\n",
    "], dtype=np.bool)\n",
    "\n",
    "# get the old done as\n",
    "old_done = new_done.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False, False, False,  True, False, False,\n",
       "         True],\n",
       "       [False, False, False,  True, False, False, False,  True, False,\n",
       "         True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_done[:,-1] = True\n",
    "old_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False, False, False,  True, False, False,\n",
       "        False],\n",
       "       [False, False, False,  True, False, False, False,  True, False,\n",
       "        False]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_done # does not change the original variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_V_trg(n_steps, discount, rewards, done, states, value_net, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute m-steps value target, with m = min(n_steps, steps-to-episode-end).\n",
    "    Formula (for precisely n-steps):\n",
    "        V^{(n)}(t) = \\sum_{k=}^{n-1} gamma^k r_{t+k+1} + gamma^n * V(s_{t+n})\n",
    "        \n",
    "    Input\n",
    "    -----\n",
    "    n_steps: int\n",
    "        How many steps in the future to consider before bootstrapping while computing the value target\n",
    "    discount: float in (0,1)\n",
    "        Discount factor of the MDP\n",
    "    rewards: np.array of shape (B,T), type float\n",
    "    done: np.array of shape (B,T), type bool\n",
    "    states: np.array of shape (B,T,...)\n",
    "    value_net: instance of nn.Module\n",
    "        outputs values of shape (B*T,) given states reshaped as (B*T,...)\n",
    "    \n",
    "    \"\"\"\n",
    "    done_plus_ending = done.copy()\n",
    "    done_plus_ending[:,-1] = True\n",
    "    n_step_rewards, episode_mask, n_steps_mask_b = compute_n_step_rewards(rewards, done_plus_ending, n_steps, discount)\n",
    "    ###\n",
    "    trg_states = states[:,1:]\n",
    "    ###\n",
    "    new_states, Gamma_V, done = compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount)\n",
    "\n",
    "    ###\n",
    "    new_states = torch.tensor(new_states).float().to(device).reshape((-1,)+states.shape[2:])\n",
    "    ###\n",
    "    done = torch.LongTensor(done.astype(int)).to(device).reshape(-1)\n",
    "    n_step_rewards = torch.tensor(n_step_rewards).float().to(device).reshape(-1)\n",
    "    Gamma_V = torch.tensor(Gamma_V).float().to(device).reshape(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        V_pred = value_net(new_states).squeeze()\n",
    "        V_trg = (1-done)*Gamma_V*V_pred + n_step_rewards\n",
    "        V_trg = V_trg.squeeze()\n",
    "    return V_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target values (reshaped): \n",
      " tensor([[0.9000, 1.0000, 0.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.3645, 0.4050,\n",
      "         0.4500],\n",
      "        [0.0000, 0.8100, 0.9000, 1.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "Expected: \n",
      " tensor([[0.9000, 1.0000, 0.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.3645, 0.4050,\n",
      "         0.4500],\n",
      "        [0.0000, 0.8100, 0.9000, 1.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "v_trg = compute_n_step_V_trg(n_steps, discount, rewards, new_done, states, value_net, device=\"cpu\")\n",
    "print(\"Target values (reshaped): \\n\", v_trg.reshape(B,T))\n",
    "print(\"Expected: \\n\", expected_v_trg)\n",
    "assert torch.allclose(v_trg.reshape(B,T), expected_v_trg), \"Wrong vtarget values\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with a state that is a dictionary of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_V_trg(n_steps, discount, rewards, done, states, value_net, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute m-steps value target, with m = min(n_steps, steps-to-episode-end).\n",
    "    Formula (for precisely n-steps):\n",
    "        V^{(n)}(t) = \\sum_{k=}^{n-1} gamma^k r_{t+k+1} + gamma^n * V(s_{t+n})\n",
    "        \n",
    "    Input\n",
    "    -----\n",
    "    n_steps: int\n",
    "        How many steps in the future to consider before bootstrapping while computing the value target\n",
    "    discount: float in (0,1)\n",
    "        Discount factor of the MDP\n",
    "    rewards: np.array of shape (B,T), type float\n",
    "    done: np.array of shape (B,T), type bool\n",
    "    states: dictionary of tensors all of shape (B,T,...)\n",
    "    value_net: instance of nn.Module\n",
    "        outputs values of shape (B*T,) given states reshaped as (B*T,...)\n",
    "    \n",
    "    \"\"\"\n",
    "    done_plus_ending = done.copy()\n",
    "    done_plus_ending[:,-1] = True\n",
    "    n_step_rewards, episode_mask, n_steps_mask_b = compute_n_step_rewards(rewards, done_plus_ending, n_steps, discount)\n",
    "    trg_states = {}\n",
    "    for k in states.keys():\n",
    "        trg_states[k] = states[k][:,1:]\n",
    "    new_states, Gamma_V, done = compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount)\n",
    "\n",
    "    new_states_reshaped = {}\n",
    "    for k in new_states.keys():\n",
    "        new_states_reshaped[k] = new_states[k].reshape((-1,)+new_states[k].shape[2:])\n",
    "    done = torch.LongTensor(done.astype(int)).to(device).reshape(-1)\n",
    "    n_step_rewards = torch.tensor(n_step_rewards).float().to(device).reshape(-1)\n",
    "    Gamma_V = torch.tensor(Gamma_V).float().to(device).reshape(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        V_pred = value_net(new_states_reshaped).squeeze()\n",
    "        V_trg = (1-done)*Gamma_V*V_pred + n_step_rewards\n",
    "        V_trg = V_trg.squeeze()\n",
    "    return V_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount):\n",
    "    \"\"\"\n",
    "    Computes n-steps target states (to be used by the critic as target values together with the\n",
    "    n-steps discounted reward). For last n-1 elements the target state is the last one available.\n",
    "    Adjusts also the `done` mask used for disabling the bootstrapping in the case of terminal states\n",
    "    and returns Gamma_V, that are the discount factors for the target state-values, since they are \n",
    "    n-steps away (except for the last n-1 states, whose discount is adjusted accordingly).\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    new_states, Gamma_V, done: arrays with first dimension = len(states)-1\n",
    "    \"\"\"\n",
    "\n",
    "    B = done.shape[0]\n",
    "    T = done.shape[1]\n",
    "    V_mask = episode_mask*n_steps_mask_b\n",
    "    b, x, y = np.nonzero(V_mask)\n",
    "    V_trg_index = [[] for _ in range(B)]\n",
    "    for b_i in range(B):\n",
    "        valid_x = (b==b_i)\n",
    "        for i in range(T):\n",
    "            matching_x = (x==i)\n",
    "            V_trg_index[b_i].append(y[valid_x*matching_x][-1])\n",
    "    V_trg_index = np.array(V_trg_index)\n",
    "\n",
    "    cols = np.array([], dtype=np.int)\n",
    "    rows = np.array([], dtype=np.int)\n",
    "    for i, v in enumerate(V_trg_index):\n",
    "        cols = np.concatenate([cols, v], axis=0)\n",
    "        row = np.full(V_trg_index.shape[1], i)\n",
    "        rows = np.concatenate([rows, row], axis=0)\n",
    "    \n",
    "    new_states = {}\n",
    "    for k in trg_states.keys(): \n",
    "        new_states[k] = trg_states[k][rows, cols].reshape(trg_states[k].shape)\n",
    "\n",
    "    pw = V_trg_index - np.arange(V_trg_index.shape[1]) + 1\n",
    "    Gamma_V = discount**pw\n",
    "    shifted_done = done[rows, cols].reshape(done.shape)\n",
    "    return new_states, Gamma_V, shifted_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame': tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming all entries are tensors of shape (B,T,...)\n",
    "frames = {\"frame\":torch.tensor(states).float()} # pay attention here to the format in float\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetFrames(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5*x[\"frame\"]\n",
    "    \n",
    "value_net = ValueNetFrames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target values (reshaped): \n",
      " tensor([[0.9000, 1.0000, 0.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.3645, 0.4050,\n",
      "         0.4500],\n",
      "        [0.0000, 0.8100, 0.9000, 1.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "Expected: \n",
      " tensor([[0.9000, 1.0000, 0.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.3645, 0.4050,\n",
      "         0.4500],\n",
      "        [0.0000, 0.8100, 0.9000, 1.0000, 0.0000, 0.8100, 0.9000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "v_trg = compute_n_step_V_trg(n_steps, discount, rewards, new_done, frames, value_net, device=\"cpu\")\n",
    "print(\"Target values (reshaped): \\n\", v_trg.reshape(B,T))\n",
    "print(\"Expected: \\n\", expected_v_trg)\n",
    "assert torch.allclose(v_trg.reshape(B,T), expected_v_trg), \"Wrong vtarget values\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to integrate this with the replay buffer\n",
    "\n",
    "1. Make sure that everything except frames is a numpy array\n",
    "2. Use functions above instead of get_cumulative_reward and use them during get_batch instead of batch_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replay buffer stuff ###\n",
    "def get_cumulative_rewards(rewards, discount, dones):\n",
    "    cum_disc_rewards = []\n",
    "    cum_r = 0\n",
    "    for i,r in enumerate(reversed(rewards)):\n",
    "        not_done = 1 - dones[-(i+1)]\n",
    "        cum_r = not_done*discount*cum_r + r\n",
    "        cum_disc_rewards.append (cum_r)\n",
    "    cum_disc_rewards = torch.tensor(cum_disc_rewards[::-1])\n",
    "    return cum_disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nStepsReplayBuffer:\n",
    "    def __init__(self, mem_size, discount):\n",
    "        self.mem_size = mem_size\n",
    "        self.discount = discount\n",
    "        self.frame_buffer = []\n",
    "        self.reward_buffer = []\n",
    "        self.done_buffer = []\n",
    "        \n",
    "    def store_episode(self, frame_lst, reward_lst, done_lst):\n",
    "        frames, rewards, done = self.batch_episode(frame_lst, reward_lst, done_lst)\n",
    "        self.frame_buffer.append(frames)\n",
    "        self.reward_buffer.append(rewards)\n",
    "        self.done_buffer.append(done)\n",
    "        if len(self.frame_buffer) > self.mem_size:\n",
    "            self.frame_buffer.pop(0)\n",
    "            self.reward_buffer.pop(0)\n",
    "            self.done_buffer.pop(0)\n",
    "            \n",
    "    def batch_episode(self, frame_lst, reward_lst, done_lst):\n",
    "        \"\"\"\n",
    "        Unifies the time dimension fo the data and adds a batch dimension of 1 in front\n",
    "        \"\"\"\n",
    "        episode_len = len(reward_lst)\n",
    "        frames = {}\n",
    "        for k in frame_lst[0].keys():\n",
    "            k_value_lst = []\n",
    "            for b in range(episode_len):\n",
    "                k_value_lst.append(frame_lst[b][k])\n",
    "            k_value_lst = torch.cat(k_value_lst, axis=0)\n",
    "            frames[k] = k_value_lst.unsqueeze(0) # add batch size dimension in front\n",
    "            \n",
    "        rewards = np.array(reward_lst, dtype=np.float).reshape(1,-1)  # add batch size dimension in front\n",
    "        done = np.array(done_lst, dtype=np.bool).reshape(1,-1)  # add batch size dimension in front\n",
    "        \n",
    "        return frames, rewards, done\n",
    "    \n",
    "    def get_batch(self, batch_size, n_steps, discount, target_net, device=\"cpu\"):\n",
    "        # Decide which indexes to sample\n",
    "        id_range = len(self.frame_buffer)\n",
    "        assert id_range >= batch_size, \"Not enough samples stored to get this batch size\"\n",
    "        sampled_ids = np.random.choice(id_range, size=batch_size, replace=False)\n",
    "        \n",
    "        # Sample frames, rewards and done\n",
    "        sampled_rewards = np.array([self.reward_buffer[i] for i in sampled_ids])\n",
    "        sampled_done = np.array([self.done_buffer[i] for i in sampled_ids])\n",
    "        # batch together frames \n",
    "        sampled_frames = {}\n",
    "        for k in self.frame_buffer[0].keys():\n",
    "            key_values = torch.cat([self.frame_buffer[i][k] for i in sampled_ids], axis=0)\n",
    "            sampled_frames[k] = key_values\n",
    "            \n",
    "        # sampled_targets of shape (B*T,)\n",
    "        sampled_targets = self.compute_n_step_V_trg(n_steps, discount, sampled_rewards, sampled_done, \n",
    "                                                    sampled_frames, target_net, device)\n",
    "        # Flatten also the sampled_frames\n",
    "        reshaped_frames = {}\n",
    "        for k in sampled_frames.keys():\n",
    "            shape = sampled_frames[k].shape\n",
    "            reshaped_frames[k] = sampled_frames[k].reshape(-1,*shape[2:])\n",
    "\n",
    "        return reshaped_frames, sampled_targets\n",
    "    \n",
    "    def compute_n_step_V_trg(self, n_steps, discount, rewards, done, states, value_net, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Compute m-steps value target, with m = min(n_steps, steps-to-episode-end).\n",
    "        Formula (for precisely n-steps):\n",
    "            V^{(n)}(t) = \\sum_{k=}^{n-1} gamma^k r_{t+k+1} + gamma^n * V(s_{t+n})\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        n_steps: int\n",
    "            How many steps in the future to consider before bootstrapping while computing the value target\n",
    "        discount: float in (0,1)\n",
    "            Discount factor of the MDP\n",
    "        rewards: np.array of shape (B,T), type float\n",
    "        done: np.array of shape (B,T), type bool\n",
    "        states: dictionary of tensors all of shape (B,T,...)\n",
    "        value_net: instance of nn.Module\n",
    "            outputs values of shape (B*T,) given states reshaped as (B*T,...)\n",
    "\n",
    "        \"\"\"\n",
    "        done_plus_ending = done.copy()\n",
    "        done_plus_ending[:,-1] = True\n",
    "        n_step_rewards, episode_mask, n_steps_mask_b = compute_n_step_rewards(rewards, done_plus_ending, n_steps, discount)\n",
    "        trg_states = {}\n",
    "        for k in states.keys():\n",
    "            trg_states[k] = states[k][:,1:]\n",
    "        new_states, Gamma_V, done = compute_n_step_states(trg_states, done, episode_mask, n_steps_mask_b, discount)\n",
    "\n",
    "        new_states_reshaped = {}\n",
    "        for k in new_states.keys():\n",
    "            new_states_reshaped[k] = new_states[k].reshape((-1,)+new_states[k].shape[2:])\n",
    "        done = torch.LongTensor(done.astype(int)).to(device).reshape(-1)\n",
    "        n_step_rewards = torch.tensor(n_step_rewards).float().to(device).reshape(-1)\n",
    "        Gamma_V = torch.tensor(Gamma_V).float().to(device).reshape(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            V_pred = value_net(new_states_reshaped).squeeze()\n",
    "            V_trg = (1-done)*Gamma_V*V_pred + n_step_rewards\n",
    "            V_trg = V_trg.squeeze()\n",
    "        return V_trg\n",
    "    \n",
    "    def compute_n_step_rewards(self, rewards, done, n_steps, discount):\n",
    "        \"\"\"\n",
    "        Computes n-steps discounted reward. \n",
    "        Note: the rewards considered are AT MOST n, but can be less for the last n-1 elements.\n",
    "        \"\"\"\n",
    "\n",
    "        B = done.shape[0]\n",
    "        T = done.shape[1]\n",
    "\n",
    "        # Compute episode mask (i-th row contains 1 if col j is in the same episode of col i, 0 otherwise)\n",
    "        episode_mask = [[] for _ in range(B)]\n",
    "        last = [-1 for _ in range(B)]\n",
    "        xs, ys = np.nonzero(done)\n",
    "\n",
    "        # Add done at the end of every batch to avoid exceptions -> not used in real target computations\n",
    "        xs = np.concatenate([xs, np.arange(B)])\n",
    "        ys = np.concatenate([ys, np.full(B, T-1)])\n",
    "        for x, y in zip(xs, ys):\n",
    "            m = [1 if (i > last[x] and i <= y) else 0 for i in range(T)]\n",
    "            for _ in range(y-last[x]):\n",
    "                episode_mask[x].append(m)\n",
    "            last[x] = y\n",
    "        episode_mask = np.array(episode_mask)\n",
    "\n",
    "        # Compute n-steps mask and repeat it B times\n",
    "        n_steps_mask = []\n",
    "        for i in range(T):\n",
    "            m = [1 if (j>=i and j<i+n_steps) else 0 for j in range(T)]\n",
    "            n_steps_mask.append(m)\n",
    "        n_steps_mask = np.array(n_steps_mask)\n",
    "        n_steps_mask_b = np.repeat(n_steps_mask[np.newaxis,...] , B, axis=0)\n",
    "\n",
    "        # Broadcast rewards to use multiplicative masks\n",
    "        rewards_repeated = np.repeat(rewards[:,np.newaxis,:], T, axis=1)\n",
    "\n",
    "        # Exponential discount factor\n",
    "        Gamma = np.array([discount**i for i in range(T)]).reshape(1,-1)\n",
    "        n_steps_r = (Gamma*rewards_repeated*episode_mask*n_steps_mask_b).sum(axis=2)/Gamma\n",
    "        return n_steps_r, episode_mask, n_steps_mask_b\n",
    "\n",
    "    def compute_n_step_states(self, trg_states, done, episode_mask, n_steps_mask_b, discount):\n",
    "        \"\"\"\n",
    "        Computes n-steps target states (to be used by the critic as target values together with the\n",
    "        n-steps discounted reward). For last n-1 elements the target state is the last one available.\n",
    "        Adjusts also the `done` mask used for disabling the bootstrapping in the case of terminal states\n",
    "        and returns Gamma_V, that are the discount factors for the target state-values, since they are \n",
    "        n-steps away (except for the last n-1 states, whose discount is adjusted accordingly).\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        new_states, Gamma_V, done: arrays with first dimension = len(states)-1\n",
    "        \"\"\"\n",
    "\n",
    "        B = done.shape[0]\n",
    "        T = done.shape[1]\n",
    "        V_mask = episode_mask*n_steps_mask_b\n",
    "        b, x, y = np.nonzero(V_mask)\n",
    "        V_trg_index = [[] for _ in range(B)]\n",
    "        for b_i in range(B):\n",
    "            valid_x = (b==b_i)\n",
    "            for i in range(T):\n",
    "                matching_x = (x==i)\n",
    "                V_trg_index[b_i].append(y[valid_x*matching_x][-1])\n",
    "        V_trg_index = np.array(V_trg_index)\n",
    "\n",
    "        cols = np.array([], dtype=np.int)\n",
    "        rows = np.array([], dtype=np.int)\n",
    "        for i, v in enumerate(V_trg_index):\n",
    "            cols = np.concatenate([cols, v], axis=0)\n",
    "            row = np.full(V_trg_index.shape[1], i)\n",
    "            rows = np.concatenate([rows, row], axis=0)\n",
    "\n",
    "        new_states = {}\n",
    "        for k in trg_states.keys(): \n",
    "            new_states[k] = trg_states[k][rows, cols].reshape(trg_states[k].shape)\n",
    "\n",
    "        pw = V_trg_index - np.arange(V_trg_index.shape[1]) + 1\n",
    "        Gamma_V = discount**pw\n",
    "        shifted_done = done[rows, cols].reshape(done.shape)\n",
    "        return new_states, Gamma_V, shifted_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_update_v1(value_net, frames, targets, loss_fn, optimizer):\n",
    "    values = value_net(reshaped_frames).squeeze(1)\n",
    "    \n",
    "    loss = loss_fn(values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [\n",
    "    [0,1,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,1,0,0,0,1,0,0]\n",
    "]\n",
    "\n",
    "rewards = np.array(r)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
