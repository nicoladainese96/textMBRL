{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rtfm import featurizer as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "vprint = print if verbose else lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    def __init__(self, \n",
    "                 simulator,\n",
    "                 valid_actions,\n",
    "                 ucb_c,\n",
    "                 discount,\n",
    "                 max_actions,\n",
    "                 root=None,\n",
    "                 render=False):\n",
    "        \"\"\"\n",
    "        Monte Carlo Tree Search assuming deterministic dynamics.\n",
    "        \n",
    "        simulator: \n",
    "            wrapper of the environment that returns a scalar reward, a list of valid actions \n",
    "            and a 'done' boolean flag when presented with an action\n",
    "        valid_moves:\n",
    "            list of valid moves for the root node\n",
    "        ucb_c:\n",
    "            Constantused in the UCB1 formula for trees\n",
    "            UCB(s,a) = Q(s,a) + ucb_c*sqrt(log N(s,a)/(\\sum_b N(s,b)))\n",
    "        discount:\n",
    "            discoung factor gamma of the MDP\n",
    "        max_actions:\n",
    "            number of actions to be taken at most from the root node to the end of a rollout\n",
    "        root: \n",
    "            might be the child of an old root node; use it to keep all the cached computations \n",
    "            from previous searches with a different root node. \n",
    "        \"\"\"\n",
    "        self.simulator = simulator\n",
    "        self.original_dict = simulator.save_state_dict()\n",
    "        self.valid_actions = valid_actions\n",
    "        self.action_space = len(valid_actions)\n",
    "        self.ucb_c = ucb_c\n",
    "        self.discount = discount\n",
    "        self.max_actions = max_actions\n",
    "        self.root = root\n",
    "        self.render = render\n",
    "    \n",
    "    def get_subtree(self, action):\n",
    "        \"\"\"\n",
    "        Returns the subtree whose root node is the current root's child corresponding to\n",
    "        the given action.\n",
    "        \"\"\"\n",
    "        return self.root.children[action]\n",
    "    \n",
    "    def run(self, num_simulations):\n",
    "        \"\"\"\n",
    "        Runs num_simulations searches starting from the root node corresponding to the internal\n",
    "        state of the simulator given during initialization.\n",
    "        Returns the root node and an extra_info dictionary\n",
    "        \"\"\"\n",
    "        if self.root is None:\n",
    "            self.root = Node()\n",
    "            self.root.expand(\n",
    "                self.valid_actions,\n",
    "                0, # reward to get to root\n",
    "                False, # terminal node\n",
    "                self.simulator # state of the simulator at the root node \n",
    "            )\n",
    "            # not sure about this\n",
    "            self.root.visit_count += 1\n",
    "        \n",
    "        max_tree_depth = 0\n",
    "        root = self.root\n",
    "        for n in range(num_simulations):\n",
    "            ### Start of a simulation/search ###\n",
    "            vprint(\"\\nSimulation %d started.\"%(n+1))\n",
    "            node = root\n",
    "            # make sure that the simulator internal state is reset to the original one\n",
    "            self.simulator.load_state_dict(root.simulator_dict)\n",
    "            search_path = [node]\n",
    "            current_tree_depth = 0\n",
    "            if self.render:\n",
    "                node.render(self.simulator)\n",
    "            ### Selection phase until leaf node is reached ###\n",
    "            while node.expanded or (current_tree_depth<self.max_actions):\n",
    "                current_tree_depth += 1\n",
    "                action, node = self.select(node)\n",
    "                if self.render and node.expanded:\n",
    "                    node.render(self.simulator)\n",
    "                vprint(\"Current tree depth: \", current_tree_depth)\n",
    "                vprint(\"Action selected: \", action)\n",
    "                vprint(\"Child node terminal: \", node.terminal)\n",
    "                vprint(\"Child node expanded: \", node.expanded)\n",
    "                if node.expanded or node.terminal:\n",
    "                    search_path.append(node)\n",
    "                    if node.terminal:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            ### Expansion of leaf node (if not terminal)###\n",
    "            vprint(\"Expansion phase started\")\n",
    "            if not node.terminal:\n",
    "                parent = search_path[-1] # last expanded node on the search path\n",
    "                node = self.expand(node, parent, action)\n",
    "                if self.render:\n",
    "                    node.render(self.simulator)\n",
    "                search_path.append(node)\n",
    "            \n",
    "            ### Simulation phase for self.max_actions - current_tree_depth steps ###\n",
    "            vprint(\"Simulation  phase started\")\n",
    "            value = self.simulate(node, current_tree_depth)\n",
    "            vprint(\"Simulated value: \", value)\n",
    "            \n",
    "            ### Backpropagation of the leaf node value along the seach_path ###\n",
    "            vprint(\"Backpropagation phase started\")\n",
    "            self.backprop(search_path, value)\n",
    "        \n",
    "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "            vprint(\"Simulation %d done.\"%(n+1))\n",
    "        extra_info = {\n",
    "            \"max_tree_depth\": max_tree_depth\n",
    "        }\n",
    "        # just a check to see if root works as a shallow copy of self.root\n",
    "        assert root.visit_count == self.root.visit_count, \"self.root not updated during search\"\n",
    "        \n",
    "        # make sure that the simulator internal state is reset to the original one\n",
    "        self.simulator.load_state_dict(root.simulator_dict)\n",
    "        return root, extra_info\n",
    "        \n",
    "    def select(self, node):\n",
    "        \"\"\"\n",
    "        Use UCT formula on the input node; return the action selected and the corresponding child node \n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        ucb_values = []\n",
    "        for action, child in node.children.items():\n",
    "            actions.append(action)\n",
    "            ucb_values.append(self.ucb_score(node, child))\n",
    "        actions = np.array(actions)\n",
    "        vprint(\"actions: \", actions)\n",
    "        \n",
    "        ucb_values = np.array(ucb_values)\n",
    "        vprint(\"ucb_values: \", ucb_values)\n",
    "        \n",
    "        max_U = ucb_values.max()\n",
    "        vprint(\"max_U: \", max_U)\n",
    "        \n",
    "        mask = (ucb_values==max_U)\n",
    "        vprint(\"mask: \", mask)\n",
    "        \n",
    "        best_actions = actions[mask]\n",
    "        vprint(\"best_actions: \", best_actions)\n",
    "        \n",
    "        action = np.random.choice(best_actions)\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, eps=1e-3):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus.\n",
    "        \"\"\"\n",
    "        exploration_term = self.ucb_c*np.sqrt(np.log(parent.visit_count)/(child.visit_count+eps))\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_term = child.reward + self.discount*child.value() \n",
    "        else:\n",
    "            value_term = 0\n",
    "\n",
    "        return value_term + exploration_term\n",
    "    \n",
    "    def expand(self, node, parent, action):\n",
    "        \"\"\"\n",
    "        Expand the node obtained by taking the given action from the parent node \n",
    "        \"\"\"\n",
    "        simulator = parent.get_simulator(self.simulator) # get a deepcopy of the simulator with the parent's state stored\n",
    "        valid_actions, reward, done = simulator.step(action) # this also updates the simulator's internal state\n",
    "        vprint(\"reward: \", reward)\n",
    "        vprint(\"done: \", done)\n",
    "        node.expand(valid_actions, reward, done, simulator)\n",
    "        return node\n",
    "    \n",
    "    def simulate(self, node, current_depth):\n",
    "        \"\"\"\n",
    "        Simulate a rollout with a random policy starting from the input node\n",
    "        until the end of the episode or self.max_actions are reached \n",
    "        (also considering the current depth of the input node from the root)\n",
    "        \"\"\"\n",
    "        if not node.terminal:\n",
    "            simulator = node.get_simulator(self.simulator)\n",
    "            valid_actions = node.valid_actions\n",
    "            steps = self.max_actions - current_depth\n",
    "            cum_discounted_reward = 0\n",
    "            for i in range(steps):\n",
    "                action = np.random.choice(valid_actions)\n",
    "                valid_actions, reward, done = simulator.step(action)\n",
    "                cum_discounted_reward += (self.discount**i)*reward\n",
    "                if done:\n",
    "                    break\n",
    "        else:\n",
    "            cum_discounted_reward = 0\n",
    "        return cum_discounted_reward\n",
    "            \n",
    "    def backprop(self, search_path, value):\n",
    "        \"\"\"\n",
    "        Update the value sum and visit count of all nodes along the search path.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            value = node.reward + self.discount*value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.reward = 0\n",
    "        self.simulator = None\n",
    "        self.expanded = False\n",
    "        self.terminal = False\n",
    "        self.simulator_dict = None\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, valid_actions, reward, done, simulator):\n",
    "        self.expanded = True\n",
    "        vprint(\"Valid actions as child: \", valid_actions)\n",
    "        vprint(\"Terminal node: \", done)\n",
    "        self.reward = reward\n",
    "        self.terminal = done\n",
    "        self.valid_actions = valid_actions\n",
    "        if not done:\n",
    "            for action in valid_actions:\n",
    "                self.children[action] = Node()\n",
    "        self.simulator_dict = simulator.save_state_dict()\n",
    "        \n",
    "    def get_simulator(self, simulator):\n",
    "        if self.simulator_dict is not None:\n",
    "            # load a deepcoy of the simulator_dict, so that the internal variable remains unchanged\n",
    "            simulator.load_state_dict(copy.deepcopy(self.simulator_dict)) \n",
    "            return simulator\n",
    "        else:\n",
    "            print(\"Trying to load simulator_dict, but it was never instantiated.\")\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def best_action(self, discount):\n",
    "        \"\"\"\n",
    "        Look among the children and take the one with higher Q-value. \n",
    "        Exclude children with 0 visits.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        Qvalues = []\n",
    "        for action, child in self.children.items():\n",
    "            actions.append(action)\n",
    "            Qvalues.append(child.reward + discount*child.value())\n",
    "        actions = np.array(actions)\n",
    "        Qvalues = np.array(Qvalues)\n",
    "        max_Q = Qvalues.max()\n",
    "        mask = (Qvalues==max_Q)\n",
    "        best_actions = actions[mask]\n",
    "        return np.random.choice(best_actions)\n",
    "    \n",
    "    def render(self, simulator):\n",
    "        if self.simulator_dict is not None:\n",
    "            simulator.load_state_dict(self.simulator_dict)\n",
    "            simulator.render()\n",
    "        else:\n",
    "            raise Exception(\"Node simulator not initialized yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueSimulator():\n",
    "    \"\"\"\n",
    "    Returns only valid actions, reward and done signal from env.step() - no state is returned\n",
    "    \"\"\"\n",
    "    def __init__(self, env, featurizer=None):\n",
    "        self.env = env\n",
    "        self.action_space = len(gym_env.action_space)\n",
    "        self.featurizer = featurizer\n",
    "        \n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        valid_moves = frame['valid'].numpy().astype(bool) # boolean mask of shape (action_space)\n",
    "        actions = np.arange(self.action_space)\n",
    "        valid_actions = actions[valid_moves]\n",
    "        return valid_actions\n",
    "    \n",
    "    def step(self, action, *args, **kwargs):\n",
    "        frame, reward, done, _ = self.env.step(int(action), *args, **kwargs)\n",
    "        valid_moves = frame['valid'].numpy().astype(bool) # boolean mask of shape (action_space)\n",
    "        actions = np.arange(self.action_space)\n",
    "        valid_actions = actions[valid_moves]\n",
    "        return valid_actions, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        self.featurizer.featurize(self.env)\n",
    "        \n",
    "    def save_state_dict(self):\n",
    "        return self.env.save_state_dict()\n",
    "        \n",
    "    def load_state_dict(self, d):\n",
    "        self.env.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters ###\n",
    "ucb_C = 1.0\n",
    "discount = 0.997\n",
    "episode_length = 50\n",
    "max_actions = 100\n",
    "num_simulations = 50\n",
    "\n",
    "flags = utils.Flags(env=\"rtfm:groups_simple_stationary-v0\")\n",
    "gym_env = utils.create_env(flags)\n",
    "#gym_env = utils.create_env(flags, featurizer=X.Concat([X.Text(), X.ValidMoves(), X.Render()]))\n",
    "featurizer = X.Render()\n",
    "game_simulator = TrueSimulator(gym_env, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "██████\n",
      "█    █\n",
      "█ n@ █\n",
      "█! y?█\n",
      "█    █\n",
      "██████\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game_simulator.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_root_summary(root, discount):\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    \n",
    "    for action, child in root.children.items():\n",
    "        Q =  child.reward + discount*child.value()\n",
    "        visits = child.visit_count\n",
    "        print(\"Action \", action_dict[action], \": Q-value=%.3f - Visit counts=%d\"%(Q,visits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_v0(\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    render = True,\n",
    "    debug_render=False\n",
    "):\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    for i in range(episode_length):\n",
    "        mcts = MCTS(env, valid_actions, ucb_C, discount, max_actions, render=debug_render)\n",
    "        print(\"\\n\",\"-\"*40)\n",
    "        print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        show_root_summary(root, discount)\n",
    "        print(\"-\"*40)\n",
    "        print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        valid_actions, reward, done = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        print(\"Reward received: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_v1(\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    render = True,\n",
    "    debug_render=False\n",
    "):\n",
    "    \"\"\"\n",
    "    W.r.t. version 0 it re-uses the information cached in the child node selected \n",
    "    \"\"\"\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    for i in range(episode_length):\n",
    "        mcts = MCTS(env, valid_actions, ucb_C, discount, max_actions, render=debug_render, root=new_root)\n",
    "        print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        show_root_summary(root, discount)\n",
    "        print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        new_root = mcts.get_subtree(action)\n",
    "        valid_actions, reward, done = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        print(\"Reward received: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██████\n",
      "█ @n!█\n",
      "█ y  █\n",
      "█    █\n",
      "█  ? █\n",
      "██████\n",
      "\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.948 - Visit counts=7\n",
      "Action  Down : Q-value=-0.625 - Visit counts=24\n",
      "Action  Left : Q-value=-0.788 - Visit counts=12\n",
      "Action  Right : Q-value=-0.961 - Visit counts=7\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█  n!█\n",
      "█ @  █\n",
      "█    █\n",
      "█  ? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.755 - Visit counts=10\n",
      "Action  Up : Q-value=-0.912 - Visit counts=6\n",
      "Action  Down : Q-value=-0.759 - Visit counts=10\n",
      "Action  Left : Q-value=-0.738 - Visit counts=10\n",
      "Action  Right : Q-value=-0.128 - Visit counts=14\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  n!█\n",
      "█  @ █\n",
      "█    █\n",
      "█  ? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.973 - Visit counts=1\n",
      "Action  Up : Q-value=-0.997 - Visit counts=1\n",
      "Action  Down : Q-value=-0.950 - Visit counts=1\n",
      "Action  Left : Q-value=-0.950 - Visit counts=1\n",
      "Action  Right : Q-value=0.781 - Visit counts=46\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  n!█\n",
      "█   @█\n",
      "█    █\n",
      "█  ? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.334 - Visit counts=3\n",
      "Action  Up : Q-value=1.000 - Visit counts=45\n",
      "Action  Down : Q-value=-0.988 - Visit counts=1\n",
      "Action  Left : Q-value=-0.979 - Visit counts=1\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 2}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  n@█\n",
      "█    █\n",
      "█    █\n",
      "█  ? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "CPU times: user 1.18 s, sys: 48.7 ms, total: 1.22 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = play_episode_v0(\n",
    "    game_simulator,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    debug_render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██████\n",
      "█  ? █\n",
      "█n  !█\n",
      "█  y@█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.069 - Visit counts=15\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Down : Q-value=-0.856 - Visit counts=2\n",
      "Action  Left : Q-value=0.492 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█n  !█\n",
      "█  @ █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.935 - Visit counts=2\n",
      "Action  Up : Q-value=-0.303 - Visit counts=3\n",
      "Action  Down : Q-value=0.011 - Visit counts=4\n",
      "Action  Left : Q-value=-0.896 - Visit counts=2\n",
      "Action  Right : Q-value=0.857 - Visit counts=69\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█n  !█\n",
      "█   @█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.011 - Visit counts=4\n",
      "Action  Up : Q-value=1.000 - Visit counts=101\n",
      "Action  Down : Q-value=0.399 - Visit counts=10\n",
      "Action  Left : Q-value=-0.258 - Visit counts=3\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█n  @█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "CPU times: user 539 ms, sys: 13.3 ms, total: 552 ms\n",
      "Wall time: 596 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = play_episode_v1(\n",
    "    game_simulator,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    debug_render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing a value network\n",
    "\n",
    "**TODO**\n",
    "- define a **value target** with which to train the value net\n",
    "- choose on which data to train (whole trajectory? just one trajectory or many? on-policy or with experience replay?)\n",
    "- define training cycle\n",
    "\n",
    "**EXTRA**\n",
    "- use some muti-threaded application, like torch.multiprocessing, to run many episodes in parallel; adapt code from IMPALA\n",
    "\n",
    "**DONE**\n",
    "- **make the simulations faster** (function to set the state of the simulator instead of having to make a deepcopy every time?) \n",
    "- get the **state** to predict the value\n",
    "- define a **ValueNet** architecture that can process frame dictionaries with the batch dimension included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTrueSimulator():\n",
    "    \"\"\"\n",
    "    Returns the full state from the environment step.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, featurizer=None):\n",
    "        self.env = env\n",
    "        self.action_space = len(gym_env.action_space)\n",
    "        self.featurizer = featurizer\n",
    "        \n",
    "    def _process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Extracts from frame a valid_action numpy array containing integers from 0 to self.action_space-1\n",
    "        Adds batch dimension to all values stored inside frame dictionary\n",
    "        \"\"\"\n",
    "        # do this before batch dim is added\n",
    "        valid_moves = frame['valid'].numpy().astype(bool) # boolean mask of shape (action_space)\n",
    "        actions = np.arange(self.action_space)\n",
    "        valid_actions = actions[valid_moves]\n",
    "        \n",
    "        for k in frame.keys():\n",
    "            frame[k] = frame[k].unsqueeze(0)\n",
    "        \n",
    "        return frame, valid_actions\n",
    "    \n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        frame, valid_actions = self._process_frame(frame)\n",
    "        return frame, valid_actions\n",
    "    \n",
    "    def step(self, action, *args, **kwargs):\n",
    "        frame, reward, done, _ = self.env.step(int(action), *args, **kwargs)\n",
    "        frame, valid_actions = self._process_frame(frame)\n",
    "        return frame, valid_actions, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        self.featurizer.featurize(self.env)\n",
    "        \n",
    "    def save_state_dict(self):\n",
    "        return self.env.save_state_dict()\n",
    "        \n",
    "    def load_state_dict(self, d):\n",
    "        self.env.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedDynamicsValueNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 gym_env,\n",
    "                 emb_dim=10,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(gym_env.vocab), emb_dim)\n",
    "        \n",
    "        name_shape = gym_env.observation_space['name']\n",
    "        inv_shape = gym_env.observation_space['inv']\n",
    "        n_channels = (name_shape[2]*name_shape[3]+inv_shape[0])*emb_dim\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(gym_env.observation_space['name'][1])\n",
    "\n",
    "        self.value_mlp = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, frame):\n",
    "        device = next(self.parameters()).device\n",
    "        x = self.embedding(frame['name'].to(device))\n",
    "        s = x.shape\n",
    "        B, W, H = s[:3]\n",
    "        x = x.reshape(*s[:3],-1).permute(0, 3, 1, 2)\n",
    "        inv = self.embedding(frame['inv'].to(device))\n",
    "        inv = inv.reshape(B,-1,1,1)\n",
    "        inv = inv.expand(B,-1,W,H)\n",
    "        z = torch.cat([x,inv], axis=1)\n",
    "        z_conv = self.conv_net(z)\n",
    "        z_flat = self.maxpool(z_conv).view(B,-1)\n",
    "        v = self.value_mlp(z_flat)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_simulator = FullTrueSimulator(gym_env, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, valid_actions = game_simulator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = FixedDynamicsValueNet(gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0035]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value_net(frame)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network and Value Target\n",
    "\n",
    "#### When and why the value network is used?\n",
    "As a default all our information about the value of a state comes from a single rollout of a random policy. \n",
    "In general this is highly inaccurate, since the probability of executing the optimal sequence of decisions from a given state is exponentially decreasing with the number of steps $t$ taken during the rollout (e.g. $~|A|^{-t}$). In our specific case it's more a proxy signal for how likely it is that a good, neutral or bad outcome is obtained from the current state; for instance if we are near the goal state (which is an absorbing state), there is a relevant probability of getting there, especially if many simulations are directed to the node from which the rollout starts.\n",
    "\n",
    "The idea about the value network is that it should learn to predict given some policy what is the EXPECTED value if we made infinite rollouts from there following that policy. This in general will be a BIASED estimate but with LOWER VARIANCE than a Monte Carlo rollout. \n",
    "\n",
    "#### On which states is trained?\n",
    "The value network is trained to predict the value of the root nodes, i.e. the states actually visited along a trajectory, which are also the states on which we possess better informations thanks to the MCTS. \n",
    "\n",
    "#### On which target is trained?\n",
    "This is the key question, since it determines of which policy are we estimating the value. If we were to estimate the value of the rollout policy, this would be guaranteed to be smaller or at best equal to the value of the MCTS policy (that is, the policy that runs $n$ simulations arranged in a tree and selects the action corresponding to the highest Q-value, or the most visited child). \n",
    "\n",
    "I think that the most sensible way to train the network is to use the formula:\n",
    "$$\n",
    "V_{trg}^{\\pi_{mcts}}(s_t) = \\sum_{k=0}^{n-1}\\gamma^k r_{t+k} + \\gamma^n \\hat{V}(s_t+n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.frame = None\n",
    "\n",
    "    def expand(self, frame, valid_actions, reward, done, simulator):\n",
    "        self.expanded = True\n",
    "        vprint(\"Valid actions as child: \", valid_actions)\n",
    "        vprint(\"Terminal node: \", done)\n",
    "        self.frame = frame\n",
    "        self.reward = reward\n",
    "        self.terminal = done\n",
    "        self.valid_actions = valid_actions\n",
    "        if not done:\n",
    "            for action in valid_actions:\n",
    "                self.children[action] = ValueNode()\n",
    "        self.simulator_dict = simulator.save_state_dict()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using device \"+device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueMCTS(MCTS):\n",
    "    def __init__(self, \n",
    "                 root_frame,\n",
    "                 simulator,\n",
    "                 valid_actions,\n",
    "                 ucb_c,\n",
    "                 discount,\n",
    "                 max_actions,\n",
    "                 value_net,\n",
    "                 root=None,\n",
    "                 render=False):\n",
    "        \n",
    "        super().__init__(simulator,\n",
    "                     valid_actions,\n",
    "                     ucb_c,\n",
    "                     discount,\n",
    "                     max_actions,\n",
    "                     root,\n",
    "                     render)\n",
    "        self.value_net = value_net\n",
    "        self.root_frame = root_frame\n",
    "        \n",
    "    def run(self, num_simulations):\n",
    "        \"\"\"\n",
    "        Runs num_simulations searches starting from the root node corresponding to the internal\n",
    "        state of the simulator given during initialization.\n",
    "        Returns the root node and an extra_info dictionary\n",
    "        \"\"\"\n",
    "        if self.root is None:\n",
    "            self.root = ValueNode()\n",
    "            self.root.expand(\n",
    "                self.root_frame,\n",
    "                self.valid_actions,\n",
    "                0, # reward to get to root\n",
    "                False, # terminal node\n",
    "                self.simulator # state of the simulator at the root node \n",
    "            )\n",
    "            # not sure about this\n",
    "            self.root.visit_count += 1\n",
    "        \n",
    "        max_tree_depth = 0\n",
    "        root = self.root\n",
    "        for n in range(num_simulations):\n",
    "            ### Start of a simulation/search ###\n",
    "            vprint(\"\\nSimulation %d started.\"%(n+1))\n",
    "            node = root\n",
    "            # make sure that the simulator internal state is reset to the original one\n",
    "            self.simulator.load_state_dict(root.simulator_dict)\n",
    "            search_path = [node]\n",
    "            current_tree_depth = 0\n",
    "            if self.render:\n",
    "                node.render(self.simulator)\n",
    "            ### Selection phase until leaf node is reached ###\n",
    "            while node.expanded or (current_tree_depth<self.max_actions):\n",
    "                current_tree_depth += 1\n",
    "                action, node = self.select(node)\n",
    "                if self.render and node.expanded:\n",
    "                    node.render(self.simulator)\n",
    "                vprint(\"Current tree depth: \", current_tree_depth)\n",
    "                vprint(\"Action selected: \", action)\n",
    "                vprint(\"Child node terminal: \", node.terminal)\n",
    "                vprint(\"Child node expanded: \", node.expanded)\n",
    "                if node.expanded or node.terminal:\n",
    "                    search_path.append(node)\n",
    "                    if node.terminal:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            ### Expansion of leaf node (if not terminal)###\n",
    "            vprint(\"Expansion phase started\")\n",
    "            if not node.terminal:\n",
    "                parent = search_path[-1] # last expanded node on the search path\n",
    "                node = self.expand(node, parent, action)\n",
    "                if self.render:\n",
    "                    node.render(self.simulator)\n",
    "                search_path.append(node)\n",
    "            \n",
    "            ### Simulation phase for self.max_actions - current_tree_depth steps ###\n",
    "            vprint(\"Value prediction phase started\")\n",
    "            value = self.simulate_and_predict(node, current_tree_depth)\n",
    "            #value = self.simulate(node, current_tree_depth)\n",
    "            vprint(\"Predicted value: \", value)\n",
    "            \n",
    "            ### Backpropagation of the leaf node value along the seach_path ###\n",
    "            vprint(\"Backpropagation phase started\")\n",
    "            self.backprop(search_path, value)\n",
    "        \n",
    "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "            vprint(\"Simulation %d done.\"%(n+1))\n",
    "        extra_info = {\n",
    "            \"max_tree_depth\": max_tree_depth\n",
    "        }\n",
    "        # just a check to see if root works as a shallow copy of self.root\n",
    "        assert root.visit_count == self.root.visit_count, \"self.root not updated during search\"\n",
    "        \n",
    "        # make sure that the simulator internal state is reset to the original one\n",
    "        self.simulator.load_state_dict(root.simulator_dict)\n",
    "        return root, extra_info\n",
    "    \n",
    "    def expand(self, node, parent, action):\n",
    "        \"\"\"\n",
    "        Expand the node obtained by taking the given action from the parent node \n",
    "        \"\"\"\n",
    "        simulator = parent.get_simulator(self.simulator) # get a deepcopy of the simulator with the parent's state stored\n",
    "        frame, valid_actions, reward, done = simulator.step(action) # this also updates the simulator's internal state\n",
    "        vprint(\"reward: \", reward)\n",
    "        vprint(\"done: \", done)\n",
    "        node.expand(frame, valid_actions, reward, done, simulator)\n",
    "        return node\n",
    "    \n",
    "    def predict(self, node):\n",
    "        with torch.no_grad():\n",
    "            value = self.value_net(node.frame)\n",
    "        return value.item()\n",
    "    \n",
    "    def simulate(self, node, current_depth):\n",
    "        \"\"\"\n",
    "        Simulate a rollout with a random policy starting from the input node\n",
    "        until the end of the episode or self.max_actions are reached \n",
    "        (also considering the current depth of the input node from the root)\n",
    "        \"\"\"\n",
    "        if not node.terminal:\n",
    "            simulator = node.get_simulator(self.simulator)\n",
    "            valid_actions = node.valid_actions\n",
    "            steps = self.max_actions - current_depth\n",
    "            cum_discounted_reward = 0\n",
    "            for i in range(steps):\n",
    "                action = np.random.choice(valid_actions)\n",
    "                frame, valid_actions, reward, done = simulator.step(action)\n",
    "                cum_discounted_reward += (self.discount**i)*reward\n",
    "                if done:\n",
    "                    break\n",
    "        else:\n",
    "            cum_discounted_reward = 0\n",
    "        return cum_discounted_reward\n",
    "    \n",
    "    def simulate_and_predict(self, node, current_depth, n_steps=5):\n",
    "        \"\"\"\n",
    "        Simulate a rollout with a random policy starting from the input node\n",
    "        until the end of the episode or self.max_actions are reached \n",
    "        (also considering the current depth of the input node from the root)\n",
    "        or at most n_steps before calling the value_net to approximate the rest of the trajectory.\n",
    "        \"\"\"\n",
    "        if not node.terminal:\n",
    "            simulator = node.get_simulator(self.simulator)\n",
    "            valid_actions = node.valid_actions\n",
    "            steps = min(self.max_actions - current_depth, n_steps)\n",
    "            cum_discounted_reward = 0\n",
    "            for i in range(steps):\n",
    "                action = np.random.choice(valid_actions)\n",
    "                frame, valid_actions, reward, done = simulator.step(action)\n",
    "                cum_discounted_reward += (self.discount**i)*reward\n",
    "                if done:\n",
    "                    break\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    bootstrap_value = self.value_net(frame).item()\n",
    "                cum_discounted_reward += (self.discount**steps)*bootstrap_value\n",
    "        else:\n",
    "            cum_discounted_reward = 0\n",
    "        return cum_discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_value_net(\n",
    "    value_net,\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    render = False,\n",
    "    debug_render=False\n",
    "):\n",
    "    \"\"\"\n",
    "    W.r.t. version 0 it re-uses the information cached in the child node selected \n",
    "    \"\"\"\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    frame, valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    # variables used for training of value net\n",
    "    frame_lst = [frame]\n",
    "    reward_lst = []\n",
    "    done_lst = []\n",
    "    for i in range(episode_length):\n",
    "        mcts = ValueMCTS(frame, \n",
    "                         env, \n",
    "                         valid_actions, \n",
    "                         ucb_C, \n",
    "                         discount, \n",
    "                         max_actions, \n",
    "                         value_net,\n",
    "                         render=debug_render, \n",
    "                         root=new_root\n",
    "                        )\n",
    "        #print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        #show_root_summary(root, discount)\n",
    "        #print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        #print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        new_root = mcts.get_subtree(action)\n",
    "        frame, valid_actions, reward, done = env.step(action)\n",
    "        \n",
    "        frame_lst.append(frame)\n",
    "        reward_lst.append(reward)\n",
    "        done_lst.append(done)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        #print(\"Reward received: \", reward)\n",
    "        #print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, frame_lst, reward_lst, done_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_rollout_value_net(\n",
    "        value_net,\n",
    "        env,\n",
    "        episode_length,\n",
    "        ucb_C,\n",
    "        discount,\n",
    "        max_actions,\n",
    "        num_simulations,\n",
    "        render = False,\n",
    "        debug_render=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    W.r.t. version 0 it re-uses the information cached in the child node selected \n",
    "    \"\"\"\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    frame, valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    # variables used for training of value net\n",
    "    frame_lst = [frame]\n",
    "    reward_lst = []\n",
    "    done_lst = []\n",
    "    for i in range(episode_length):\n",
    "        mcts = ValueMCTS(frame, \n",
    "                         env, \n",
    "                         valid_actions, \n",
    "                         ucb_C, \n",
    "                         discount, \n",
    "                         max_actions, \n",
    "                         value_net,\n",
    "                         render=debug_render, \n",
    "                         root=new_root\n",
    "                        )\n",
    "        if render:\n",
    "            print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        if render:\n",
    "            show_root_summary(root, discount)\n",
    "            print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        if render:\n",
    "            print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        new_root = mcts.get_subtree(action)\n",
    "        frame, valid_actions, reward, done = env.step(action)\n",
    "        \n",
    "        frame_lst.append(frame)\n",
    "        reward_lst.append(reward)\n",
    "        done_lst.append(done)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            print(\"Reward received: \", reward)\n",
    "            print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            frame, valid_actions = env.reset()\n",
    "            done = False\n",
    "            new_root = None\n",
    "            \n",
    "    return total_reward, frame_lst, reward_lst, done_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters ###\n",
    "ucb_C = 1.0\n",
    "discount = 0.997\n",
    "episode_length = 100\n",
    "max_actions = 100\n",
    "num_simulations = 50\n",
    "\n",
    "flags = utils.Flags(env=\"rtfm:groups_simple_stationary-v0\")\n",
    "gym_env = utils.create_env(flags)\n",
    "#gym_env = utils.create_env(flags, featurizer=X.Concat([X.Text(), X.ValidMoves(), X.Render()]))\n",
    "featurizer = X.Render()\n",
    "game_simulator = FullTrueSimulator(gym_env, featurizer)\n",
    "value_net = FixedDynamicsValueNet(gym_env).to(device)\n",
    "optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-2)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██████\n",
      "█   ?█\n",
      "█  n █\n",
      "█ y  █\n",
      "█  @!█\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.579 - Visit counts=5\n",
      "Action  Up : Q-value=-0.190 - Visit counts=18\n",
      "Action  Left : Q-value=-0.098 - Visit counts=24\n",
      "Action  Right : Q-value=-1.000 - Visit counts=3\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█  n █\n",
      "█ y  █\n",
      "█ @ !█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.134 - Visit counts=12\n",
      "Action  Up : Q-value=0.087 - Visit counts=31\n",
      "Action  Left : Q-value=-0.018 - Visit counts=19\n",
      "Action  Right : Q-value=-0.244 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█  n █\n",
      "█ @  █\n",
      "█   !█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.027 - Visit counts=9\n",
      "Action  Up : Q-value=-0.134 - Visit counts=6\n",
      "Action  Down : Q-value=0.375 - Visit counts=31\n",
      "Action  Left : Q-value=0.123 - Visit counts=11\n",
      "Action  Right : Q-value=0.400 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█  n █\n",
      "█  @ █\n",
      "█   !█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.233 - Visit counts=9\n",
      "Action  Up : Q-value=-0.988 - Visit counts=1\n",
      "Action  Down : Q-value=0.829 - Visit counts=52\n",
      "Action  Left : Q-value=0.352 - Visit counts=9\n",
      "Action  Right : Q-value=-0.985 - Visit counts=1\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█  n █\n",
      "█    █\n",
      "█  @!█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.741 - Visit counts=19\n",
      "Action  Up : Q-value=0.039 - Visit counts=4\n",
      "Action  Left : Q-value=0.226 - Visit counts=5\n",
      "Action  Right : Q-value=1.000 - Visit counts=73\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█  n █\n",
      "█    █\n",
      "█   @█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.651 - Visit counts=3\n",
      "Action  Down : Q-value=-1.000 - Visit counts=2\n",
      "Action  Left : Q-value=0.307 - Visit counts=37\n",
      "Action  Right : Q-value=-0.348 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█y?  █\n",
      "█ !n █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.648 - Visit counts=3\n",
      "Action  Down : Q-value=0.663 - Visit counts=80\n",
      "Action  Right : Q-value=-0.653 - Visit counts=3\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█@?  █\n",
      "█ !n █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.005 - Visit counts=5\n",
      "Action  Up : Q-value=-0.219 - Visit counts=4\n",
      "Action  Down : Q-value=0.889 - Visit counts=118\n",
      "Action  Right : Q-value=-1.000 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█@!n █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.021 - Visit counts=4\n",
      "Action  Up : Q-value=0.183 - Visit counts=6\n",
      "Action  Down : Q-value=0.410 - Visit counts=10\n",
      "Action  Right : Q-value=1.000 - Visit counts=147\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█ @n █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.075 - Visit counts=9\n",
      "Action  Up : Q-value=-0.064 - Visit counts=10\n",
      "Action  Down : Q-value=0.040 - Visit counts=12\n",
      "Action  Left : Q-value=-0.253 - Visit counts=7\n",
      "Action  Right : Q-value=0.039 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█?  !█\n",
      "█    █\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.040 - Visit counts=18\n",
      "Action  Up : Q-value=-0.039 - Visit counts=13\n",
      "Action  Left : Q-value=-0.038 - Visit counts=13\n",
      "Action  Right : Q-value=0.040 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█?  !█\n",
      "█    █\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-d6340e3c6d07>\u001b[0m in \u001b[0;36mplay_rollout_value_net\u001b[0;34m(value_net, env, episode_length, ucb_C, discount, max_actions, num_simulations, render, debug_render)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing MCTS step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mshow_root_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-bb5bf7545106>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_simulations)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m### Simulation phase for self.max_actions - current_tree_depth steps ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mvprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value prediction phase started\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_tree_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;31m#value = self.simulate(node, current_tree_depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mvprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted value: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-bb5bf7545106>\u001b[0m in \u001b[0;36msimulate_and_predict\u001b[0;34m(self, node, current_depth, n_steps)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                     \u001b[0mbootstrap_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0mcum_discounted_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbootstrap_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/work/modules/automatic/anaconda/envs/aalto-ubuntu1804-generic/software/anaconda/2020-04-tf2/1b2b24f2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-820ed9bcbd6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mz_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mz_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/work/modules/automatic/anaconda/envs/aalto-ubuntu1804-generic/software/anaconda/2020-04-tf2/1b2b24f2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/work/modules/automatic/anaconda/envs/aalto-ubuntu1804-generic/software/anaconda/2020-04-tf2/1b2b24f2/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/work/modules/automatic/anaconda/envs/aalto-ubuntu1804-generic/software/anaconda/2020-04-tf2/1b2b24f2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/work/modules/automatic/anaconda/envs/aalto-ubuntu1804-generic/software/anaconda/2020-04-tf2/1b2b24f2/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/work/modules/automatic/anaconda/envs/aalto-ubuntu1804-generic/software/anaconda/2020-04-tf2/1b2b24f2/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_reward, frame_lst, reward_lst, done_lst = play_rollout_value_net(value_net,\n",
    "                                                                        game_simulator,\n",
    "                                                                        episode_length,\n",
    "                                                                        ucb_C,\n",
    "                                                                        discount,\n",
    "                                                                        max_actions,\n",
    "                                                                        num_simulations,\n",
    "                                                                        render=True\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, discount, dones):\n",
    "    cum_disc_rewards = []\n",
    "    cum_r = 0\n",
    "    for i,r in enumerate(reversed(rewards)):\n",
    "        not_done = 1 - dones[-(i+1)]\n",
    "        cum_r = not_done*discount*cum_r + r\n",
    "        cum_disc_rewards.append (cum_r)\n",
    "    cum_disc_rewards = torch.tensor(cum_disc_rewards[::-1])\n",
    "    return cum_disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, mem_size, discount):\n",
    "        self.mem_size = mem_size\n",
    "        self.discount = discount\n",
    "        self.frame_buffer = []\n",
    "        self.V_target_buffer = []\n",
    "    \n",
    "    def store_episode(self, frame_lst, reward_lst, done_lst):\n",
    "        frames, targets = self.batch_episode(frame_lst, reward_lst, done_lst)\n",
    "        self.frame_buffer.append(frames)\n",
    "        self.V_target_buffer.append(targets)\n",
    "        if len(self.frame_buffer) > self.mem_size:\n",
    "            self.frame_buffer.pop(0)\n",
    "            self.V_target_buffer.pop(0)\n",
    "            \n",
    "    def batch_episode(self, frame_lst, reward_lst, done_lst):\n",
    "        episode_len = len(reward_lst)\n",
    "        frames = {}\n",
    "        for k in frame_lst[0].keys():\n",
    "            k_value_lst = []\n",
    "            for b in range(episode_len):\n",
    "                k_value_lst.append(frame_lst[b][k])\n",
    "            k_value_lst = torch.cat(k_value_lst, axis=0)\n",
    "            frames[k] = k_value_lst.unsqueeze(0)\n",
    "        rewards = torch.tensor(reward_lst).float()\n",
    "        dones = torch.tensor(done_lst).float()\n",
    "        targets = get_cumulative_rewards(rewards, self.discount, dones)\n",
    "        \n",
    "        return frames, targets.unsqueeze(0)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        id_range = len(self.V_target_buffer)\n",
    "        assert id_range >= batch_size, \"Not enough samples stored to get this batch size\"\n",
    "        sampled_ids = np.random.choice(id_range, size=batch_size, replace=False)\n",
    "        print(\"sampled_ids: \", sampled_ids)\n",
    "        sampled_targets = torch.cat([self.V_target_buffer[i] for i in sampled_ids], axis=0)\n",
    "        print(\"sampled_targets.shape: \", sampled_targets.shape)\n",
    "        # batch together frames \n",
    "        sampled_frames = {}\n",
    "        for k in self.frame_buffer[0].keys():\n",
    "            key_values = torch.cat([self.frame_buffer[i][k] for i in sampled_ids], axis=0)\n",
    "            print(k, key_values.shape)\n",
    "            sampled_frames[k] = key_values\n",
    "        return sampled_frames, sampled_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ReplayBuffer(1000, discount)\n",
    "for i in range(32):\n",
    "    rb.store_episode(frame_lst, reward_lst, done_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_ids:  [17 13  0 26 24 25  1  4 19  8 18  6 10 27 16 28]\n",
      "sampled_targets.shape:  torch.Size([16, 100])\n",
      "name torch.Size([16, 100, 6, 6, 2, 2])\n",
      "name_len torch.Size([16, 100, 6, 6, 2])\n",
      "inv torch.Size([16, 100, 2])\n",
      "inv_len torch.Size([16, 100, 1])\n",
      "wiki torch.Size([16, 100, 80])\n",
      "wiki_len torch.Size([16, 100, 1])\n",
      "task torch.Size([16, 100, 40])\n",
      "task_len torch.Size([16, 100, 1])\n",
      "valid torch.Size([16, 100, 5])\n"
     ]
    }
   ],
   "source": [
    "frames, targets = rb.get_batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_episode(frame_lst, reward_lst, done_lst, value_net, discount):\n",
    "    \"\"\"\n",
    "    Single episode of varying length and no restarts in between. \n",
    "    batch_size is actually the temporal dimension.\n",
    "    Keep it simple just to see if the training procedure works.\n",
    "    \"\"\"\n",
    "    batch_size = len(reward_lst)\n",
    "    frames = {}\n",
    "    for k in frame_lst[0].keys():\n",
    "        k_value_lst = []\n",
    "        for b in range(batch_size):\n",
    "            k_value_lst.append(frame_lst[b][k])\n",
    "        k_value_lst = torch.cat(k_value_lst, axis=0)\n",
    "        frames[k] = k_value_lst\n",
    "    rewards = torch.tensor(reward_lst).float()\n",
    "    dones = torch.tensor(done_lst).float()\n",
    "    \n",
    "    values = value_net(frames) # leave out last state's value\n",
    "    targets = get_cumulative_rewards(rewards, discount, dones)\n",
    "    return values, targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, targets = batch_episode(frame_lst, reward_lst, done_lst, value_net, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1 - Total reward 0\n",
      "Loss: 0.0073\n",
      "Rollout time: 1.14 - Update time: 0.00\n",
      "\n",
      "Episode 2 - Total reward 0\n",
      "Loss: 11.1406\n",
      "Rollout time: 1.55 - Update time: 0.00\n",
      "\n",
      "Episode 3 - Total reward 0\n",
      "Loss: 0.0390\n",
      "Rollout time: 1.11 - Update time: 0.00\n",
      "\n",
      "Episode 4 - Total reward 7\n",
      "Loss: 0.4768\n",
      "Rollout time: 0.83 - Update time: 0.00\n",
      "\n",
      "Episode 5 - Total reward 3\n",
      "Loss: 0.2516\n",
      "Rollout time: 0.85 - Update time: 0.00\n",
      "\n",
      "Episode 6 - Total reward 0\n",
      "Loss: 0.1004\n",
      "Rollout time: 1.55 - Update time: 0.00\n",
      "\n",
      "Episode 7 - Total reward 0\n",
      "Loss: 0.0166\n",
      "Rollout time: 1.59 - Update time: 0.00\n",
      "\n",
      "Episode 8 - Total reward 1\n",
      "Loss: 0.1088\n",
      "Rollout time: 1.05 - Update time: 0.00\n",
      "\n",
      "Episode 9 - Total reward 0\n",
      "Loss: 0.0001\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 10 - Total reward 1\n",
      "Values:  tensor([-0.0866, -0.1832, -0.1888, -0.1920, -0.0903, -0.0876, -0.0903, -0.0903,\n",
      "        -0.0895, -0.0903, -0.0895, -0.0868, -0.0895, -0.0903, -0.0903, -0.0876,\n",
      "        -0.0876, -0.0903, -0.0876, -0.0876, -0.0876, -0.0876, -0.0876, -0.0876,\n",
      "        -0.0868, -0.0868, -0.0868, -0.0864, -0.1720, -0.1730, -0.1741, -0.1741,\n",
      "        -0.1741, -0.1734, -0.1734, -0.1734, -0.1720, -0.1734, -0.1720, -0.1710,\n",
      "        -0.1720, -0.1720, -0.1716, -0.1716, -0.1709, -0.1716, -0.1724, -0.1724,\n",
      "        -0.1716, -0.1716, -0.1709, -0.1716, -0.1724, -0.1724, -0.1716, -0.1709,\n",
      "        -0.1716, -0.1724, -0.1716, -0.1724, -0.1718, -0.1724, -0.1716, -0.1724,\n",
      "        -0.1724, -0.1718, -0.1724, -0.1718, -0.1724, -0.1718, -0.1718, -0.1718,\n",
      "        -0.1709, -0.1716, -0.1716, -0.1724, -0.1724, -0.1724, -0.1716, -0.1716,\n",
      "        -0.1724, -0.1724, -0.1718, -0.1709, -0.1710, -0.1720, -0.1710, -0.1709,\n",
      "        -0.1718, -0.1709, -0.1709, -0.1716, -0.1709, -0.1710, -0.1709, -0.1709,\n",
      "        -0.1718, -0.1709, -0.1716, -0.1709], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.0633\n",
      "Rollout time: 1.32 - Update time: 0.00\n",
      "\n",
      "Episode 11 - Total reward 3\n",
      "Loss: 0.2554\n",
      "Rollout time: 0.92 - Update time: 0.00\n",
      "\n",
      "Episode 12 - Total reward 3\n",
      "Loss: 0.1661\n",
      "Rollout time: 1.19 - Update time: 0.00\n",
      "\n",
      "Episode 13 - Total reward 1\n",
      "Loss: 0.0571\n",
      "Rollout time: 1.05 - Update time: 0.00\n",
      "\n",
      "Episode 14 - Total reward 3\n",
      "Loss: 0.5019\n",
      "Rollout time: 1.00 - Update time: 0.00\n",
      "\n",
      "Episode 15 - Total reward 2\n",
      "Loss: 0.3575\n",
      "Rollout time: 0.93 - Update time: 0.00\n",
      "\n",
      "Episode 16 - Total reward 7\n",
      "Loss: 0.1389\n",
      "Rollout time: 0.80 - Update time: 0.00\n",
      "\n",
      "Episode 17 - Total reward 3\n",
      "Loss: 0.7829\n",
      "Rollout time: 1.01 - Update time: 0.00\n",
      "\n",
      "Episode 18 - Total reward 0\n",
      "Loss: 0.1469\n",
      "Rollout time: 1.13 - Update time: 0.00\n",
      "\n",
      "Episode 19 - Total reward 0\n",
      "Loss: 0.0153\n",
      "Rollout time: 1.58 - Update time: 0.00\n",
      "\n",
      "Episode 20 - Total reward 0\n",
      "Values:  tensor([ 0.0290,  0.0290,  0.0470,  0.0290,  0.0470,  0.0450,  0.0450,  0.0484,\n",
      "         0.0393,  0.0314,  0.0314,  0.0314,  0.0393,  0.0314,  0.0314,  0.0393,\n",
      "         0.0394,  0.0393,  0.0314,  0.0314,  0.0314,  0.0314,  0.0314,  0.0314,\n",
      "         0.0393,  0.0314,  0.0393,  0.0394,  0.0393,  0.0484,  0.0484,  0.0484,\n",
      "         0.0393,  0.0393,  0.0314,  0.0314,  0.0393,  0.0484,  0.0484,  0.0484,\n",
      "        -0.2145, -0.2135, -0.2145, -0.2145, -0.2158, -0.2158, -0.2145, -0.2145,\n",
      "        -0.2145, -0.2155, -0.2155, -0.2145, -0.2158, -0.2158, -0.2145, -0.2158,\n",
      "        -0.2158, -0.2158, -0.2158, -0.2158, -0.2155, -0.2158, -0.2158, -0.2145,\n",
      "        -0.2145, -0.2158, -0.2155, -0.2135, -0.2145, -0.2145, -0.2135, -0.2145,\n",
      "        -0.2158, -0.2155, -0.2158, -0.2158, -0.2158, -0.2158, -0.2158, -0.2145,\n",
      "        -0.2158, -0.2158, -0.2158, -0.2158, -0.2155, -0.2158, -0.2158, -0.2155,\n",
      "        -0.2155, -0.2155, -0.2158, -0.2158, -0.2145, -0.2158, -0.2145, -0.2135,\n",
      "        -0.2145, -0.2145, -0.2158, -0.2155], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], device='cuda:0')\n",
      "Loss: 0.0284\n",
      "Rollout time: 1.30 - Update time: 0.00\n",
      "\n",
      "Episode 21 - Total reward 3\n",
      "Loss: 1.2017\n",
      "Rollout time: 1.13 - Update time: 0.00\n",
      "\n",
      "Episode 22 - Total reward 11\n",
      "Loss: 1.0216\n",
      "Rollout time: 0.51 - Update time: 0.00\n",
      "\n",
      "Episode 23 - Total reward 3\n",
      "Loss: 0.2113\n",
      "Rollout time: 0.90 - Update time: 0.00\n",
      "\n",
      "Episode 24 - Total reward 3\n",
      "Loss: 0.3157\n",
      "Rollout time: 0.80 - Update time: 0.00\n",
      "\n",
      "Episode 25 - Total reward 3\n",
      "Loss: 0.3237\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 26 - Total reward 0\n",
      "Loss: 0.0032\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 27 - Total reward 3\n",
      "Loss: 0.4001\n",
      "Rollout time: 0.81 - Update time: 0.00\n",
      "\n",
      "Episode 28 - Total reward 4\n",
      "Loss: 0.5062\n",
      "Rollout time: 0.90 - Update time: 0.00\n",
      "\n",
      "Episode 29 - Total reward 5\n",
      "Loss: 0.5231\n",
      "Rollout time: 0.74 - Update time: 0.00\n",
      "\n",
      "Episode 30 - Total reward 5\n",
      "Values:  tensor([0.1751, 0.1856, 0.1854, 0.1846, 0.1857, 0.1857, 0.1856, 0.1855, 0.1859,\n",
      "        0.1852, 0.1842, 0.1848, 0.1856, 0.1750, 0.1763, 0.1849, 0.1854, 0.1848,\n",
      "        0.1795, 0.1847, 0.1844, 0.1854, 0.1861, 0.1746, 0.1729, 0.1743, 0.1751,\n",
      "        0.1751, 0.1768, 0.1768, 0.1751, 0.1768, 0.1751, 0.1743, 0.1751, 0.1743,\n",
      "        0.1743, 0.1729, 0.1743, 0.1743, 0.1729, 0.1743, 0.1743, 0.1743, 0.1751,\n",
      "        0.1743, 0.1743, 0.1729, 0.1743, 0.1751, 0.1743, 0.1751, 0.1768, 0.1768,\n",
      "        0.1751, 0.1743, 0.1729, 0.1729, 0.1743, 0.1743, 0.1743, 0.1743, 0.1743,\n",
      "        0.1729, 0.1729, 0.1729, 0.1729, 0.1743, 0.1743, 0.1729, 0.1729, 0.1743,\n",
      "        0.1729, 0.1743, 0.1743, 0.1743, 0.1729, 0.1729, 0.1743, 0.1751, 0.1743,\n",
      "        0.1743, 0.1743, 0.1751, 0.1743, 0.1743, 0.1743, 0.1729, 0.1743, 0.1743,\n",
      "        0.1743, 0.1751, 0.1768, 0.1751, 0.1751, 0.1743, 0.1743, 0.1751, 0.1751,\n",
      "        0.1751], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9940, 0.9970, 1.0000, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9940, 0.9970, 1.0000, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9881,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.1684\n",
      "Rollout time: 0.89 - Update time: 0.00\n",
      "\n",
      "Episode 31 - Total reward 1\n",
      "Loss: 0.1056\n",
      "Rollout time: 1.35 - Update time: 0.00\n",
      "\n",
      "Episode 32 - Total reward 2\n",
      "Loss: 0.1748\n",
      "Rollout time: 1.08 - Update time: 0.00\n",
      "\n",
      "Episode 33 - Total reward 0\n",
      "Loss: 0.1116\n",
      "Rollout time: 1.16 - Update time: 0.00\n",
      "\n",
      "Episode 34 - Total reward 11\n",
      "Loss: 0.3731\n",
      "Rollout time: 0.59 - Update time: 0.00\n",
      "\n",
      "Episode 35 - Total reward 6\n",
      "Loss: 0.3038\n",
      "Rollout time: 0.72 - Update time: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 36 - Total reward 2\n",
      "Loss: 0.1915\n",
      "Rollout time: 1.12 - Update time: 0.00\n",
      "\n",
      "Episode 37 - Total reward 1\n",
      "Loss: 0.1811\n",
      "Rollout time: 0.93 - Update time: 0.00\n",
      "\n",
      "Episode 38 - Total reward 5\n",
      "Loss: 0.0760\n",
      "Rollout time: 1.01 - Update time: 0.00\n",
      "\n",
      "Episode 39 - Total reward 3\n",
      "Loss: 0.4920\n",
      "Rollout time: 0.96 - Update time: 0.00\n",
      "\n",
      "Episode 40 - Total reward 1\n",
      "Values:  tensor([0.8874, 0.8830, 0.8833, 0.8830, 0.8827, 0.8838, 0.8838, 0.8833, 0.8830,\n",
      "        0.8833, 0.8830, 0.8833, 0.8846, 0.8833, 0.8833, 0.8838, 0.8838, 0.8838,\n",
      "        0.8838, 0.8827, 0.8827, 0.8827, 0.8827, 0.8760, 0.8827, 0.8827, 0.8827,\n",
      "        0.8838, 0.8838, 0.8827, 0.8838, 0.8827, 0.8838, 0.8838, 0.8827, 0.8827,\n",
      "        0.8830, 0.8819, 0.9387, 0.9398, 0.8758, 0.8758, 0.8758, 0.8753, 0.8758,\n",
      "        0.8760, 0.8758, 0.8758, 0.8753, 0.8758, 0.8758, 0.8760, 0.8758, 0.8758,\n",
      "        0.8753, 0.8758, 0.8760, 0.8758, 0.8760, 0.8760, 0.8749, 0.8760, 0.8760,\n",
      "        0.8758, 0.8753, 0.8758, 0.8758, 0.8760, 0.8760, 0.8758, 0.8758, 0.8758,\n",
      "        0.8753, 0.8758, 0.8760, 0.8760, 0.8758, 0.8758, 0.8760, 0.8758, 0.8758,\n",
      "        0.8753, 0.8766, 0.8753, 0.8758, 0.8753, 0.8758, 0.8760, 0.8705, 0.8760,\n",
      "        0.8758, 0.8758, 0.8760, 0.8758, 0.8753, 0.8749, 0.8760, 0.8758, 0.8758,\n",
      "        0.8760], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.8921, 0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138,\n",
      "        0.9166, 0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389,\n",
      "        0.9417, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646,\n",
      "        0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910,\n",
      "        0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.4707\n",
      "Rollout time: 0.87 - Update time: 0.00\n",
      "\n",
      "Episode 41 - Total reward 0\n",
      "Loss: 0.5690\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 42 - Total reward 3\n",
      "Loss: 0.1371\n",
      "Rollout time: 0.88 - Update time: 0.00\n",
      "\n",
      "Episode 43 - Total reward 2\n",
      "Loss: 0.1728\n",
      "Rollout time: 0.91 - Update time: 0.00\n",
      "\n",
      "Episode 44 - Total reward 1\n",
      "Loss: 0.2119\n",
      "Rollout time: 1.52 - Update time: 0.00\n",
      "\n",
      "Episode 45 - Total reward 7\n",
      "Loss: 0.3062\n",
      "Rollout time: 0.65 - Update time: 0.00\n",
      "\n",
      "Episode 46 - Total reward 6\n",
      "Loss: 0.2448\n",
      "Rollout time: 0.79 - Update time: 0.00\n",
      "\n",
      "Episode 47 - Total reward 3\n",
      "Loss: 0.1675\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 48 - Total reward 0\n",
      "Loss: 0.0860\n",
      "Rollout time: 1.56 - Update time: 0.00\n",
      "\n",
      "Episode 49 - Total reward 6\n",
      "Loss: 0.3413\n",
      "Rollout time: 0.75 - Update time: 0.00\n",
      "\n",
      "██████\n",
      "█  !n█\n",
      "█    █\n",
      "█@y  █\n",
      "█ ?  █\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.378 - Visit counts=8\n",
      "Action  Up : Q-value=-0.252 - Visit counts=10\n",
      "Action  Down : Q-value=-0.276 - Visit counts=12\n",
      "Action  Right : Q-value=-0.142 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  !n█\n",
      "█    █\n",
      "█ @  █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.499 - Visit counts=5\n",
      "Action  Up : Q-value=0.343 - Visit counts=42\n",
      "Action  Down : Q-value=-1.000 - Visit counts=2\n",
      "Action  Left : Q-value=-0.048 - Visit counts=12\n",
      "Action  Right : Q-value=-0.129 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  !n█\n",
      "█ @  █\n",
      "█    █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.166 - Visit counts=7\n",
      "Action  Up : Q-value=0.362 - Visit counts=13\n",
      "Action  Down : Q-value=0.213 - Visit counts=10\n",
      "Action  Left : Q-value=-0.988 - Visit counts=2\n",
      "Action  Right : Q-value=0.772 - Visit counts=59\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  !n█\n",
      "█  @ █\n",
      "█    █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.285 - Visit counts=6\n",
      "Action  Up : Q-value=1.000 - Visit counts=95\n",
      "Action  Down : Q-value=-0.994 - Visit counts=1\n",
      "Action  Left : Q-value=-0.377 - Visit counts=2\n",
      "Action  Right : Q-value=-0.072 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 1}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  @n█\n",
      "█    █\n",
      "█    █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.624 - Visit counts=10\n",
      "Action  Down : Q-value=-0.308 - Visit counts=36\n",
      "Action  Left : Q-value=-1.000 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n @█\n",
      "█    █\n",
      "█  y █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.274 - Visit counts=12\n",
      "Action  Up : Q-value=-0.993 - Visit counts=3\n",
      "Action  Down : Q-value=-0.005 - Visit counts=56\n",
      "Action  Left : Q-value=-0.290 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█   @█\n",
      "█  y █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.083 - Visit counts=23\n",
      "Action  Up : Q-value=-0.308 - Visit counts=9\n",
      "Action  Down : Q-value=0.060 - Visit counts=42\n",
      "Action  Left : Q-value=0.037 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█    █\n",
      "█  y@█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.075 - Visit counts=38\n",
      "Action  Up : Q-value=-0.253 - Visit counts=10\n",
      "Action  Left : Q-value=0.091 - Visit counts=43\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█    █\n",
      "█  @ █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.154 - Visit counts=31\n",
      "Action  Up : Q-value=-0.099 - Visit counts=11\n",
      "Action  Left : Q-value=0.050 - Visit counts=20\n",
      "Action  Right : Q-value=0.151 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█    █\n",
      "█  @ █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.105 - Visit counts=19\n",
      "Action  Up : Q-value=0.097 - Visit counts=18\n",
      "Action  Left : Q-value=0.162 - Visit counts=24\n",
      "Action  Right : Q-value=0.040 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█    █\n",
      "█ @  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.128 - Visit counts=16\n",
      "Action  Up : Q-value=0.070 - Visit counts=15\n",
      "Action  Left : Q-value=0.212 - Visit counts=23\n",
      "Action  Right : Q-value=0.168 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█    █\n",
      "█@   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.266 - Visit counts=31\n",
      "Action  Up : Q-value=0.177 - Visit counts=17\n",
      "Action  Right : Q-value=0.213 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█    █\n",
      "█@   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.111 - Visit counts=20\n",
      "Action  Up : Q-value=0.197 - Visit counts=31\n",
      "Action  Right : Q-value=0.191 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█ n  █\n",
      "█@   █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.181 - Visit counts=9\n",
      "Action  Up : Q-value=0.694 - Visit counts=52\n",
      "Action  Down : Q-value=0.291 - Visit counts=13\n",
      "Action  Right : Q-value=-0.174 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ !? █\n",
      "█@n  █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.001 - Visit counts=4\n",
      "Action  Up : Q-value=0.920 - Visit counts=87\n",
      "Action  Down : Q-value=0.176 - Visit counts=8\n",
      "Action  Right : Q-value=-0.379 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██████\n",
      "█@!? █\n",
      "█ n  █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.884 - Visit counts=41\n",
      "Action  Down : Q-value=-0.380 - Visit counts=2\n",
      "Action  Right : Q-value=1.000 - Visit counts=93\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ @? █\n",
      "█ n  █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.550 - Visit counts=14\n",
      "Action  Down : Q-value=-1.000 - Visit counts=4\n",
      "Action  Left : Q-value=-0.373 - Visit counts=26\n",
      "Action  Right : Q-value=-0.785 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.490 - Visit counts=42\n",
      "Action  Down : Q-value=-0.784 - Visit counts=12\n",
      "Action  Right : Q-value=-0.639 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.424 - Visit counts=50\n",
      "Action  Down : Q-value=-0.564 - Visit counts=23\n",
      "Action  Right : Q-value=-0.647 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.464 - Visit counts=21\n",
      "Action  Down : Q-value=-0.238 - Visit counts=62\n",
      "Action  Right : Q-value=-0.528 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 6}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█@! ?█\n",
      "█   y█\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.376 - Visit counts=22\n",
      "Action  Up : Q-value=-0.515 - Visit counts=13\n",
      "Action  Down : Q-value=-0.173 - Visit counts=72\n",
      "Action  Right : Q-value=-1.000 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 6}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█@  y█\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.146 - Visit counts=45\n",
      "Action  Up : Q-value=-0.501 - Visit counts=10\n",
      "Action  Down : Q-value=-0.160 - Visit counts=34\n",
      "Action  Right : Q-value=-0.186 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█@  y█\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.322 - Visit counts=11\n",
      "Action  Up : Q-value=-0.224 - Visit counts=16\n",
      "Action  Down : Q-value=0.007 - Visit counts=43\n",
      "Action  Right : Q-value=-0.122 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█@n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.037 - Visit counts=27\n",
      "Action  Up : Q-value=-0.026 - Visit counts=30\n",
      "Action  Right : Q-value=-0.011 - Visit counts=35\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.151 - Visit counts=19\n",
      "Action  Up : Q-value=-0.127 - Visit counts=17\n",
      "Action  Left : Q-value=-0.059 - Visit counts=25\n",
      "Action  Right : Q-value=-0.085 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█@   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.056 - Visit counts=38\n",
      "Action  Up : Q-value=-0.376 - Visit counts=12\n",
      "Action  Right : Q-value=-0.173 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█@   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.109 - Visit counts=32\n",
      "Action  Up : Q-value=-0.289 - Visit counts=14\n",
      "Action  Right : Q-value=-0.035 - Visit counts=41\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.029 - Visit counts=28\n",
      "Action  Up : Q-value=-0.224 - Visit counts=16\n",
      "Action  Left : Q-value=-0.115 - Visit counts=21\n",
      "Action  Right : Q-value=-0.108 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.024 - Visit counts=38\n",
      "Action  Up : Q-value=-0.583 - Visit counts=6\n",
      "Action  Left : Q-value=-0.225 - Visit counts=16\n",
      "Action  Right : Q-value=-0.197 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.020 - Visit counts=29\n",
      "Action  Up : Q-value=-0.237 - Visit counts=13\n",
      "Action  Left : Q-value=-0.196 - Visit counts=17\n",
      "Action  Right : Q-value=-0.046 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.254 - Visit counts=10\n",
      "Action  Up : Q-value=-0.687 - Visit counts=4\n",
      "Action  Left : Q-value=0.048 - Visit counts=33\n",
      "Action  Right : Q-value=0.075 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█  @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.056 - Visit counts=33\n",
      "Action  Up : Q-value=-0.174 - Visit counts=12\n",
      "Action  Left : Q-value=-0.092 - Visit counts=15\n",
      "Action  Right : Q-value=-0.072 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█  @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.152 - Visit counts=19\n",
      "Action  Up : Q-value=-0.316 - Visit counts=12\n",
      "Action  Left : Q-value=-0.155 - Visit counts=22\n",
      "Action  Right : Q-value=-0.068 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█   @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.009 - Visit counts=39\n",
      "Action  Up : Q-value=-0.045 - Visit counts=26\n",
      "Action  Left : Q-value=-0.237 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   y█\n",
      "█   @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.272 - Visit counts=13\n",
      "Action  Up : Q-value=0.111 - Visit counts=59\n",
      "Action  Left : Q-value=-0.071 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 6}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█   @█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.085 - Visit counts=13\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Down : Q-value=0.151 - Visit counts=29\n",
      "Action  Left : Q-value=0.558 - Visit counts=64\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█  @ █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.270 - Visit counts=8\n",
      "Action  Up : Q-value=0.049 - Visit counts=5\n",
      "Action  Down : Q-value=-0.048 - Visit counts=6\n",
      "Action  Left : Q-value=0.838 - Visit counts=90\n",
      "Action  Right : Q-value=-0.074 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ! ?█\n",
      "█ @  █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.680 - Visit counts=17\n",
      "Action  Up : Q-value=1.000 - Visit counts=102\n",
      "Action  Down : Q-value=0.547 - Visit counts=12\n",
      "Action  Left : Q-value=0.233 - Visit counts=6\n",
      "Action  Right : Q-value=-0.376 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ @ ?█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.096 - Visit counts=22\n",
      "Action  Down : Q-value=-0.463 - Visit counts=7\n",
      "Action  Left : Q-value=-0.583 - Visit counts=6\n",
      "Action  Right : Q-value=-0.169 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.215 - Visit counts=19\n",
      "Action  Down : Q-value=-0.328 - Visit counts=13\n",
      "Action  Left : Q-value=-0.532 - Visit counts=8\n",
      "Action  Right : Q-value=-0.078 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.015 - Visit counts=29\n",
      "Action  Down : Q-value=-0.098 - Visit counts=22\n",
      "Action  Left : Q-value=-0.201 - Visit counts=14\n",
      "Action  Right : Q-value=-0.170 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.152 - Visit counts=22\n",
      "Action  Down : Q-value=-0.097 - Visit counts=22\n",
      "Action  Left : Q-value=-0.288 - Visit counts=14\n",
      "Action  Right : Q-value=-0.128 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.528 - Visit counts=8\n",
      "Action  Up : Q-value=-0.144 - Visit counts=32\n",
      "Action  Down : Q-value=-0.331 - Visit counts=13\n",
      "Action  Left : Q-value=-0.499 - Visit counts=10\n",
      "Action  Right : Q-value=-0.528 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.237 - Visit counts=18\n",
      "Action  Down : Q-value=-0.170 - Visit counts=21\n",
      "Action  Left : Q-value=-0.170 - Visit counts=24\n",
      "Action  Right : Q-value=-0.236 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.442 - Visit counts=9\n",
      "Action  Down : Q-value=-0.098 - Visit counts=22\n",
      "Action  Left : Q-value=-0.458 - Visit counts=6\n",
      "Action  Right : Q-value=0.001 - Visit counts=36\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.169 - Visit counts=21\n",
      "Action  Down : Q-value=-0.155 - Visit counts=25\n",
      "Action  Left : Q-value=-0.187 - Visit counts=23\n",
      "Action  Right : Q-value=-0.296 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.377 - Visit counts=6\n",
      "Action  Up : Q-value=-0.142 - Visit counts=16\n",
      "Action  Down : Q-value=-0.150 - Visit counts=16\n",
      "Action  Left : Q-value=-0.209 - Visit counts=11\n",
      "Action  Right : Q-value=-0.008 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.212 - Visit counts=19\n",
      "Action  Up : Q-value=-0.170 - Visit counts=24\n",
      "Action  Down : Q-value=-1.000 - Visit counts=3\n",
      "Action  Left : Q-value=-0.070 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.578 - Visit counts=6\n",
      "Action  Up : Q-value=-0.234 - Visit counts=13\n",
      "Action  Down : Q-value=-0.203 - Visit counts=14\n",
      "Action  Left : Q-value=-0.195 - Visit counts=17\n",
      "Action  Right : Q-value=-0.034 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.188 - Visit counts=23\n",
      "Action  Up : Q-value=-0.105 - Visit counts=25\n",
      "Action  Down : Q-value=-1.000 - Visit counts=3\n",
      "Action  Left : Q-value=-0.155 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.251 - Visit counts=20\n",
      "Action  Down : Q-value=-0.375 - Visit counts=16\n",
      "Action  Left : Q-value=-0.148 - Visit counts=38\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.110 - Visit counts=28\n",
      "Action  Down : Q-value=-0.092 - Visit counts=26\n",
      "Action  Left : Q-value=-0.289 - Visit counts=14\n",
      "Action  Right : Q-value=-0.212 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.464 - Visit counts=7\n",
      "Action  Up : Q-value=-0.058 - Visit counts=33\n",
      "Action  Down : Q-value=-0.582 - Visit counts=6\n",
      "Action  Left : Q-value=-0.526 - Visit counts=8\n",
      "Action  Right : Q-value=-0.171 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.284 - Visit counts=14\n",
      "Action  Down : Q-value=-0.070 - Visit counts=32\n",
      "Action  Left : Q-value=-0.190 - Visit counts=20\n",
      "Action  Right : Q-value=-0.220 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.253 - Visit counts=15\n",
      "Action  Up : Q-value=-0.068 - Visit counts=24\n",
      "Action  Down : Q-value=-0.205 - Visit counts=14\n",
      "Action  Left : Q-value=-0.253 - Visit counts=15\n",
      "Action  Right : Q-value=-0.235 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.148 - Visit counts=19\n",
      "Action  Down : Q-value=-0.377 - Visit counts=10\n",
      "Action  Left : Q-value=-0.318 - Visit counts=11\n",
      "Action  Right : Q-value=-0.022 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.206 - Visit counts=33\n",
      "Action  Down : Q-value=-0.637 - Visit counts=7\n",
      "Action  Left : Q-value=-0.140 - Visit counts=42\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.170 - Visit counts=27\n",
      "Action  Down : Q-value=-0.203 - Visit counts=25\n",
      "Action  Left : Q-value=-0.142 - Visit counts=29\n",
      "Action  Right : Q-value=-0.494 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.542 - Visit counts=11\n",
      "Action  Down : Q-value=-0.782 - Visit counts=6\n",
      "Action  Left : Q-value=-0.502 - Visit counts=10\n",
      "Action  Right : Q-value=-0.146 - Visit counts=51\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.138 - Visit counts=26\n",
      "Action  Down : Q-value=-0.145 - Visit counts=32\n",
      "Action  Left : Q-value=-0.335 - Visit counts=15\n",
      "Action  Right : Q-value=-0.169 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.429 - Visit counts=11\n",
      "Action  Down : Q-value=-0.215 - Visit counts=27\n",
      "Action  Left : Q-value=-0.430 - Visit counts=11\n",
      "Action  Right : Q-value=-0.232 - Visit counts=26\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.288 - Visit counts=14\n",
      "Action  Up : Q-value=-0.089 - Visit counts=30\n",
      "Action  Down : Q-value=-0.380 - Visit counts=10\n",
      "Action  Left : Q-value=-0.318 - Visit counts=11\n",
      "Action  Right : Q-value=-0.317 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.633 - Visit counts=7\n",
      "Action  Down : Q-value=-0.172 - Visit counts=27\n",
      "Action  Left : Q-value=-0.170 - Visit counts=30\n",
      "Action  Right : Q-value=-0.334 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.430 - Visit counts=11\n",
      "Action  Down : Q-value=-0.640 - Visit counts=7\n",
      "Action  Left : Q-value=-0.482 - Visit counts=12\n",
      "Action  Right : Q-value=-0.135 - Visit counts=49\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.186 - Visit counts=23\n",
      "Action  Down : Q-value=-0.741 - Visit counts=5\n",
      "Action  Left : Q-value=-0.077 - Visit counts=39\n",
      "Action  Right : Q-value=-0.076 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.336 - Visit counts=17\n",
      "Action  Down : Q-value=-0.243 - Visit counts=28\n",
      "Action  Left : Q-value=-0.180 - Visit counts=35\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.226 - Visit counts=21\n",
      "Action  Down : Q-value=-0.268 - Visit counts=17\n",
      "Action  Left : Q-value=-0.206 - Visit counts=22\n",
      "Action  Right : Q-value=-0.219 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.170 - Visit counts=27\n",
      "Action  Down : Q-value=-0.745 - Visit counts=5\n",
      "Action  Left : Q-value=-0.434 - Visit counts=11\n",
      "Action  Right : Q-value=-0.198 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.187 - Visit counts=26\n",
      "Action  Down : Q-value=-0.431 - Visit counts=11\n",
      "Action  Left : Q-value=-0.435 - Visit counts=11\n",
      "Action  Right : Q-value=-0.155 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.085 - Visit counts=34\n",
      "Action  Down : Q-value=-0.299 - Visit counts=16\n",
      "Action  Left : Q-value=-0.170 - Visit counts=24\n",
      "Action  Right : Q-value=-0.991 - Visit counts=3\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.141 - Visit counts=42\n",
      "Action  Down : Q-value=-0.477 - Visit counts=12\n",
      "Action  Left : Q-value=-0.376 - Visit counts=14\n",
      "Action  Right : Q-value=-0.332 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.018 - Visit counts=43\n",
      "Action  Down : Q-value=-0.442 - Visit counts=9\n",
      "Action  Left : Q-value=-0.155 - Visit counts=25\n",
      "Action  Right : Q-value=-0.285 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.155 - Visit counts=28\n",
      "Action  Down : Q-value=-0.430 - Visit counts=11\n",
      "Action  Left : Q-value=-0.170 - Visit counts=24\n",
      "Action  Right : Q-value=-0.141 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.239 - Visit counts=23\n",
      "Action  Down : Q-value=-0.338 - Visit counts=17\n",
      "Action  Left : Q-value=-0.115 - Visit counts=38\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.341 - Visit counts=19\n",
      "Action  Down : Q-value=-0.188 - Visit counts=23\n",
      "Action  Left : Q-value=-0.374 - Visit counts=16\n",
      "Action  Right : Q-value=-0.225 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.070 - Visit counts=20\n",
      "Action  Up : Q-value=-0.142 - Visit counts=16\n",
      "Action  Down : Q-value=-0.073 - Visit counts=16\n",
      "Action  Left : Q-value=-0.684 - Visit counts=4\n",
      "Action  Right : Q-value=-0.145 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.376 - Visit counts=10\n",
      "Action  Up : Q-value=-0.051 - Visit counts=38\n",
      "Action  Down : Q-value=-0.584 - Visit counts=6\n",
      "Action  Left : Q-value=-0.581 - Visit counts=6\n",
      "Action  Right : Q-value=-0.445 - Visit counts=9\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.267 - Visit counts=17\n",
      "Action  Down : Q-value=-0.154 - Visit counts=25\n",
      "Action  Left : Q-value=-0.112 - Visit counts=28\n",
      "Action  Right : Q-value=-0.265 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.107 - Visit counts=32\n",
      "Action  Down : Q-value=-0.156 - Visit counts=25\n",
      "Action  Left : Q-value=-0.990 - Visit counts=3\n",
      "Action  Right : Q-value=-0.264 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y@  █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.577 - Visit counts=9\n",
      "Action  Down : Q-value=-0.479 - Visit counts=12\n",
      "Action  Left : Q-value=-0.640 - Visit counts=7\n",
      "Action  Right : Q-value=-0.131 - Visit counts=53\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.182 - Visit counts=29\n",
      "Action  Down : Q-value=-0.085 - Visit counts=38\n",
      "Action  Left : Q-value=-0.432 - Visit counts=11\n",
      "Action  Right : Q-value=-0.219 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.443 - Visit counts=9\n",
      "Action  Up : Q-value=-0.050 - Visit counts=34\n",
      "Action  Down : Q-value=-0.195 - Visit counts=20\n",
      "Action  Left : Q-value=-0.444 - Visit counts=9\n",
      "Action  Right : Q-value=-0.253 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.095 - Visit counts=22\n",
      "Action  Down : Q-value=-0.223 - Visit counts=16\n",
      "Action  Left : Q-value=-0.318 - Visit counts=11\n",
      "Action  Right : Q-value=-0.014 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.093 - Visit counts=37\n",
      "Action  Down : Q-value=-0.251 - Visit counts=20\n",
      "Action  Left : Q-value=-0.184 - Visit counts=26\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.116 - Visit counts=38\n",
      "Action  Down : Q-value=-0.748 - Visit counts=5\n",
      "Action  Left : Q-value=-0.074 - Visit counts=43\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.091 - Visit counts=37\n",
      "Action  Down : Q-value=-0.268 - Visit counts=17\n",
      "Action  Left : Q-value=-0.267 - Visit counts=17\n",
      "Action  Right : Q-value=-0.170 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.020 - Visit counts=38\n",
      "Action  Down : Q-value=-0.251 - Visit counts=15\n",
      "Action  Left : Q-value=-0.151 - Visit counts=22\n",
      "Action  Right : Q-value=-0.317 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.185 - Visit counts=26\n",
      "Action  Down : Q-value=-0.747 - Visit counts=5\n",
      "Action  Left : Q-value=-0.171 - Visit counts=24\n",
      "Action  Right : Q-value=-0.143 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.181 - Visit counts=32\n",
      "Action  Down : Q-value=-0.637 - Visit counts=7\n",
      "Action  Left : Q-value=-0.111 - Visit counts=42\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.158 - Visit counts=34\n",
      "Action  Down : Q-value=-0.264 - Visit counts=22\n",
      "Action  Left : Q-value=-0.637 - Visit counts=7\n",
      "Action  Right : Q-value=-0.198 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.132 - Visit counts=23\n",
      "Action  Down : Q-value=-0.171 - Visit counts=18\n",
      "Action  Left : Q-value=-0.156 - Visit counts=20\n",
      "Action  Right : Q-value=-0.097 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.076 - Visit counts=39\n",
      "Action  Down : Q-value=-0.328 - Visit counts=13\n",
      "Action  Left : Q-value=-0.212 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.062 - Visit counts=49\n",
      "Action  Down : Q-value=-0.376 - Visit counts=14\n",
      "Action  Left : Q-value=-0.203 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.157 - Visit counts=31\n",
      "Action  Down : Q-value=-0.375 - Visit counts=12\n",
      "Action  Left : Q-value=-0.051 - Visit counts=55\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.090 - Visit counts=26\n",
      "Action  Down : Q-value=-0.042 - Visit counts=35\n",
      "Action  Left : Q-value=-0.254 - Visit counts=15\n",
      "Action  Right : Q-value=-0.066 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.092 - Visit counts=26\n",
      "Action  Up : Q-value=-0.188 - Visit counts=20\n",
      "Action  Down : Q-value=-0.448 - Visit counts=9\n",
      "Action  Left : Q-value=-0.120 - Visit counts=24\n",
      "Action  Right : Q-value=-0.743 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.170 - Visit counts=18\n",
      "Action  Up : Q-value=-0.374 - Visit counts=10\n",
      "Action  Down : Q-value=-0.254 - Visit counts=15\n",
      "Action  Left : Q-value=-0.289 - Visit counts=14\n",
      "Action  Right : Q-value=-0.170 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.272 - Visit counts=12\n",
      "Action  Up : Q-value=-0.068 - Visit counts=24\n",
      "Action  Down : Q-value=-1.000 - Visit counts=3\n",
      "Action  Left : Q-value=0.019 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.274 - Visit counts=12\n",
      "Action  Up : Q-value=-0.169 - Visit counts=18\n",
      "Action  Down : Q-value=-0.292 - Visit counts=14\n",
      "Action  Left : Q-value=-0.272 - Visit counts=12\n",
      "Action  Right : Q-value=-0.170 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.188 - Visit counts=20\n",
      "Action  Down : Q-value=-0.443 - Visit counts=9\n",
      "Action  Left : Q-value=-0.252 - Visit counts=15\n",
      "Action  Right : Q-value=-0.134 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█  n!█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      "Episode 50 - Total reward 3\n",
      "Values:  tensor([0.2433, 0.2383, 0.2383, 0.2383, 0.2383, 0.2449, 0.2451, 0.2440, 0.2383,\n",
      "        0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2443,\n",
      "        0.2443, 0.2443, 0.2448, 0.2435, 0.2435, 0.2406, 0.2383, 0.2383, 0.2383,\n",
      "        0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383, 0.2383,\n",
      "        0.2383, 0.2383, 0.2383, 0.2453, 0.2470, 0.2470, 0.2488, 0.2470, 0.2453,\n",
      "        0.2470, 0.2488, 0.2462, 0.2488, 0.2462, 0.2458, 0.2470, 0.2488, 0.2470,\n",
      "        0.2488, 0.2470, 0.2458, 0.2470, 0.2453, 0.2470, 0.2470, 0.2488, 0.2470,\n",
      "        0.2453, 0.2470, 0.2458, 0.2470, 0.2453, 0.2453, 0.2470, 0.2470, 0.2470,\n",
      "        0.2470, 0.2458, 0.2470, 0.2488, 0.2488, 0.2470, 0.2453, 0.2453, 0.2470,\n",
      "        0.2488, 0.2470, 0.2458, 0.2458, 0.2470, 0.2470, 0.2470, 0.2458, 0.2470,\n",
      "        0.2470, 0.2458, 0.2458, 0.2458, 0.2470, 0.2488, 0.2488, 0.2462, 0.2488,\n",
      "        0.2470], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9910, 0.9940, 0.9970, 1.0000, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792,\n",
      "        0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9389, 0.9417,\n",
      "        0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2441\n",
      "Rollout time: 0.95 - Update time: 0.00\n",
      "\n",
      "Episode 51 - Total reward 6\n",
      "Loss: 0.2319\n",
      "Rollout time: 0.96 - Update time: 0.00\n",
      "\n",
      "Episode 52 - Total reward 4\n",
      "Loss: 0.4425\n",
      "Rollout time: 0.83 - Update time: 0.00\n",
      "\n",
      "Episode 53 - Total reward 0\n",
      "Loss: 0.0541\n",
      "Rollout time: 1.52 - Update time: 0.00\n",
      "\n",
      "Episode 54 - Total reward 3\n",
      "Loss: 0.4144\n",
      "Rollout time: 0.94 - Update time: 0.00\n",
      "\n",
      "Episode 55 - Total reward 7\n",
      "Loss: 0.4688\n",
      "Rollout time: 0.63 - Update time: 0.00\n",
      "\n",
      "Episode 56 - Total reward 1\n",
      "Loss: 0.1614\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 57 - Total reward 4\n",
      "Loss: 0.2815\n",
      "Rollout time: 0.72 - Update time: 0.00\n",
      "\n",
      "Episode 58 - Total reward 0\n",
      "Loss: 0.1516\n",
      "Rollout time: 1.32 - Update time: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 59 - Total reward 4\n",
      "Loss: 0.2368\n",
      "Rollout time: 0.82 - Update time: 0.00\n",
      "\n",
      "Episode 60 - Total reward 4\n",
      "Values:  tensor([0.4170, 0.4170, 0.4115, 0.4079, 0.4120, 0.4120, 0.4079, 0.4120, 0.4079,\n",
      "        0.4120, 0.4120, 0.4139, 0.4120, 0.4120, 0.4139, 0.4167, 0.4139, 0.4139,\n",
      "        0.4120, 0.4079, 0.4079, 0.4079, 0.4079, 0.4079, 0.4079, 0.4079, 0.4079,\n",
      "        0.4079, 0.4120, 0.4120, 0.4079, 0.4079, 0.4120, 0.4139, 0.4139, 0.3961,\n",
      "        0.3961, 0.3961, 0.3961, 0.3961, 0.4168, 0.4185, 0.4182, 0.3961, 0.3961,\n",
      "        0.3961, 0.4089, 0.3961, 0.3961, 0.3961, 0.4134, 0.4127, 0.3961, 0.3961,\n",
      "        0.3961, 0.4093, 0.4065, 0.4065, 0.4065, 0.4065, 0.4065, 0.4093, 0.4065,\n",
      "        0.4070, 0.4150, 0.4070, 0.4065, 0.4093, 0.4065, 0.4065, 0.4065, 0.4093,\n",
      "        0.4093, 0.4150, 0.4093, 0.4093, 0.4065, 0.4070, 0.4070, 0.4065, 0.4070,\n",
      "        0.4065, 0.4093, 0.4093, 0.4065, 0.4093, 0.4065, 0.4093, 0.4065, 0.4065,\n",
      "        0.4065, 0.4065, 0.4065, 0.4093, 0.4065, 0.4093, 0.4065, 0.4065, 0.4070,\n",
      "        0.4065], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.8921, 0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138,\n",
      "        0.9166, 0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389,\n",
      "        0.9417, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646,\n",
      "        0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910,\n",
      "        0.9940, 0.9970, 1.0000, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2410\n",
      "Rollout time: 0.77 - Update time: 0.00\n",
      "\n",
      "Episode 61 - Total reward 3\n",
      "Loss: 0.1947\n",
      "Rollout time: 0.95 - Update time: 0.00\n",
      "\n",
      "Episode 62 - Total reward 0\n",
      "Loss: 0.1658\n",
      "Rollout time: 1.52 - Update time: 0.00\n",
      "\n",
      "Episode 63 - Total reward 2\n",
      "Loss: 0.1974\n",
      "Rollout time: 1.22 - Update time: 0.00\n",
      "\n",
      "Episode 64 - Total reward 3\n",
      "Loss: 0.1701\n",
      "Rollout time: 1.34 - Update time: 0.00\n",
      "\n",
      "Episode 65 - Total reward 4\n",
      "Loss: 0.2024\n",
      "Rollout time: 1.12 - Update time: 0.00\n",
      "\n",
      "Episode 66 - Total reward 1\n",
      "Loss: 0.1491\n",
      "Rollout time: 1.31 - Update time: 0.00\n",
      "\n",
      "Episode 67 - Total reward 10\n",
      "Loss: 0.4882\n",
      "Rollout time: 0.60 - Update time: 0.00\n",
      "\n",
      "Episode 68 - Total reward 8\n",
      "Loss: 0.4984\n",
      "Rollout time: 0.58 - Update time: 0.00\n",
      "\n",
      "Episode 69 - Total reward 11\n",
      "Loss: 0.5355\n",
      "Rollout time: 0.58 - Update time: 0.00\n",
      "\n",
      "Episode 70 - Total reward 2\n",
      "Values:  tensor([0.2823, 0.2828, 0.2828, 0.2828, 0.2828, 0.2828, 0.2830, 0.2828, 0.2823,\n",
      "        0.2823, 0.2828, 0.2830, 0.2828, 0.2828, 0.2828, 0.2841, 0.2841, 0.2829,\n",
      "        0.2823, 0.2823, 0.2828, 0.2828, 0.2841, 0.2828, 0.2830, 0.2828, 0.2823,\n",
      "        0.2828, 0.2823, 0.2823, 0.2823, 0.2823, 0.2823, 0.2828, 0.2828, 0.2828,\n",
      "        0.2830, 0.2828, 0.2830, 0.2830, 0.2828, 0.2828, 0.2828, 0.2828, 0.2823,\n",
      "        0.2828, 0.2828, 0.2828, 0.2828, 0.2830, 0.2830, 0.2828, 0.2830, 0.2830,\n",
      "        0.2828, 0.2823, 0.2828, 0.2830, 0.2830, 0.2804, 0.2830, 0.2828, 0.2823,\n",
      "        0.2828, 0.2828, 0.2830, 0.2804, 0.2804, 0.3058, 0.3058, 0.3058, 0.3058,\n",
      "        0.3058, 0.3058, 0.3058, 0.2797, 0.3058, 0.3058, 0.3058, 0.3058, 0.3058,\n",
      "        0.3058, 0.3058, 0.3058, 0.3058, 0.3058, 0.2839, 0.2825, 0.2839, 0.2825,\n",
      "        0.2825, 0.2825, 0.2841, 0.2841, 0.2825, 0.2839, 0.2839, 0.2825, 0.2839,\n",
      "        0.2839], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.8031, 0.8055, 0.8079, 0.8103, 0.8128, 0.8152, 0.8177, 0.8201, 0.8226,\n",
      "        0.8251, 0.8276, 0.8300, 0.8325, 0.8350, 0.8376, 0.8401, 0.8426, 0.8451,\n",
      "        0.8477, 0.8502, 0.8528, 0.8554, 0.8579, 0.8605, 0.8631, 0.8657, 0.8683,\n",
      "        0.8709, 0.8735, 0.8762, 0.8788, 0.8814, 0.8841, 0.8868, 0.8894, 0.8921,\n",
      "        0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138, 0.9166,\n",
      "        0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389, 0.9417,\n",
      "        0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.3440\n",
      "Rollout time: 0.87 - Update time: 0.00\n",
      "\n",
      "Episode 71 - Total reward 3\n",
      "Loss: 0.1601\n",
      "Rollout time: 0.98 - Update time: 0.00\n",
      "\n",
      "Episode 72 - Total reward 0\n",
      "Loss: 0.1618\n",
      "Rollout time: 1.16 - Update time: 0.00\n",
      "\n",
      "Episode 73 - Total reward 0\n",
      "Loss: 0.1771\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 74 - Total reward 1\n",
      "Loss: 0.1820\n",
      "Rollout time: 1.50 - Update time: 0.00\n",
      "\n",
      "Episode 75 - Total reward 0\n",
      "Loss: 0.1600\n",
      "Rollout time: 1.53 - Update time: 0.00\n",
      "\n",
      "Episode 76 - Total reward 0\n",
      "Loss: 0.1351\n",
      "Rollout time: 1.23 - Update time: 0.00\n",
      "\n",
      "Episode 77 - Total reward 6\n",
      "Loss: 0.3968\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 78 - Total reward 9\n",
      "Loss: 0.4039\n",
      "Rollout time: 0.58 - Update time: 0.00\n",
      "\n",
      "Episode 79 - Total reward 4\n",
      "Loss: 0.2109\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 80 - Total reward 1\n",
      "Values:  tensor([0.3155, 0.3161, 0.3161, 0.3161, 0.3161, 0.3161, 0.3161, 0.3154, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3154, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3154, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3154, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157, 0.3157,\n",
      "        0.3157], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.1211\n",
      "Rollout time: 1.07 - Update time: 0.00\n",
      "\n",
      "Episode 81 - Total reward 12\n",
      "Loss: 0.3770\n",
      "Rollout time: 0.55 - Update time: 0.00\n",
      "\n",
      "Episode 82 - Total reward 3\n",
      "Loss: 0.2001\n",
      "Rollout time: 0.87 - Update time: 0.00\n",
      "\n",
      "Episode 83 - Total reward 1\n",
      "Loss: 0.1220\n",
      "Rollout time: 1.09 - Update time: 0.00\n",
      "\n",
      "Episode 84 - Total reward 5\n",
      "Loss: 0.2821\n",
      "Rollout time: 0.81 - Update time: 0.00\n",
      "\n",
      "Episode 85 - Total reward 1\n",
      "Loss: 0.2646\n",
      "Rollout time: 0.95 - Update time: 0.00\n",
      "\n",
      "Episode 86 - Total reward 3\n",
      "Loss: 0.2587\n",
      "Rollout time: 0.95 - Update time: 0.00\n",
      "\n",
      "Episode 87 - Total reward 4\n",
      "Loss: 0.1816\n",
      "Rollout time: 1.18 - Update time: 0.00\n",
      "\n",
      "Episode 88 - Total reward 4\n",
      "Loss: 0.3036\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 89 - Total reward 2\n",
      "Loss: 0.2363\n",
      "Rollout time: 0.89 - Update time: 0.00\n",
      "\n",
      "Episode 90 - Total reward 7\n",
      "Values:  tensor([0.4350, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4351,\n",
      "        0.4349, 0.4349, 0.4377, 0.4377, 0.4377, 0.4377, 0.4350, 0.4350, 0.4350,\n",
      "        0.4350, 0.4350, 0.4363, 0.4346, 0.4363, 0.4350, 0.4350, 0.4350, 0.4377,\n",
      "        0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4346, 0.4346, 0.4345, 0.4345,\n",
      "        0.4363, 0.4345, 0.4345, 0.4345, 0.4363, 0.4346, 0.4345, 0.4346, 0.4346,\n",
      "        0.4355, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377,\n",
      "        0.4377, 0.4345, 0.4345, 0.4345, 0.4345, 0.4345, 0.4377, 0.4377, 0.4377,\n",
      "        0.4377, 0.4373, 0.4376, 0.4376, 0.4374, 0.4376, 0.4374, 0.4376, 0.4374,\n",
      "        0.4376, 0.4376, 0.4376, 0.4373, 0.4376, 0.4376, 0.4373, 0.4373, 0.4376,\n",
      "        0.4374, 0.4376, 0.4376, 0.4374, 0.4376, 0.4376, 0.4376, 0.4374, 0.4377,\n",
      "        0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377, 0.4377,\n",
      "        0.4377], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9910, 0.9940, 0.9970, 1.0000, 0.9940, 0.9970, 1.0000, 0.9821, 0.9851,\n",
      "        0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9531, 0.9559, 0.9588, 0.9617,\n",
      "        0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646,\n",
      "        0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910,\n",
      "        0.9940, 0.9970, 1.0000, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2596\n",
      "Rollout time: 0.68 - Update time: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 91 - Total reward 3\n",
      "Loss: 0.2317\n",
      "Rollout time: 0.86 - Update time: 0.00\n",
      "\n",
      "Episode 92 - Total reward 2\n",
      "Loss: 0.2594\n",
      "Rollout time: 0.94 - Update time: 0.00\n",
      "\n",
      "Episode 93 - Total reward 5\n",
      "Loss: 0.1561\n",
      "Rollout time: 0.81 - Update time: 0.00\n",
      "\n",
      "Episode 94 - Total reward 0\n",
      "Loss: 0.3669\n",
      "Rollout time: 1.14 - Update time: 0.00\n",
      "\n",
      "Episode 95 - Total reward 0\n",
      "Loss: 0.3712\n",
      "Rollout time: 1.14 - Update time: 0.00\n",
      "\n",
      "Episode 96 - Total reward 0\n",
      "Loss: 0.3294\n",
      "Rollout time: 1.54 - Update time: 0.00\n",
      "\n",
      "Episode 97 - Total reward 4\n",
      "Loss: 0.2350\n",
      "Rollout time: 0.80 - Update time: 0.00\n",
      "\n",
      "Episode 98 - Total reward 3\n",
      "Loss: 0.1833\n",
      "Rollout time: 1.37 - Update time: 0.00\n",
      "\n",
      "Episode 99 - Total reward 2\n",
      "Loss: 0.1943\n",
      "Rollout time: 1.01 - Update time: 0.00\n",
      "\n",
      "██████\n",
      "█   n█\n",
      "█ y  █\n",
      "█  !?█\n",
      "█@   █\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.052 - Visit counts=23\n",
      "Action  Up : Q-value=0.099 - Visit counts=22\n",
      "Action  Right : Q-value=-0.442 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   n█\n",
      "█ y  █\n",
      "█@ !?█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.041 - Visit counts=12\n",
      "Action  Up : Q-value=0.330 - Visit counts=39\n",
      "Action  Down : Q-value=-0.130 - Visit counts=8\n",
      "Action  Right : Q-value=-0.023 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   n█\n",
      "█@y  █\n",
      "█  !?█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.987 - Visit counts=2\n",
      "Action  Up : Q-value=0.302 - Visit counts=19\n",
      "Action  Down : Q-value=0.310 - Visit counts=19\n",
      "Action  Right : Q-value=0.471 - Visit counts=48\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   n█\n",
      "█ @  █\n",
      "█  !?█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.465 - Visit counts=13\n",
      "Action  Up : Q-value=0.434 - Visit counts=12\n",
      "Action  Down : Q-value=0.791 - Visit counts=58\n",
      "Action  Left : Q-value=0.367 - Visit counts=9\n",
      "Action  Right : Q-value=-0.043 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█   n█\n",
      "█    █\n",
      "█ @!?█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.654 - Visit counts=13\n",
      "Action  Up : Q-value=0.518 - Visit counts=9\n",
      "Action  Down : Q-value=0.534 - Visit counts=11\n",
      "Action  Left : Q-value=0.601 - Visit counts=11\n",
      "Action  Right : Q-value=1.000 - Visit counts=63\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   n█\n",
      "█    █\n",
      "█  @?█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.122 - Visit counts=21\n",
      "Action  Up : Q-value=-0.301 - Visit counts=8\n",
      "Action  Left : Q-value=0.056 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   n█\n",
      "█!?  █\n",
      "█y  @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.009 - Visit counts=22\n",
      "Action  Up : Q-value=0.148 - Visit counts=35\n",
      "Action  Left : Q-value=-0.144 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   n█\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.231 - Visit counts=27\n",
      "Action  Up : Q-value=0.322 - Visit counts=45\n",
      "Action  Down : Q-value=-0.203 - Visit counts=7\n",
      "Action  Left : Q-value=-0.442 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.225 - Visit counts=26\n",
      "Action  Up : Q-value=0.282 - Visit counts=27\n",
      "Action  Down : Q-value=0.197 - Visit counts=22\n",
      "Action  Left : Q-value=0.167 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.004 - Visit counts=18\n",
      "Action  Down : Q-value=0.188 - Visit counts=42\n",
      "Action  Left : Q-value=-0.043 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.182 - Visit counts=34\n",
      "Action  Up : Q-value=0.019 - Visit counts=15\n",
      "Action  Down : Q-value=0.062 - Visit counts=17\n",
      "Action  Left : Q-value=0.164 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.156 - Visit counts=24\n",
      "Action  Up : Q-value=0.275 - Visit counts=38\n",
      "Action  Down : Q-value=0.096 - Visit counts=19\n",
      "Action  Left : Q-value=-0.993 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.251 - Visit counts=31\n",
      "Action  Down : Q-value=0.250 - Visit counts=31\n",
      "Action  Left : Q-value=0.164 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.058 - Visit counts=21\n",
      "Action  Down : Q-value=0.182 - Visit counts=34\n",
      "Action  Left : Q-value=0.109 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.269 - Visit counts=36\n",
      "Action  Up : Q-value=-0.007 - Visit counts=14\n",
      "Action  Down : Q-value=0.110 - Visit counts=20\n",
      "Action  Left : Q-value=-0.039 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.080 - Visit counts=9\n",
      "Action  Up : Q-value=0.313 - Visit counts=39\n",
      "Action  Down : Q-value=0.231 - Visit counts=23\n",
      "Action  Left : Q-value=0.089 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.319 - Visit counts=43\n",
      "Action  Down : Q-value=0.135 - Visit counts=22\n",
      "Action  Left : Q-value=0.145 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.316 - Visit counts=41\n",
      "Action  Down : Q-value=0.274 - Visit counts=32\n",
      "Action  Left : Q-value=0.095 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.231 - Visit counts=36\n",
      "Action  Down : Q-value=-0.007 - Visit counts=14\n",
      "Action  Left : Q-value=0.280 - Visit counts=40\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.167 - Visit counts=19\n",
      "Action  Down : Q-value=0.141 - Visit counts=17\n",
      "Action  Left : Q-value=0.273 - Visit counts=25\n",
      "Action  Right : Q-value=0.285 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.120 - Visit counts=26\n",
      "Action  Down : Q-value=0.201 - Visit counts=30\n",
      "Action  Left : Q-value=0.123 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.098 - Visit counts=24\n",
      "Action  Up : Q-value=0.142 - Visit counts=34\n",
      "Action  Down : Q-value=-0.114 - Visit counts=11\n",
      "Action  Left : Q-value=-0.164 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.196 - Visit counts=22\n",
      "Action  Down : Q-value=0.290 - Visit counts=38\n",
      "Action  Left : Q-value=0.205 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.274 - Visit counts=25\n",
      "Action  Up : Q-value=0.345 - Visit counts=36\n",
      "Action  Down : Q-value=0.149 - Visit counts=15\n",
      "Action  Left : Q-value=0.009 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.305 - Visit counts=35\n",
      "Action  Down : Q-value=0.127 - Visit counts=16\n",
      "Action  Left : Q-value=0.302 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.080 - Visit counts=18\n",
      "Action  Down : Q-value=0.259 - Visit counts=33\n",
      "Action  Left : Q-value=0.217 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.240 - Visit counts=19\n",
      "Action  Up : Q-value=0.339 - Visit counts=31\n",
      "Action  Down : Q-value=0.173 - Visit counts=13\n",
      "Action  Left : Q-value=0.239 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.110 - Visit counts=15\n",
      "Action  Down : Q-value=0.225 - Visit counts=26\n",
      "Action  Left : Q-value=0.312 - Visit counts=39\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.282 - Visit counts=27\n",
      "Action  Down : Q-value=0.041 - Visit counts=12\n",
      "Action  Left : Q-value=0.011 - Visit counts=11\n",
      "Action  Right : Q-value=0.310 - Visit counts=38\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.251 - Visit counts=31\n",
      "Action  Down : Q-value=0.250 - Visit counts=31\n",
      "Action  Left : Q-value=0.165 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.348 - Visit counts=40\n",
      "Action  Down : Q-value=0.285 - Visit counts=28\n",
      "Action  Left : Q-value=0.041 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.251 - Visit counts=31\n",
      "Action  Down : Q-value=0.155 - Visit counts=24\n",
      "Action  Left : Q-value=0.262 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.201 - Visit counts=30\n",
      "Action  Down : Q-value=0.180 - Visit counts=27\n",
      "Action  Left : Q-value=-0.226 - Visit counts=9\n",
      "Action  Right : Q-value=0.062 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.026 - Visit counts=10\n",
      "Action  Down : Q-value=0.166 - Visit counts=19\n",
      "Action  Left : Q-value=-0.027 - Visit counts=10\n",
      "Action  Right : Q-value=0.314 - Visit counts=40\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.314 - Visit counts=40\n",
      "Action  Down : Q-value=0.188 - Visit counts=21\n",
      "Action  Left : Q-value=0.237 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.219 - Visit counts=25\n",
      "Action  Down : Q-value=0.205 - Visit counts=23\n",
      "Action  Left : Q-value=0.316 - Visit counts=41\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.188 - Visit counts=21\n",
      "Action  Down : Q-value=0.127 - Visit counts=16\n",
      "Action  Left : Q-value=0.204 - Visit counts=23\n",
      "Action  Right : Q-value=0.292 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.269 - Visit counts=24\n",
      "Action  Down : Q-value=0.302 - Visit counts=34\n",
      "Action  Left : Q-value=0.188 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.127 - Visit counts=16\n",
      "Action  Up : Q-value=0.305 - Visit counts=35\n",
      "Action  Down : Q-value=0.090 - Visit counts=14\n",
      "Action  Left : Q-value=0.155 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.212 - Visit counts=24\n",
      "Action  Down : Q-value=0.319 - Visit counts=43\n",
      "Action  Left : Q-value=0.062 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.225 - Visit counts=26\n",
      "Action  Up : Q-value=0.135 - Visit counts=22\n",
      "Action  Down : Q-value=0.282 - Visit counts=35\n",
      "Action  Left : Q-value=-0.073 - Visit counts=9\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.252 - Visit counts=21\n",
      "Action  Up : Q-value=0.259 - Visit counts=22\n",
      "Action  Down : Q-value=0.320 - Visit counts=34\n",
      "Action  Left : Q-value=-0.205 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█y  @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.123 - Visit counts=21\n",
      "Action  Up : Q-value=0.218 - Visit counts=33\n",
      "Action  Left : Q-value=0.162 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.003 - Visit counts=18\n",
      "Action  Up : Q-value=0.212 - Visit counts=40\n",
      "Action  Down : Q-value=-0.006 - Visit counts=14\n",
      "Action  Left : Q-value=-0.166 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.062 - Visit counts=17\n",
      "Action  Up : Q-value=0.222 - Visit counts=34\n",
      "Action  Down : Q-value=0.122 - Visit counts=21\n",
      "Action  Left : Q-value=0.060 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.324 - Visit counts=47\n",
      "Action  Down : Q-value=0.095 - Visit counts=19\n",
      "Action  Left : Q-value=0.063 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.236 - Visit counts=28\n",
      "Action  Down : Q-value=0.291 - Visit counts=38\n",
      "Action  Left : Q-value=0.246 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.095 - Visit counts=19\n",
      "Action  Up : Q-value=0.188 - Visit counts=21\n",
      "Action  Down : Q-value=0.178 - Visit counts=20\n",
      "Action  Left : Q-value=0.202 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█  @ █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.440 - Visit counts=5\n",
      "Action  Up : Q-value=0.272 - Visit counts=37\n",
      "Action  Down : Q-value=0.034 - Visit counts=14\n",
      "Action  Left : Q-value=-0.648 - Visit counts=4\n",
      "Action  Right : Q-value=0.126 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.338 - Visit counts=30\n",
      "Action  Down : Q-value=0.110 - Visit counts=15\n",
      "Action  Left : Q-value=0.090 - Visit counts=14\n",
      "Action  Right : Q-value=0.332 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.127 - Visit counts=16\n",
      "Action  Down : Q-value=0.068 - Visit counts=13\n",
      "Action  Left : Q-value=0.109 - Visit counts=15\n",
      "Action  Right : Q-value=0.305 - Visit counts=35\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.314 - Visit counts=40\n",
      "Action  Down : Q-value=0.197 - Visit counts=22\n",
      "Action  Left : Q-value=0.197 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.262 - Visit counts=34\n",
      "Action  Down : Q-value=0.173 - Visit counts=26\n",
      "Action  Left : Q-value=0.195 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.212 - Visit counts=40\n",
      "Action  Down : Q-value=0.170 - Visit counts=32\n",
      "Action  Left : Q-value=-0.114 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.195 - Visit counts=29\n",
      "Action  Down : Q-value=0.246 - Visit counts=30\n",
      "Action  Left : Q-value=0.247 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.289 - Visit counts=29\n",
      "Action  Down : Q-value=0.041 - Visit counts=12\n",
      "Action  Left : Q-value=-0.025 - Visit counts=10\n",
      "Action  Right : Q-value=0.236 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.091 - Visit counts=14\n",
      "Action  Down : Q-value=0.205 - Visit counts=23\n",
      "Action  Left : Q-value=0.142 - Visit counts=17\n",
      "Action  Right : Q-value=0.269 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.178 - Visit counts=20\n",
      "Action  Down : Q-value=0.339 - Visit counts=31\n",
      "Action  Left : Q-value=0.259 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.129 - Visit counts=8\n",
      "Action  Up : Q-value=0.295 - Visit counts=31\n",
      "Action  Down : Q-value=0.262 - Visit counts=29\n",
      "Action  Left : Q-value=0.042 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.343 - Visit counts=34\n",
      "Action  Down : Q-value=0.265 - Visit counts=23\n",
      "Action  Left : Q-value=0.264 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.222 - Visit counts=34\n",
      "Action  Down : Q-value=0.110 - Visit counts=25\n",
      "Action  Left : Q-value=0.156 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.265 - Visit counts=35\n",
      "Action  Down : Q-value=0.035 - Visit counts=14\n",
      "Action  Left : Q-value=0.263 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.095 - Visit counts=19\n",
      "Action  Down : Q-value=0.188 - Visit counts=21\n",
      "Action  Left : Q-value=0.334 - Visit counts=44\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.167 - Visit counts=19\n",
      "Action  Down : Q-value=0.181 - Visit counts=24\n",
      "Action  Left : Q-value=0.094 - Visit counts=19\n",
      "Action  Right : Q-value=0.251 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.348 - Visit counts=39\n",
      "Action  Down : Q-value=0.197 - Visit counts=22\n",
      "Action  Left : Q-value=0.167 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.345 - Visit counts=36\n",
      "Action  Down : Q-value=0.285 - Visit counts=28\n",
      "Action  Left : Q-value=0.269 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.295 - Visit counts=31\n",
      "Action  Down : Q-value=0.294 - Visit counts=31\n",
      "Action  Left : Q-value=0.205 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.225 - Visit counts=26\n",
      "Action  Down : Q-value=0.349 - Visit counts=40\n",
      "Action  Left : Q-value=0.091 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.197 - Visit counts=22\n",
      "Action  Up : Q-value=0.302 - Visit counts=34\n",
      "Action  Down : Q-value=0.187 - Visit counts=21\n",
      "Action  Left : Q-value=0.042 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.087 - Visit counts=23\n",
      "Action  Down : Q-value=0.203 - Visit counts=38\n",
      "Action  Left : Q-value=0.135 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.266 - Visit counts=35\n",
      "Action  Up : Q-value=0.011 - Visit counts=11\n",
      "Action  Down : Q-value=0.225 - Visit counts=26\n",
      "Action  Left : Q-value=0.110 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.318 - Visit counts=33\n",
      "Action  Up : Q-value=0.285 - Visit counts=28\n",
      "Action  Down : Q-value=-0.073 - Visit counts=9\n",
      "Action  Left : Q-value=0.090 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.110 - Visit counts=20\n",
      "Action  Up : Q-value=0.231 - Visit counts=27\n",
      "Action  Down : Q-value=0.189 - Visit counts=25\n",
      "Action  Left : Q-value=-0.028 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.206 - Visit counts=31\n",
      "Action  Down : Q-value=0.138 - Visit counts=28\n",
      "Action  Left : Q-value=0.062 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.196 - Visit counts=44\n",
      "Action  Down : Q-value=0.099 - Visit counts=24\n",
      "Action  Left : Q-value=-0.073 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.148 - Visit counts=29\n",
      "Action  Down : Q-value=0.216 - Visit counts=41\n",
      "Action  Left : Q-value=0.145 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.230 - Visit counts=27\n",
      "Action  Up : Q-value=0.236 - Visit counts=28\n",
      "Action  Down : Q-value=0.109 - Visit counts=20\n",
      "Action  Left : Q-value=0.019 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.173 - Visit counts=26\n",
      "Action  Down : Q-value=0.122 - Visit counts=21\n",
      "Action  Left : Q-value=0.200 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.123 - Visit counts=21\n",
      "Action  Down : Q-value=-0.027 - Visit counts=10\n",
      "Action  Left : Q-value=0.094 - Visit counts=19\n",
      "Action  Right : Q-value=0.194 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█    █\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.110 - Visit counts=20\n",
      "Action  Down : Q-value=0.222 - Visit counts=34\n",
      "Action  Left : Q-value=0.155 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.260 - Visit counts=22\n",
      "Action  Up : Q-value=0.259 - Visit counts=22\n",
      "Action  Down : Q-value=0.258 - Visit counts=22\n",
      "Action  Left : Q-value=0.141 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█   @█\n",
      "█!?  █\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.264 - Visit counts=23\n",
      "Action  Up : Q-value=0.042 - Visit counts=12\n",
      "Action  Down : Q-value=0.357 - Visit counts=30\n",
      "Action  Left : Q-value=-0.302 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.178 - Visit counts=20\n",
      "Action  Up : Q-value=0.167 - Visit counts=19\n",
      "Action  Down : Q-value=0.225 - Visit counts=31\n",
      "Action  Left : Q-value=-0.074 - Visit counts=9\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█y  @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.180 - Visit counts=24\n",
      "Action  Up : Q-value=0.254 - Visit counts=38\n",
      "Action  Left : Q-value=0.112 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.292 - Visit counts=32\n",
      "Action  Up : Q-value=0.266 - Visit counts=35\n",
      "Action  Down : Q-value=0.113 - Visit counts=18\n",
      "Action  Left : Q-value=-0.994 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.270 - Visit counts=31\n",
      "Action  Up : Q-value=0.162 - Visit counts=22\n",
      "Action  Down : Q-value=0.178 - Visit counts=20\n",
      "Action  Left : Q-value=-0.131 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!? @█\n",
      "█y   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.080 - Visit counts=16\n",
      "Action  Up : Q-value=0.167 - Visit counts=19\n",
      "Action  Down : Q-value=0.264 - Visit counts=36\n",
      "Action  Left : Q-value=-0.075 - Visit counts=9\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█y  @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.126 - Visit counts=35\n",
      "Action  Up : Q-value=0.101 - Visit counts=27\n",
      "Action  Left : Q-value=0.019 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█y  @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.027 - Visit counts=23\n",
      "Action  Up : Q-value=-0.019 - Visit counts=17\n",
      "Action  Left : Q-value=0.161 - Visit counts=44\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█y @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.034 - Visit counts=10\n",
      "Action  Up : Q-value=0.211 - Visit counts=16\n",
      "Action  Left : Q-value=0.674 - Visit counts=52\n",
      "Action  Right : Q-value=0.151 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█y@  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.533 - Visit counts=3\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Left : Q-value=0.894 - Visit counts=88\n",
      "Action  Right : Q-value=0.289 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█!?  █\n",
      "█@   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.844 - Visit counts=34\n",
      "Action  Up : Q-value=1.000 - Visit counts=98\n",
      "Action  Right : Q-value=-0.043 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 2}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█@?  █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.124 - Visit counts=27\n",
      "Action  Down : Q-value=-1.000 - Visit counts=3\n",
      "Action  Left : Q-value=-0.471 - Visit counts=8\n",
      "Action  Right : Q-value=-0.302 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█ ?  █\n",
      "█  !n█\n",
      "█ y  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.343 - Visit counts=17\n",
      "Action  Down : Q-value=-1.000 - Visit counts=4\n",
      "Action  Left : Q-value=-0.222 - Visit counts=34\n",
      "Action  Right : Q-value=-0.270 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█ ?  █\n",
      "█  !n█\n",
      "█ y  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.226 - Visit counts=27\n",
      "Action  Down : Q-value=-0.136 - Visit counts=40\n",
      "Action  Right : Q-value=-0.390 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█@?  █\n",
      "█  !n█\n",
      "█ y  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.205 - Visit counts=20\n",
      "Action  Up : Q-value=-0.401 - Visit counts=14\n",
      "Action  Down : Q-value=-0.089 - Visit counts=52\n",
      "Action  Right : Q-value=-1.000 - Visit counts=3\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█@ !n█\n",
      "█ y  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.162 - Visit counts=14\n",
      "Action  Up : Q-value=-0.401 - Visit counts=7\n",
      "Action  Down : Q-value=0.369 - Visit counts=48\n",
      "Action  Right : Q-value=0.056 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█  !n█\n",
      "█@y  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.008 - Visit counts=7\n",
      "Action  Up : Q-value=-0.301 - Visit counts=4\n",
      "Action  Right : Q-value=0.682 - Visit counts=86\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█  !n█\n",
      "█ @  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.422 - Visit counts=12\n",
      "Action  Up : Q-value=-0.152 - Visit counts=4\n",
      "Action  Left : Q-value=-0.988 - Visit counts=2\n",
      "Action  Right : Q-value=0.872 - Visit counts=117\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█  !n█\n",
      "█  @ █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.709 - Visit counts=22\n",
      "Action  Up : Q-value=1.000 - Visit counts=130\n",
      "Action  Left : Q-value=0.256 - Visit counts=6\n",
      "Action  Right : Q-value=0.363 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ ?  █\n",
      "█  @n█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "\n",
      "Episode 100 - Total reward 3\n",
      "Values:  tensor([0.3907, 0.3907, 0.3907, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924, 0.3924,\n",
      "        0.3924], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.7723, 0.7746, 0.7769, 0.7793,\n",
      "        0.7816, 0.7840, 0.7863, 0.7887, 0.7911, 0.7935, 0.7959, 0.7982, 0.8006,\n",
      "        0.8031, 0.8055, 0.8079, 0.8103, 0.8128, 0.8152, 0.8177, 0.8201, 0.8226,\n",
      "        0.8251, 0.8276, 0.8300, 0.8325, 0.8350, 0.8376, 0.8401, 0.8426, 0.8451,\n",
      "        0.8477, 0.8502, 0.8528, 0.8554, 0.8579, 0.8605, 0.8631, 0.8657, 0.8683,\n",
      "        0.8709, 0.8735, 0.8762, 0.8788, 0.8814, 0.8841, 0.8868, 0.8894, 0.8921,\n",
      "        0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138, 0.9166,\n",
      "        0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389, 0.9417,\n",
      "        0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970,\n",
      "        1.0000], device='cuda:0')\n",
      "Loss: 0.2585\n",
      "Rollout time: 1.41 - Update time: 0.00\n",
      "\n",
      "Episode 101 - Total reward 1\n",
      "Loss: 0.1939\n",
      "Rollout time: 1.19 - Update time: 0.00\n",
      "\n",
      "Episode 102 - Total reward 9\n",
      "Loss: 0.3574\n",
      "Rollout time: 0.66 - Update time: 0.00\n",
      "\n",
      "Episode 103 - Total reward 0\n",
      "Loss: 0.1345\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 104 - Total reward 1\n",
      "Loss: 0.1675\n",
      "Rollout time: 1.01 - Update time: 0.00\n",
      "\n",
      "Episode 105 - Total reward 2\n",
      "Loss: 0.2576\n",
      "Rollout time: 0.88 - Update time: 0.00\n",
      "\n",
      "Episode 106 - Total reward 8\n",
      "Loss: 0.2966\n",
      "Rollout time: 0.66 - Update time: 0.00\n",
      "\n",
      "Episode 107 - Total reward 3\n",
      "Loss: 0.3099\n",
      "Rollout time: 0.92 - Update time: 0.00\n",
      "\n",
      "Episode 108 - Total reward 8\n",
      "Loss: 0.2322\n",
      "Rollout time: 0.19 - Update time: 0.00\n",
      "\n",
      "Episode 109 - Total reward 6\n",
      "Loss: 0.2572\n",
      "Rollout time: 0.72 - Update time: 0.00\n",
      "\n",
      "Episode 110 - Total reward 2\n",
      "Values:  tensor([0.3794, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819, 0.3819,\n",
      "        0.3819, 0.3819, 0.3819, 0.3791, 0.3791, 0.3791, 0.3791, 0.3791, 0.3791,\n",
      "        0.3819, 0.3819, 0.3819, 0.3796, 0.3796, 0.3791, 0.3795, 0.3791, 0.3796,\n",
      "        0.3791], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.7816, 0.7840, 0.7863, 0.7887, 0.7911, 0.7935, 0.7959, 0.7982, 0.8006,\n",
      "        0.8031, 0.8055, 0.8079, 0.8103, 0.8128, 0.8152, 0.8177, 0.8201, 0.8226,\n",
      "        0.8251, 0.8276, 0.8300, 0.8325, 0.8350, 0.8376, 0.8401, 0.8426, 0.8451,\n",
      "        0.8477, 0.8502, 0.8528, 0.8554, 0.8579, 0.8605, 0.8631, 0.8657, 0.8683,\n",
      "        0.8709, 0.8735, 0.8762, 0.8788, 0.8814, 0.8841, 0.8868, 0.8894, 0.8921,\n",
      "        0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138, 0.9166,\n",
      "        0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389, 0.9417,\n",
      "        0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2595\n",
      "Rollout time: 1.23 - Update time: 0.00\n",
      "\n",
      "Episode 111 - Total reward 6\n",
      "Loss: 0.2734\n",
      "Rollout time: 0.76 - Update time: 0.00\n",
      "\n",
      "Episode 112 - Total reward 0\n",
      "Loss: 0.2162\n",
      "Rollout time: 1.56 - Update time: 0.00\n",
      "\n",
      "Episode 113 - Total reward 5\n",
      "Loss: 0.2332\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 114 - Total reward 5\n",
      "Loss: 0.1983\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 115 - Total reward 8\n",
      "Loss: 0.1635\n",
      "Rollout time: 0.71 - Update time: 0.00\n",
      "\n",
      "Episode 116 - Total reward 2\n",
      "Loss: 0.2408\n",
      "Rollout time: 1.01 - Update time: 0.00\n",
      "\n",
      "Episode 117 - Total reward 6\n",
      "Loss: 0.2308\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 118 - Total reward 0\n",
      "Loss: 0.5401\n",
      "Rollout time: 1.53 - Update time: 0.00\n",
      "\n",
      "Episode 119 - Total reward 1\n",
      "Loss: 0.4293\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 120 - Total reward 7\n",
      "Values:  tensor([0.6500, 0.6556, 0.6494, 0.6640, 0.6640, 0.6640, 0.6643, 0.6632, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6643, 0.6643,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6643, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6583, 0.6583, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6618, 0.6640, 0.6640, 0.6615, 0.6635, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640, 0.6640,\n",
      "        0.6640], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9474, 0.9502, 0.9531, 0.9559,\n",
      "        0.9588, 0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821,\n",
      "        0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9910, 0.9940, 0.9970, 1.0000, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9940,\n",
      "        0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2827\n",
      "Rollout time: 0.87 - Update time: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 121 - Total reward 0\n",
      "Loss: 0.3632\n",
      "Rollout time: 1.11 - Update time: 0.00\n",
      "\n",
      "Episode 122 - Total reward 0\n",
      "Loss: 0.2648\n",
      "Rollout time: 1.09 - Update time: 0.00\n",
      "\n",
      "Episode 123 - Total reward 6\n",
      "Loss: 0.2490\n",
      "Rollout time: 0.81 - Update time: 0.00\n",
      "\n",
      "Episode 124 - Total reward 1\n",
      "Loss: 0.2389\n",
      "Rollout time: 0.97 - Update time: 0.00\n",
      "\n",
      "Episode 125 - Total reward 0\n",
      "Loss: 0.1029\n",
      "Rollout time: 1.55 - Update time: 0.00\n",
      "\n",
      "Episode 126 - Total reward 7\n",
      "Loss: 0.2639\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 127 - Total reward 3\n",
      "Loss: 0.1302\n",
      "Rollout time: 0.96 - Update time: 0.00\n",
      "\n",
      "Episode 128 - Total reward 2\n",
      "Loss: 0.1401\n",
      "Rollout time: 0.96 - Update time: 0.00\n",
      "\n",
      "Episode 129 - Total reward 1\n",
      "Loss: 0.2081\n",
      "Rollout time: 1.06 - Update time: 0.00\n",
      "\n",
      "Episode 130 - Total reward 4\n",
      "Values:  tensor([0.1597, 0.1659, 0.1659, 0.1659, 0.1659, 0.1577, 0.1577, 0.1659, 0.1659,\n",
      "        0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659,\n",
      "        0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659,\n",
      "        0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659,\n",
      "        0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659,\n",
      "        0.1659, 0.1609, 0.1659, 0.1659, 0.1659, 0.1627, 0.1634, 0.1643, 0.1639,\n",
      "        0.1643, 0.1643, 0.1639, 0.1643, 0.1643, 0.1634, 0.1627, 0.1634, 0.1623,\n",
      "        0.1634, 0.1643, 0.1634, 0.1643, 0.1634, 0.1623, 0.1639, 0.1639, 0.1643,\n",
      "        0.1643, 0.1634, 0.1643, 0.1643, 0.1639, 0.1643, 0.1643, 0.1643, 0.1634,\n",
      "        0.1643, 0.1643, 0.1643, 0.1643, 0.1634, 0.1623, 0.1634, 0.1627, 0.1659,\n",
      "        0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659, 0.1659,\n",
      "        0.1659], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9910, 0.9940, 0.9970, 1.0000, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588,\n",
      "        0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851,\n",
      "        0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9389, 0.9417, 0.9445, 0.9474,\n",
      "        0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675, 0.9704, 0.9733,\n",
      "        0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.3352\n",
      "Rollout time: 0.79 - Update time: 0.00\n",
      "\n",
      "Episode 131 - Total reward 3\n",
      "Loss: 0.0996\n",
      "Rollout time: 1.33 - Update time: 0.00\n",
      "\n",
      "Episode 132 - Total reward 4\n",
      "Loss: 0.5671\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 133 - Total reward 5\n",
      "Loss: 0.5552\n",
      "Rollout time: 0.69 - Update time: 0.00\n",
      "\n",
      "Episode 134 - Total reward 5\n",
      "Loss: 0.3673\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 135 - Total reward 1\n",
      "Loss: 0.0409\n",
      "Rollout time: 1.53 - Update time: 0.00\n",
      "\n",
      "Episode 136 - Total reward 5\n",
      "Loss: 0.5406\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 137 - Total reward 12\n",
      "Loss: 0.6244\n",
      "Rollout time: 0.54 - Update time: 0.00\n",
      "\n",
      "Episode 138 - Total reward 3\n",
      "Loss: 0.2061\n",
      "Rollout time: 0.86 - Update time: 0.00\n",
      "\n",
      "Episode 139 - Total reward 5\n",
      "Loss: 0.2488\n",
      "Rollout time: 0.79 - Update time: 0.00\n",
      "\n",
      "Episode 140 - Total reward 2\n",
      "Values:  tensor([0.3082, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173,\n",
      "        0.3173, 0.3173, 0.3173, 0.2989, 0.2989, 0.2989, 0.2991, 0.2993, 0.3173,\n",
      "        0.3173], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.7608, 0.7631, 0.7654, 0.7677, 0.7700, 0.7723, 0.7746, 0.7769, 0.7793,\n",
      "        0.7816, 0.7840, 0.7863, 0.7887, 0.7911, 0.7935, 0.7959, 0.7982, 0.8006,\n",
      "        0.8031, 0.8055, 0.8079, 0.8103, 0.8128, 0.8152, 0.8177, 0.8201, 0.8226,\n",
      "        0.8251, 0.8276, 0.8300, 0.8325, 0.8350, 0.8376, 0.8401, 0.8426, 0.8451,\n",
      "        0.8477, 0.8502, 0.8528, 0.8554, 0.8579, 0.8605, 0.8631, 0.8657, 0.8683,\n",
      "        0.8709, 0.8735, 0.8762, 0.8788, 0.8814, 0.8841, 0.8868, 0.8894, 0.8921,\n",
      "        0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138, 0.9166,\n",
      "        0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389, 0.9417,\n",
      "        0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970,\n",
      "        1.0000], device='cuda:0')\n",
      "Loss: 0.3282\n",
      "Rollout time: 1.35 - Update time: 0.00\n",
      "\n",
      "Episode 141 - Total reward 11\n",
      "Loss: 0.3058\n",
      "Rollout time: 0.59 - Update time: 0.00\n",
      "\n",
      "Episode 142 - Total reward 2\n",
      "Loss: 0.2364\n",
      "Rollout time: 1.21 - Update time: 0.00\n",
      "\n",
      "Episode 143 - Total reward 3\n",
      "Loss: 0.2713\n",
      "Rollout time: 0.97 - Update time: 0.00\n",
      "\n",
      "Episode 144 - Total reward 1\n",
      "Loss: 0.3092\n",
      "Rollout time: 1.33 - Update time: 0.00\n",
      "\n",
      "Episode 145 - Total reward 0\n",
      "Loss: 0.2988\n",
      "Rollout time: 1.54 - Update time: 0.00\n",
      "\n",
      "Episode 146 - Total reward 1\n",
      "Loss: 0.2247\n",
      "Rollout time: 1.12 - Update time: 0.00\n",
      "\n",
      "Episode 147 - Total reward 1\n",
      "Loss: 0.1873\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 148 - Total reward 2\n",
      "Loss: 0.2152\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 149 - Total reward 2\n",
      "Loss: 0.3256\n",
      "Rollout time: 0.91 - Update time: 0.00\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y @█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.233 - Visit counts=16\n",
      "Action  Up : Q-value=-0.589 - Visit counts=6\n",
      "Action  Down : Q-value=-0.127 - Visit counts=21\n",
      "Action  Left : Q-value=-0.470 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█   @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.180 - Visit counts=15\n",
      "Action  Up : Q-value=-0.251 - Visit counts=13\n",
      "Action  Left : Q-value=0.090 - Visit counts=42\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█  @ █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.050 - Visit counts=32\n",
      "Action  Up : Q-value=-0.083 - Visit counts=16\n",
      "Action  Left : Q-value=-0.040 - Visit counts=20\n",
      "Action  Right : Q-value=-0.042 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█  @ █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.124 - Visit counts=45\n",
      "Action  Up : Q-value=-0.472 - Visit counts=7\n",
      "Action  Left : Q-value=-0.061 - Visit counts=16\n",
      "Action  Right : Q-value=-0.153 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█  @ █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.149 - Visit counts=35\n",
      "Action  Up : Q-value=0.073 - Visit counts=22\n",
      "Action  Left : Q-value=-0.009 - Visit counts=16\n",
      "Action  Right : Q-value=0.024 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█  @ █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.123 - Visit counts=14\n",
      "Action  Up : Q-value=0.060 - Visit counts=26\n",
      "Action  Left : Q-value=-0.077 - Visit counts=16\n",
      "Action  Right : Q-value=0.073 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█   @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.185 - Visit counts=15\n",
      "Action  Up : Q-value=-0.144 - Visit counts=20\n",
      "Action  Left : Q-value=0.054 - Visit counts=42\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█  @ █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.160 - Visit counts=14\n",
      "Action  Up : Q-value=-0.039 - Visit counts=25\n",
      "Action  Left : Q-value=0.069 - Visit counts=35\n",
      "Action  Right : Q-value=-0.136 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█ @  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.044 - Visit counts=23\n",
      "Action  Up : Q-value=0.053 - Visit counts=24\n",
      "Action  Left : Q-value=0.101 - Visit counts=21\n",
      "Action  Right : Q-value=-0.083 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█@   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.037 - Visit counts=22\n",
      "Action  Up : Q-value=-0.993 - Visit counts=3\n",
      "Action  Right : Q-value=0.154 - Visit counts=45\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ y  █\n",
      "█ @  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.005 - Visit counts=11\n",
      "Action  Up : Q-value=0.432 - Visit counts=54\n",
      "Action  Left : Q-value=0.003 - Visit counts=11\n",
      "Action  Right : Q-value=0.122 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█ @  █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.381 - Visit counts=12\n",
      "Action  Up : Q-value=-0.330 - Visit counts=3\n",
      "Action  Down : Q-value=0.057 - Visit counts=7\n",
      "Action  Left : Q-value=-0.109 - Visit counts=5\n",
      "Action  Right : Q-value=0.799 - Visit counts=76\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? ! █\n",
      "█  @ █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.613 - Visit counts=14\n",
      "Action  Up : Q-value=1.000 - Visit counts=90\n",
      "Action  Down : Q-value=0.282 - Visit counts=6\n",
      "Action  Left : Q-value=0.419 - Visit counts=8\n",
      "Action  Right : Q-value=0.338 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█n   █\n",
      "█? @ █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.993 - Visit counts=2\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Left : Q-value=-0.383 - Visit counts=6\n",
      "Action  Right : Q-value=0.383 - Visit counts=40\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█ ?!y█\n",
      "█  @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.190 - Visit counts=4\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Left : Q-value=-0.192 - Visit counts=4\n",
      "Action  Right : Q-value=0.706 - Visit counts=79\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█ ?!y█\n",
      "█   @█\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.570 - Visit counts=25\n",
      "Action  Up : Q-value=0.850 - Visit counts=101\n",
      "Action  Left : Q-value=-0.993 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█ ?!@█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.743 - Visit counts=23\n",
      "Action  Up : Q-value=0.420 - Visit counts=8\n",
      "Action  Down : Q-value=0.471 - Visit counts=12\n",
      "Action  Left : Q-value=1.000 - Visit counts=107\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█ ?@ █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.280 - Visit counts=12\n",
      "Action  Up : Q-value=-0.177 - Visit counts=15\n",
      "Action  Down : Q-value=-0.991 - Visit counts=3\n",
      "Action  Left : Q-value=-0.101 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ y  █\n",
      "█! @ █\n",
      "█ n? █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.257 - Visit counts=11\n",
      "Action  Up : Q-value=0.077 - Visit counts=44\n",
      "Action  Down : Q-value=-1.000 - Visit counts=3\n",
      "Action  Left : Q-value=-0.686 - Visit counts=4\n",
      "Action  Right : Q-value=-0.471 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ y@ █\n",
      "█!   █\n",
      "█ n? █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.120 - Visit counts=7\n",
      "Action  Up : Q-value=0.137 - Visit counts=14\n",
      "Action  Down : Q-value=-0.180 - Visit counts=6\n",
      "Action  Left : Q-value=0.465 - Visit counts=46\n",
      "Action  Right : Q-value=0.162 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ @  █\n",
      "█!   █\n",
      "█ n? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.396 - Visit counts=11\n",
      "Action  Up : Q-value=0.230 - Visit counts=7\n",
      "Action  Down : Q-value=0.079 - Visit counts=6\n",
      "Action  Left : Q-value=0.810 - Visit counts=64\n",
      "Action  Right : Q-value=0.164 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█@   █\n",
      "█!   █\n",
      "█ n? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.492 - Visit counts=10\n",
      "Action  Up : Q-value=0.483 - Visit counts=9\n",
      "Action  Down : Q-value=1.000 - Visit counts=90\n",
      "Action  Right : Q-value=-0.074 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█    █\n",
      "█@   █\n",
      "█ n? █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.087 - Visit counts=17\n",
      "Action  Down : Q-value=-0.051 - Visit counts=13\n",
      "Action  Right : Q-value=0.085 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█@n  █\n",
      "█y  !█\n",
      "█    █\n",
      "█?   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.116 - Visit counts=32\n",
      "Action  Down : Q-value=0.015 - Visit counts=17\n",
      "Action  Right : Q-value=0.016 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█@n  █\n",
      "█y  !█\n",
      "█    █\n",
      "█?   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.129 - Visit counts=29\n",
      "Action  Down : Q-value=0.167 - Visit counts=34\n",
      "Action  Right : Q-value=0.028 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█@  !█\n",
      "█    █\n",
      "█?   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.173 - Visit counts=6\n",
      "Action  Up : Q-value=0.028 - Visit counts=12\n",
      "Action  Down : Q-value=-0.119 - Visit counts=7\n",
      "Action  Right : Q-value=0.509 - Visit counts=58\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█ @ !█\n",
      "█    █\n",
      "█?   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.154 - Visit counts=6\n",
      "Action  Up : Q-value=0.230 - Visit counts=7\n",
      "Action  Down : Q-value=-0.076 - Visit counts=4\n",
      "Action  Left : Q-value=0.028 - Visit counts=6\n",
      "Action  Right : Q-value=0.822 - Visit counts=84\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█  @!█\n",
      "█    █\n",
      "█?   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.613 - Visit counts=14\n",
      "Action  Up : Q-value=0.581 - Visit counts=13\n",
      "Action  Down : Q-value=0.420 - Visit counts=8\n",
      "Action  Left : Q-value=0.419 - Visit counts=8\n",
      "Action  Right : Q-value=1.000 - Visit counts=90\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█   @█\n",
      "█    █\n",
      "█?   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.294 - Visit counts=7\n",
      "Action  Up : Q-value=-0.505 - Visit counts=5\n",
      "Action  Down : Q-value=-0.129 - Visit counts=17\n",
      "Action  Right : Q-value=-0.105 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█ ?  █\n",
      "█ @  █\n",
      "█ y !█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.229 - Visit counts=8\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Down : Q-value=0.499 - Visit counts=41\n",
      "Action  Left : Q-value=-0.054 - Visit counts=13\n",
      "Action  Right : Q-value=-0.383 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█ ?  █\n",
      "█    █\n",
      "█ @ !█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.104 - Visit counts=5\n",
      "Action  Up : Q-value=-0.994 - Visit counts=2\n",
      "Action  Left : Q-value=0.029 - Visit counts=6\n",
      "Action  Right : Q-value=0.866 - Visit counts=77\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█ ?  █\n",
      "█    █\n",
      "█  @!█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.985 - Visit counts=2\n",
      "Action  Up : Q-value=0.166 - Visit counts=7\n",
      "Action  Left : Q-value=0.139 - Visit counts=5\n",
      "Action  Right : Q-value=1.000 - Visit counts=112\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ n  █\n",
      "█ ?  █\n",
      "█    █\n",
      "█   @█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.997 - Visit counts=2\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Down : Q-value=-0.009 - Visit counts=16\n",
      "Action  Left : Q-value=0.133 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ y!?█\n",
      "█  @ █\n",
      "█    █\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.187 - Visit counts=7\n",
      "Action  Up : Q-value=-1.000 - Visit counts=2\n",
      "Action  Down : Q-value=0.099 - Visit counts=10\n",
      "Action  Left : Q-value=0.547 - Visit counts=58\n",
      "Action  Right : Q-value=-0.993 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ y!?█\n",
      "█ @  █\n",
      "█    █\n",
      "█ n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.225 - Visit counts=7\n",
      "Action  Up : Q-value=0.842 - Visit counts=80\n",
      "Action  Down : Q-value=0.226 - Visit counts=7\n",
      "Action  Left : Q-value=0.376 - Visit counts=10\n",
      "Action  Right : Q-value=-0.190 - Visit counts=3\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ @!?█\n",
      "█    █\n",
      "█    █\n",
      "█ n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.584 - Visit counts=13\n",
      "Action  Down : Q-value=0.420 - Visit counts=8\n",
      "Action  Left : Q-value=0.709 - Visit counts=19\n",
      "Action  Right : Q-value=1.000 - Visit counts=89\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  @?█\n",
      "█    █\n",
      "█    █\n",
      "█ n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.158 - Visit counts=19\n",
      "Action  Up : Q-value=-0.047 - Visit counts=9\n",
      "Action  Down : Q-value=0.153 - Visit counts=18\n",
      "Action  Right : Q-value=-0.380 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█  !n█\n",
      "█@   █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.064 - Visit counts=17\n",
      "Action  Up : Q-value=-0.122 - Visit counts=14\n",
      "Action  Down : Q-value=0.055 - Visit counts=29\n",
      "Action  Right : Q-value=-0.385 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█  !n█\n",
      "█    █\n",
      "█@   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.035 - Visit counts=19\n",
      "Action  Up : Q-value=0.120 - Visit counts=36\n",
      "Action  Right : Q-value=0.010 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█  !n█\n",
      "█@   █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.040 - Visit counts=20\n",
      "Action  Up : Q-value=-0.037 - Visit counts=14\n",
      "Action  Down : Q-value=0.076 - Visit counts=25\n",
      "Action  Right : Q-value=0.079 - Visit counts=26\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█  !n█\n",
      "█ @  █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.061 - Visit counts=23\n",
      "Action  Up : Q-value=-0.384 - Visit counts=6\n",
      "Action  Down : Q-value=-0.150 - Visit counts=13\n",
      "Action  Left : Q-value=0.010 - Visit counts=23\n",
      "Action  Right : Q-value=-0.264 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█  !n█\n",
      "█ @  █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.083 - Visit counts=12\n",
      "Action  Up : Q-value=-0.233 - Visit counts=8\n",
      "Action  Down : Q-value=0.009 - Visit counts=17\n",
      "Action  Left : Q-value=0.203 - Visit counts=29\n",
      "Action  Right : Q-value=-0.386 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█  !n█\n",
      "█@   █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.109 - Visit counts=26\n",
      "Action  Up : Q-value=0.140 - Visit counts=25\n",
      "Action  Down : Q-value=0.091 - Visit counts=19\n",
      "Action  Right : Q-value=-0.232 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█@ !n█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.023 - Visit counts=24\n",
      "Action  Up : Q-value=-0.032 - Visit counts=22\n",
      "Action  Down : Q-value=0.000 - Visit counts=22\n",
      "Action  Right : Q-value=-0.382 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y ? █\n",
      "█@ !n█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.095 - Visit counts=19\n",
      "Action  Up : Q-value=-0.013 - Visit counts=22\n",
      "Action  Down : Q-value=-0.053 - Visit counts=22\n",
      "Action  Right : Q-value=-0.261 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█@ ? █\n",
      "█  !n█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.585 - Visit counts=3\n",
      "Action  Down : Q-value=0.552 - Visit counts=55\n",
      "Action  Right : Q-value=-0.029 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█@ !n█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.073 - Visit counts=4\n",
      "Action  Up : Q-value=-0.075 - Visit counts=4\n",
      "Action  Down : Q-value=0.314 - Visit counts=9\n",
      "Action  Right : Q-value=0.841 - Visit counts=87\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█ @!n█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.469 - Visit counts=11\n",
      "Action  Up : Q-value=-0.997 - Visit counts=2\n",
      "Action  Down : Q-value=0.421 - Visit counts=8\n",
      "Action  Left : Q-value=0.457 - Visit counts=10\n",
      "Action  Right : Q-value=1.000 - Visit counts=105\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█  @n█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.666 - Visit counts=15\n",
      "Action  Up : Q-value=-0.855 - Visit counts=9\n",
      "Action  Down : Q-value=-0.562 - Visit counts=20\n",
      "Action  Left : Q-value=-1.000 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█ n @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.427 - Visit counts=39\n",
      "Action  Up : Q-value=-0.987 - Visit counts=6\n",
      "Action  Left : Q-value=-0.532 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█ n @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.357 - Visit counts=56\n",
      "Action  Up : Q-value=-0.548 - Visit counts=22\n",
      "Action  Left : Q-value=-0.747 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 6}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█ n @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.582 - Visit counts=18\n",
      "Action  Up : Q-value=-0.666 - Visit counts=15\n",
      "Action  Left : Q-value=-0.294 - Visit counts=72\n",
      "Tree info:  {'max_tree_depth': 6}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█ n@ █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.381 - Visit counts=28\n",
      "Action  Up : Q-value=-1.000 - Visit counts=5\n",
      "Action  Left : Q-value=-0.165 - Visit counts=77\n",
      "Action  Right : Q-value=-0.657 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 6}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.342 - Visit counts=17\n",
      "Action  Up : Q-value=-0.226 - Visit counts=24\n",
      "Action  Left : Q-value=-0.052 - Visit counts=69\n",
      "Action  Right : Q-value=-0.379 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█@   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.191 - Visit counts=26\n",
      "Action  Up : Q-value=-0.108 - Visit counts=36\n",
      "Action  Right : Q-value=-0.052 - Visit counts=56\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█ @  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.225 - Visit counts=16\n",
      "Action  Up : Q-value=-0.049 - Visit counts=35\n",
      "Action  Left : Q-value=-0.036 - Visit counts=32\n",
      "Action  Right : Q-value=-0.158 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█@   █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.040 - Visit counts=27\n",
      "Action  Up : Q-value=0.047 - Visit counts=40\n",
      "Action  Right : Q-value=-0.205 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█@ ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.094 - Visit counts=45\n",
      "Action  Up : Q-value=-0.464 - Visit counts=7\n",
      "Action  Down : Q-value=-0.150 - Visit counts=16\n",
      "Action  Right : Q-value=-0.061 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█    █\n",
      "█@ ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.056 - Visit counts=28\n",
      "Action  Up : Q-value=0.104 - Visit counts=29\n",
      "Action  Down : Q-value=-0.002 - Visit counts=21\n",
      "Action  Right : Q-value=-0.075 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█@   █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.123 - Visit counts=27\n",
      "Action  Up : Q-value=-0.013 - Visit counts=15\n",
      "Action  Down : Q-value=-0.050 - Visit counts=13\n",
      "Action  Right : Q-value=0.051 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█@   █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.188 - Visit counts=29\n",
      "Action  Up : Q-value=0.182 - Visit counts=29\n",
      "Action  Down : Q-value=-0.074 - Visit counts=12\n",
      "Action  Right : Q-value=-0.378 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█@   █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.128 - Visit counts=24\n",
      "Action  Up : Q-value=0.133 - Visit counts=25\n",
      "Action  Down : Q-value=-0.134 - Visit counts=10\n",
      "Action  Right : Q-value=0.052 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█@y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.104 - Visit counts=29\n",
      "Action  Down : Q-value=0.104 - Visit counts=29\n",
      "Action  Right : Q-value=0.021 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ y ?█\n",
      "█@   █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.074 - Visit counts=16\n",
      "Action  Up : Q-value=0.202 - Visit counts=40\n",
      "Action  Down : Q-value=-0.176 - Visit counts=12\n",
      "Action  Right : Q-value=-0.257 - Visit counts=10\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█@y ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.015 - Visit counts=17\n",
      "Action  Down : Q-value=0.114 - Visit counts=25\n",
      "Action  Right : Q-value=0.254 - Visit counts=47\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ @ ?█\n",
      "█    █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.281 - Visit counts=26\n",
      "Action  Down : Q-value=0.455 - Visit counts=48\n",
      "Action  Left : Q-value=0.220 - Visit counts=18\n",
      "Action  Right : Q-value=-0.379 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█ @  █\n",
      "█  ! █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.153 - Visit counts=6\n",
      "Action  Up : Q-value=0.152 - Visit counts=6\n",
      "Action  Down : Q-value=0.746 - Visit counts=49\n",
      "Action  Left : Q-value=0.230 - Visit counts=7\n",
      "Action  Right : Q-value=0.663 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█    █\n",
      "█ @! █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.666 - Visit counts=14\n",
      "Action  Up : Q-value=0.230 - Visit counts=5\n",
      "Action  Down : Q-value=0.230 - Visit counts=5\n",
      "Action  Left : Q-value=0.230 - Visit counts=5\n",
      "Action  Right : Q-value=1.000 - Visit counts=69\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   ?█\n",
      "█    █\n",
      "█  @ █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.039 - Visit counts=9\n",
      "Action  Up : Q-value=-0.383 - Visit counts=4\n",
      "Action  Down : Q-value=0.230 - Visit counts=18\n",
      "Action  Left : Q-value=-0.075 - Visit counts=8\n",
      "Action  Right : Q-value=0.005 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█    █\n",
      "█y@  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.158 - Visit counts=17\n",
      "Action  Up : Q-value=0.136 - Visit counts=13\n",
      "Action  Left : Q-value=0.162 - Visit counts=18\n",
      "Action  Right : Q-value=0.230 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█    █\n",
      "█y @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.230 - Visit counts=24\n",
      "Action  Up : Q-value=-0.013 - Visit counts=10\n",
      "Action  Left : Q-value=0.158 - Visit counts=17\n",
      "Action  Right : Q-value=0.158 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█    █\n",
      "█y @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.181 - Visit counts=25\n",
      "Action  Up : Q-value=0.078 - Visit counts=16\n",
      "Action  Left : Q-value=-0.012 - Visit counts=15\n",
      "Action  Right : Q-value=0.087 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█    █\n",
      "█y @ █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.159 - Visit counts=17\n",
      "Action  Up : Q-value=-0.073 - Visit counts=8\n",
      "Action  Left : Q-value=0.259 - Visit counts=26\n",
      "Action  Right : Q-value=0.230 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█    █\n",
      "█y@  █\n",
      "██████\n",
      "shimmering sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.202 - Visit counts=17\n",
      "Action  Up : Q-value=0.230 - Visit counts=18\n",
      "Action  Left : Q-value=0.337 - Visit counts=28\n",
      "Action  Right : Q-value=0.129 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█    █\n",
      "█@   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.280 - Visit counts=15\n",
      "Action  Up : Q-value=0.537 - Visit counts=49\n",
      "Action  Right : Q-value=0.230 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█    █\n",
      "█@   █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.356 - Visit counts=12\n",
      "Action  Up : Q-value=0.741 - Visit counts=66\n",
      "Action  Down : Q-value=0.254 - Visit counts=12\n",
      "Action  Right : Q-value=0.079 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ ! ?█\n",
      "█@   █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.581 - Visit counts=19\n",
      "Action  Up : Q-value=0.903 - Visit counts=75\n",
      "Action  Down : Q-value=0.230 - Visit counts=6\n",
      "Action  Right : Q-value=0.586 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█@! ?█\n",
      "█    █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.886 - Visit counts=35\n",
      "Action  Down : Q-value=0.685 - Visit counts=15\n",
      "Action  Right : Q-value=1.000 - Visit counts=74\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ @ ?█\n",
      "█    █\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.292 - Visit counts=7\n",
      "Action  Up : Q-value=0.059 - Visit counts=17\n",
      "Action  Down : Q-value=-0.997 - Visit counts=2\n",
      "Action  Left : Q-value=0.046 - Visit counts=20\n",
      "Action  Right : Q-value=-0.684 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.207 - Visit counts=21\n",
      "Action  Down : Q-value=0.052 - Visit counts=12\n",
      "Action  Left : Q-value=0.179 - Visit counts=18\n",
      "Action  Right : Q-value=0.068 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.051 - Visit counts=23\n",
      "Action  Down : Q-value=-0.177 - Visit counts=12\n",
      "Action  Left : Q-value=-0.064 - Visit counts=18\n",
      "Action  Right : Q-value=-0.056 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.153 - Visit counts=22\n",
      "Action  Down : Q-value=-0.050 - Visit counts=12\n",
      "Action  Left : Q-value=0.146 - Visit counts=22\n",
      "Action  Right : Q-value=0.050 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.057 - Visit counts=14\n",
      "Action  Down : Q-value=0.006 - Visit counts=15\n",
      "Action  Left : Q-value=0.229 - Visit counts=29\n",
      "Action  Right : Q-value=0.006 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.019 - Visit counts=23\n",
      "Action  Down : Q-value=-0.012 - Visit counts=22\n",
      "Action  Right : Q-value=0.120 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.014 - Visit counts=19\n",
      "Action  Down : Q-value=-0.075 - Visit counts=16\n",
      "Action  Left : Q-value=-0.039 - Visit counts=18\n",
      "Action  Right : Q-value=0.156 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.074 - Visit counts=12\n",
      "Action  Down : Q-value=-0.685 - Visit counts=4\n",
      "Action  Left : Q-value=0.216 - Visit counts=33\n",
      "Action  Right : Q-value=0.192 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█  n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.065 - Visit counts=25\n",
      "Action  Down : Q-value=0.209 - Visit counts=36\n",
      "Action  Left : Q-value=-0.050 - Visit counts=13\n",
      "Action  Right : Q-value=-0.225 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ @n █\n",
      "█  ! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.056 - Visit counts=7\n",
      "Action  Up : Q-value=0.056 - Visit counts=7\n",
      "Action  Down : Q-value=0.801 - Visit counts=62\n",
      "Action  Left : Q-value=-0.073 - Visit counts=4\n",
      "Action  Right : Q-value=-0.015 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█  n █\n",
      "█ @! █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.112 - Visit counts=4\n",
      "Action  Up : Q-value=0.155 - Visit counts=6\n",
      "Action  Down : Q-value=-1.000 - Visit counts=1\n",
      "Action  Left : Q-value=0.078 - Visit counts=6\n",
      "Action  Right : Q-value=1.000 - Visit counts=94\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█  n █\n",
      "█  @ █\n",
      "█ ?  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.993 - Visit counts=9\n",
      "Action  Down : Q-value=-1.000 - Visit counts=9\n",
      "Action  Right : Q-value=-0.534 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ @? █\n",
      "█!   █\n",
      "█    █\n",
      "█ n y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.616 - Visit counts=13\n",
      "Action  Down : Q-value=-0.276 - Visit counts=58\n",
      "Action  Left : Q-value=-0.995 - Visit counts=5\n",
      "Action  Right : Q-value=-1.000 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!@  █\n",
      "█    █\n",
      "█ n y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.339 - Visit counts=15\n",
      "Action  Up : Q-value=-0.749 - Visit counts=5\n",
      "Action  Down : Q-value=-0.044 - Visit counts=58\n",
      "Action  Left : Q-value=-1.000 - Visit counts=4\n",
      "Action  Right : Q-value=-0.209 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█ @  █\n",
      "█ n y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.002 - Visit counts=32\n",
      "Action  Up : Q-value=-0.992 - Visit counts=3\n",
      "Action  Down : Q-value=-0.031 - Visit counts=28\n",
      "Action  Left : Q-value=-0.130 - Visit counts=17\n",
      "Action  Right : Q-value=0.004 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█  @ █\n",
      "█ n y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.002 - Visit counts=16\n",
      "Action  Up : Q-value=-0.293 - Visit counts=7\n",
      "Action  Down : Q-value=0.096 - Visit counts=27\n",
      "Action  Left : Q-value=-0.294 - Visit counts=7\n",
      "Action  Right : Q-value=0.038 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█    █\n",
      "█ n@y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.120 - Visit counts=22\n",
      "Action  Up : Q-value=-0.029 - Visit counts=14\n",
      "Action  Left : Q-value=0.038 - Visit counts=19\n",
      "Action  Right : Q-value=0.057 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█    █\n",
      "█ n@y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.136 - Visit counts=26\n",
      "Action  Up : Q-value=-0.291 - Visit counts=7\n",
      "Action  Left : Q-value=0.016 - Visit counts=17\n",
      "Action  Right : Q-value=0.092 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█    █\n",
      "█ n@y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.229 - Visit counts=28\n",
      "Action  Up : Q-value=0.026 - Visit counts=12\n",
      "Action  Left : Q-value=-0.174 - Visit counts=9\n",
      "Action  Right : Q-value=0.212 - Visit counts=26\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█    █\n",
      "█ n@y█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.016 - Visit counts=17\n",
      "Action  Up : Q-value=0.101 - Visit counts=19\n",
      "Action  Left : Q-value=0.101 - Visit counts=19\n",
      "Action  Right : Q-value=0.120 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█    █\n",
      "█ n @█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.056 - Visit counts=21\n",
      "Action  Up : Q-value=0.139 - Visit counts=27\n",
      "Action  Left : Q-value=0.104 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█   @█\n",
      "█ n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.157 - Visit counts=23\n",
      "Action  Up : Q-value=-0.227 - Visit counts=8\n",
      "Action  Down : Q-value=0.092 - Visit counts=21\n",
      "Action  Left : Q-value=0.142 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  ? █\n",
      "█!   █\n",
      "█   @█\n",
      "█ n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      "Episode 150 - Total reward 10\n",
      "Values:  tensor([0.2228, 0.2188, 0.2230, 0.2230, 0.2230, 0.2230, 0.2188, 0.2230, 0.2250,\n",
      "        0.2156, 0.2250, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352,\n",
      "        0.2289, 0.2238, 0.2352, 0.2352, 0.2352, 0.2230, 0.2230, 0.2352, 0.2352,\n",
      "        0.2352, 0.2352, 0.2362, 0.2352, 0.2352, 0.2352, 0.2139, 0.2151, 0.2352,\n",
      "        0.2352, 0.2208, 0.2212, 0.2208, 0.2188, 0.2188, 0.2208, 0.2213, 0.2213,\n",
      "        0.2352, 0.2352, 0.2352, 0.2352, 0.2292, 0.2292, 0.2292, 0.2294, 0.2352,\n",
      "        0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352,\n",
      "        0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352,\n",
      "        0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352,\n",
      "        0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352, 0.2352,\n",
      "        0.2319, 0.2312, 0.2317, 0.2362, 0.2343, 0.2343, 0.2343, 0.2343, 0.2352,\n",
      "        0.2352], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.9910, 0.9940, 0.9970, 1.0000, 0.9881,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970,\n",
      "        1.0000, 0.9910, 0.9940, 0.9970, 1.0000, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910,\n",
      "        0.9940, 0.9970, 1.0000, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588,\n",
      "        0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851,\n",
      "        0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9733, 0.9763, 0.9792, 0.9821,\n",
      "        0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9704, 0.9733, 0.9763,\n",
      "        0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.5109\n",
      "Rollout time: 0.71 - Update time: 0.00\n",
      "\n",
      "Episode 151 - Total reward 5\n",
      "Loss: 0.3944\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 152 - Total reward 1\n",
      "Loss: 0.1043\n",
      "Rollout time: 1.06 - Update time: 0.00\n",
      "\n",
      "Episode 153 - Total reward 1\n",
      "Loss: 0.2937\n",
      "Rollout time: 0.91 - Update time: 0.00\n",
      "\n",
      "Episode 154 - Total reward 6\n",
      "Loss: 0.3571\n",
      "Rollout time: 0.77 - Update time: 0.00\n",
      "\n",
      "Episode 155 - Total reward 1\n",
      "Loss: 0.1466\n",
      "Rollout time: 1.35 - Update time: 0.00\n",
      "\n",
      "Episode 156 - Total reward 4\n",
      "Loss: 0.2590\n",
      "Rollout time: 0.86 - Update time: 0.00\n",
      "\n",
      "Episode 157 - Total reward 3\n",
      "Loss: 0.1965\n",
      "Rollout time: 0.88 - Update time: 0.00\n",
      "\n",
      "Episode 158 - Total reward 0\n",
      "Loss: 0.1372\n",
      "Rollout time: 1.16 - Update time: 0.00\n",
      "\n",
      "Episode 159 - Total reward 4\n",
      "Loss: 0.2119\n",
      "Rollout time: 0.83 - Update time: 0.00\n",
      "\n",
      "Episode 160 - Total reward 3\n",
      "Values:  tensor([0.3546, 0.3616, 0.3653, 0.3616, 0.3561, 0.3561, 0.3706, 0.3706, 0.3706,\n",
      "        0.3706, 0.3706, 0.3706, 0.3557, 0.3589, 0.3647, 0.3589, 0.3557, 0.3557,\n",
      "        0.3557, 0.3557, 0.3589, 0.3589, 0.3557, 0.3557, 0.3548, 0.3557, 0.3589,\n",
      "        0.3589, 0.3589, 0.3557, 0.3589, 0.3589, 0.3557, 0.3557, 0.3557, 0.3589,\n",
      "        0.3557, 0.3589, 0.3559, 0.3706, 0.3706, 0.3706, 0.3642, 0.3635, 0.3635,\n",
      "        0.3635, 0.3636, 0.3634, 0.3634, 0.3597, 0.3591, 0.3597, 0.3634, 0.3597,\n",
      "        0.3597, 0.3634, 0.3597, 0.3591, 0.3597, 0.3597, 0.3634, 0.3597, 0.3597,\n",
      "        0.3591, 0.3597, 0.3634, 0.3634, 0.3634, 0.3634, 0.3597, 0.3597, 0.3597,\n",
      "        0.3597, 0.3591, 0.3597, 0.3597, 0.3591, 0.3597, 0.3591, 0.3591, 0.3597,\n",
      "        0.3591, 0.3706, 0.3706, 0.3706, 0.3706, 0.3706, 0.3610, 0.3617, 0.3630,\n",
      "        0.3586, 0.3586, 0.3656, 0.3656, 0.3586, 0.3656, 0.3660, 0.3660, 0.3656,\n",
      "        0.3586], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.9166, 0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332,\n",
      "        0.9360, 0.9389, 0.9417, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588,\n",
      "        0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851,\n",
      "        0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.8762, 0.8788, 0.8814, 0.8841,\n",
      "        0.8868, 0.8894, 0.8921, 0.8948, 0.8975, 0.9002, 0.9029, 0.9056, 0.9083,\n",
      "        0.9111, 0.9138, 0.9166, 0.9193, 0.9221, 0.9249, 0.9276, 0.9304, 0.9332,\n",
      "        0.9360, 0.9389, 0.9417, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588,\n",
      "        0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851,\n",
      "        0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.3180\n",
      "Rollout time: 0.76 - Update time: 0.00\n",
      "\n",
      "Episode 161 - Total reward 1\n",
      "Loss: 0.1486\n",
      "Rollout time: 1.09 - Update time: 0.00\n",
      "\n",
      "Episode 162 - Total reward 3\n",
      "Loss: 0.1949\n",
      "Rollout time: 0.90 - Update time: 0.00\n",
      "\n",
      "Episode 163 - Total reward 0\n",
      "Loss: 0.1360\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 164 - Total reward 0\n",
      "Loss: 0.1336\n",
      "Rollout time: 1.57 - Update time: 0.00\n",
      "\n",
      "Episode 165 - Total reward 8\n",
      "Loss: 0.3123\n",
      "Rollout time: 0.68 - Update time: 0.00\n",
      "\n",
      "Episode 166 - Total reward 2\n",
      "Loss: 0.1472\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 167 - Total reward 3\n",
      "Loss: 0.2456\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 168 - Total reward 7\n",
      "Loss: 0.3346\n",
      "Rollout time: 0.66 - Update time: 0.00\n",
      "\n",
      "Episode 169 - Total reward 10\n",
      "Loss: 0.4151\n",
      "Rollout time: 0.58 - Update time: 0.00\n",
      "\n",
      "Episode 170 - Total reward 5\n",
      "Values:  tensor([0.3167, 0.3165, 0.3167, 0.3136, 0.3424, 0.3424, 0.3424, 0.3326, 0.3319,\n",
      "        0.3319, 0.3326, 0.3328, 0.3328, 0.3424, 0.3424, 0.3424, 0.3424, 0.3367,\n",
      "        0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3202, 0.3240, 0.3424, 0.3424,\n",
      "        0.3235, 0.3243, 0.3235, 0.3235, 0.3243, 0.3243, 0.3226, 0.3243, 0.3235,\n",
      "        0.3235, 0.3235, 0.3235, 0.3235, 0.3235, 0.3232, 0.3240, 0.3232, 0.3424,\n",
      "        0.3424, 0.3424, 0.3424, 0.3320, 0.3320, 0.3424, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9733, 0.9763, 0.9792,\n",
      "        0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.9851, 0.9881,\n",
      "        0.9910, 0.9940, 0.9970, 1.0000, 0.9910, 0.9940, 0.9970, 1.0000, 0.9417,\n",
      "        0.9445, 0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2610\n",
      "Rollout time: 0.87 - Update time: 0.00\n",
      "\n",
      "Episode 171 - Total reward 1\n",
      "Loss: 0.1398\n",
      "Rollout time: 1.48 - Update time: 0.00\n",
      "\n",
      "Episode 172 - Total reward 2\n",
      "Loss: 0.1697\n",
      "Rollout time: 1.27 - Update time: 0.00\n",
      "\n",
      "Episode 173 - Total reward 3\n",
      "Loss: 0.2964\n",
      "Rollout time: 1.00 - Update time: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 174 - Total reward 2\n",
      "Loss: 0.1777\n",
      "Rollout time: 1.33 - Update time: 0.00\n",
      "\n",
      "Episode 175 - Total reward 4\n",
      "Loss: 0.2602\n",
      "Rollout time: 0.78 - Update time: 0.00\n",
      "\n",
      "Episode 176 - Total reward 2\n",
      "Loss: 0.1822\n",
      "Rollout time: 1.37 - Update time: 0.00\n",
      "\n",
      "Episode 177 - Total reward 4\n",
      "Loss: 0.2124\n",
      "Rollout time: 1.10 - Update time: 0.00\n",
      "\n",
      "Episode 178 - Total reward 0\n",
      "Loss: 0.1560\n",
      "Rollout time: 1.58 - Update time: 0.00\n",
      "\n",
      "Episode 179 - Total reward 0\n",
      "Loss: 0.1339\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 180 - Total reward 2\n",
      "Values:  tensor([0.3347, 0.3417, 0.3361, 0.3361, 0.3361, 0.3360, 0.3360, 0.3438, 0.3417,\n",
      "        0.3417, 0.3417, 0.3361, 0.3361, 0.3361, 0.3360, 0.3361, 0.3361, 0.3360,\n",
      "        0.3360, 0.3360, 0.3361, 0.3361, 0.3361, 0.3361, 0.3360, 0.3361, 0.3361,\n",
      "        0.3361, 0.3361, 0.3361, 0.3361, 0.3361, 0.3361, 0.3361, 0.3361, 0.3361,\n",
      "        0.3417, 0.3361, 0.3417, 0.3347, 0.3417, 0.3347, 0.3417, 0.3361, 0.3360,\n",
      "        0.3361, 0.3361, 0.3417, 0.3438, 0.3360, 0.3361, 0.3361, 0.3361, 0.3417,\n",
      "        0.3361, 0.3361, 0.3360, 0.3438, 0.3417, 0.3361, 0.3361, 0.3361, 0.3361,\n",
      "        0.3417, 0.3417, 0.3417, 0.3438, 0.3417, 0.3438, 0.3414, 0.3365, 0.3543,\n",
      "        0.3543, 0.3490, 0.3471, 0.3471, 0.3471, 0.3490, 0.3446, 0.3449, 0.3543,\n",
      "        0.3543, 0.3543, 0.3543, 0.3439, 0.3439, 0.3439, 0.3435, 0.3435, 0.3386,\n",
      "        0.3387, 0.3386, 0.3386, 0.3386, 0.3386, 0.3387, 0.3387, 0.3386, 0.3387,\n",
      "        0.3439], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.8079, 0.8103, 0.8128, 0.8152, 0.8177, 0.8201, 0.8226, 0.8251, 0.8276,\n",
      "        0.8300, 0.8325, 0.8350, 0.8376, 0.8401, 0.8426, 0.8451, 0.8477, 0.8502,\n",
      "        0.8528, 0.8554, 0.8579, 0.8605, 0.8631, 0.8657, 0.8683, 0.8709, 0.8735,\n",
      "        0.8762, 0.8788, 0.8814, 0.8841, 0.8868, 0.8894, 0.8921, 0.8948, 0.8975,\n",
      "        0.9002, 0.9029, 0.9056, 0.9083, 0.9111, 0.9138, 0.9166, 0.9193, 0.9221,\n",
      "        0.9249, 0.9276, 0.9304, 0.9332, 0.9360, 0.9389, 0.9417, 0.9445, 0.9474,\n",
      "        0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675, 0.9704, 0.9733,\n",
      "        0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9704, 0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940,\n",
      "        0.9970, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.2942\n",
      "Rollout time: 0.89 - Update time: 0.00\n",
      "\n",
      "Episode 181 - Total reward 7\n",
      "Loss: 0.3408\n",
      "Rollout time: 0.84 - Update time: 0.00\n",
      "\n",
      "Episode 182 - Total reward 3\n",
      "Loss: 0.1628\n",
      "Rollout time: 1.24 - Update time: 0.00\n",
      "\n",
      "Episode 183 - Total reward 0\n",
      "Loss: 0.1205\n",
      "Rollout time: 1.55 - Update time: 0.00\n",
      "\n",
      "Episode 184 - Total reward 0\n",
      "Loss: 0.1099\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 185 - Total reward 5\n",
      "Loss: 0.3268\n",
      "Rollout time: 0.71 - Update time: 0.00\n",
      "\n",
      "Episode 186 - Total reward 1\n",
      "Loss: 0.1099\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 187 - Total reward 4\n",
      "Loss: 0.3664\n",
      "Rollout time: 1.05 - Update time: 0.00\n",
      "\n",
      "Episode 188 - Total reward 2\n",
      "Loss: 0.2374\n",
      "Rollout time: 0.99 - Update time: 0.00\n",
      "\n",
      "Episode 189 - Total reward 0\n",
      "Loss: 0.0975\n",
      "Rollout time: 1.54 - Update time: 0.00\n",
      "\n",
      "Episode 190 - Total reward 4\n",
      "Values:  tensor([0.3038, 0.3038, 0.3038, 0.3040, 0.3040, 0.3097, 0.3097, 0.3097, 0.3097,\n",
      "        0.3097, 0.3084, 0.3001, 0.3038, 0.3001, 0.3038, 0.3058, 0.3097, 0.3097,\n",
      "        0.3097, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097,\n",
      "        0.3097, 0.3097, 0.3028, 0.3028, 0.3029, 0.3029, 0.3029, 0.3028, 0.3029,\n",
      "        0.3029, 0.3029, 0.3029, 0.3029, 0.3028, 0.3028, 0.3028, 0.3028, 0.3029,\n",
      "        0.3029, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097, 0.3046, 0.3033,\n",
      "        0.3036, 0.3042, 0.3042, 0.3042, 0.3042, 0.3036, 0.3007, 0.3036, 0.3036,\n",
      "        0.3042, 0.3042, 0.3036, 0.3036, 0.3042, 0.3036, 0.3036, 0.3007, 0.3036,\n",
      "        0.3036, 0.3036, 0.3007, 0.3007, 0.3016, 0.3033, 0.3016, 0.3007, 0.3007,\n",
      "        0.3036, 0.3036, 0.3042, 0.3046, 0.3042, 0.3046, 0.3033, 0.3016, 0.3007,\n",
      "        0.3007, 0.3016, 0.3012, 0.3016, 0.3097, 0.3097, 0.3097, 0.3097, 0.3097,\n",
      "        0.3097], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9474, 0.9502, 0.9531, 0.9559, 0.9588, 0.9617, 0.9646, 0.9675, 0.9704,\n",
      "        0.9733, 0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970,\n",
      "        1.0000, 0.9360, 0.9389, 0.9417, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559,\n",
      "        0.9588, 0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821,\n",
      "        0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.8762, 0.8788, 0.8814,\n",
      "        0.8841, 0.8868, 0.8894, 0.8921, 0.8948, 0.8975, 0.9002, 0.9029, 0.9056,\n",
      "        0.9083, 0.9111, 0.9138, 0.9166, 0.9193, 0.9221, 0.9249, 0.9276, 0.9304,\n",
      "        0.9332, 0.9360, 0.9389, 0.9417, 0.9445, 0.9474, 0.9502, 0.9531, 0.9559,\n",
      "        0.9588, 0.9617, 0.9646, 0.9675, 0.9704, 0.9733, 0.9763, 0.9792, 0.9821,\n",
      "        0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.4122\n",
      "Rollout time: 0.73 - Update time: 0.00\n",
      "\n",
      "Episode 191 - Total reward 1\n",
      "Loss: 0.1363\n",
      "Rollout time: 1.17 - Update time: 0.00\n",
      "\n",
      "Episode 192 - Total reward 6\n",
      "Loss: 0.2374\n",
      "Rollout time: 0.93 - Update time: 0.00\n",
      "\n",
      "Episode 193 - Total reward 8\n",
      "Loss: 0.3389\n",
      "Rollout time: 0.69 - Update time: 0.00\n",
      "\n",
      "Episode 194 - Total reward 1\n",
      "Loss: 0.1343\n",
      "Rollout time: 1.08 - Update time: 0.00\n",
      "\n",
      "Episode 195 - Total reward 5\n",
      "Loss: 0.3120\n",
      "Rollout time: 0.82 - Update time: 0.00\n",
      "\n",
      "Episode 196 - Total reward 0\n",
      "Loss: 0.1317\n",
      "Rollout time: 1.48 - Update time: 0.00\n",
      "\n",
      "Episode 197 - Total reward 0\n",
      "Loss: 0.1331\n",
      "Rollout time: 0.06 - Update time: 0.00\n",
      "\n",
      "Episode 198 - Total reward 0\n",
      "Loss: 0.1297\n",
      "Rollout time: 1.15 - Update time: 0.00\n",
      "\n",
      "Episode 199 - Total reward 2\n",
      "Loss: 0.1411\n",
      "Rollout time: 1.40 - Update time: 0.00\n",
      "\n",
      "██████\n",
      "█   y█\n",
      "█!   █\n",
      "█    █\n",
      "█?n @█\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.275 - Visit counts=26\n",
      "Action  Up : Q-value=0.151 - Visit counts=15\n",
      "Action  Left : Q-value=-0.112 - Visit counts=9\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   y█\n",
      "█!   █\n",
      "█    █\n",
      "█?n @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.174 - Visit counts=26\n",
      "Action  Up : Q-value=0.174 - Visit counts=26\n",
      "Action  Left : Q-value=0.097 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█   y█\n",
      "█!   █\n",
      "█    █\n",
      "█?n @█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.052 - Visit counts=24\n",
      "Action  Up : Q-value=0.127 - Visit counts=33\n",
      "Action  Left : Q-value=0.035 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   y█\n",
      "█!   █\n",
      "█   @█\n",
      "█?n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.108 - Visit counts=24\n",
      "Action  Up : Q-value=0.262 - Visit counts=41\n",
      "Action  Down : Q-value=-0.110 - Visit counts=9\n",
      "Action  Left : Q-value=-0.166 - Visit counts=8\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   y█\n",
      "█!  @█\n",
      "█    █\n",
      "█?n  █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.108 - Visit counts=12\n",
      "Action  Up : Q-value=0.420 - Visit counts=49\n",
      "Action  Down : Q-value=0.277 - Visit counts=27\n",
      "Action  Left : Q-value=-0.987 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█!   █\n",
      "█    █\n",
      "█?n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.438 - Visit counts=35\n",
      "Action  Down : Q-value=0.377 - Visit counts=26\n",
      "Action  Left : Q-value=0.451 - Visit counts=37\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█!   █\n",
      "█    █\n",
      "█?n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.408 - Visit counts=16\n",
      "Action  Down : Q-value=0.374 - Visit counts=14\n",
      "Action  Left : Q-value=0.659 - Visit counts=42\n",
      "Action  Right : Q-value=0.374 - Visit counts=14\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█ @  █\n",
      "█!   █\n",
      "█    █\n",
      "█?n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.593 - Visit counts=15\n",
      "Action  Down : Q-value=-0.111 - Visit counts=3\n",
      "Action  Left : Q-value=0.891 - Visit counts=64\n",
      "Action  Right : Q-value=0.400 - Visit counts=9\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█@   █\n",
      "█!   █\n",
      "█    █\n",
      "█?n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.828 - Visit counts=28\n",
      "Action  Down : Q-value=1.000 - Visit counts=78\n",
      "Action  Right : Q-value=0.421 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█@   █\n",
      "█    █\n",
      "█?n  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.125 - Visit counts=13\n",
      "Action  Up : Q-value=-0.200 - Visit counts=5\n",
      "Action  Down : Q-value=-0.049 - Visit counts=7\n",
      "Action  Left : Q-value=0.264 - Visit counts=21\n",
      "Action  Right : Q-value=-0.335 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█@   █\n",
      "█y   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.298 - Visit counts=24\n",
      "Action  Up : Q-value=-0.199 - Visit counts=5\n",
      "Action  Down : Q-value=0.264 - Visit counts=21\n",
      "Action  Right : Q-value=0.261 - Visit counts=20\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█@   █\n",
      "█y   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.095 - Visit counts=17\n",
      "Action  Up : Q-value=-0.033 - Visit counts=11\n",
      "Action  Down : Q-value=0.281 - Visit counts=29\n",
      "Action  Right : Q-value=0.080 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█    █\n",
      "█@   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.346 - Visit counts=33\n",
      "Action  Up : Q-value=0.217 - Visit counts=18\n",
      "Action  Right : Q-value=0.302 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█    █\n",
      "█@   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.125 - Visit counts=26\n",
      "Action  Up : Q-value=0.108 - Visit counts=24\n",
      "Action  Right : Q-value=0.142 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█    █\n",
      "█ @  █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.163 - Visit counts=16\n",
      "Action  Up : Q-value=-0.112 - Visit counts=9\n",
      "Action  Left : Q-value=0.241 - Visit counts=23\n",
      "Action  Right : Q-value=0.307 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█    █\n",
      "█  @ █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.164 - Visit counts=8\n",
      "Action  Up : Q-value=-0.551 - Visit counts=3\n",
      "Action  Left : Q-value=0.274 - Visit counts=25\n",
      "Action  Right : Q-value=0.516 - Visit counts=46\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█    █\n",
      "█   @█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.380 - Visit counts=13\n",
      "Action  Up : Q-value=0.850 - Visit counts=77\n",
      "Action  Left : Q-value=0.063 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?!█\n",
      "█   @█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.238 - Visit counts=7\n",
      "Action  Up : Q-value=1.000 - Visit counts=108\n",
      "Action  Down : Q-value=0.220 - Visit counts=6\n",
      "Action  Left : Q-value=0.196 - Visit counts=5\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█ n?@█\n",
      "█    █\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.238 - Visit counts=14\n",
      "Action  Up : Q-value=-1.000 - Visit counts=3\n",
      "Action  Down : Q-value=-0.200 - Visit counts=15\n",
      "Action  Right : Q-value=-0.113 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?   █\n",
      "█ @  █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.132 - Visit counts=23\n",
      "Action  Up : Q-value=-0.200 - Visit counts=20\n",
      "Action  Down : Q-value=-0.404 - Visit counts=9\n",
      "Action  Left : Q-value=-0.405 - Visit counts=9\n",
      "Action  Right : Q-value=-0.554 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?   █\n",
      "█ @  █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.152 - Visit counts=22\n",
      "Action  Up : Q-value=-0.282 - Visit counts=13\n",
      "Action  Down : Q-value=-0.994 - Visit counts=3\n",
      "Action  Left : Q-value=-0.728 - Visit counts=5\n",
      "Action  Right : Q-value=-0.129 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?   █\n",
      "█  @ █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.091 - Visit counts=19\n",
      "Action  Up : Q-value=-0.038 - Visit counts=29\n",
      "Action  Down : Q-value=-1.000 - Visit counts=3\n",
      "Action  Left : Q-value=-0.331 - Visit counts=12\n",
      "Action  Right : Q-value=-0.200 - Visit counts=15\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.033 - Visit counts=22\n",
      "Action  Up : Q-value=-0.330 - Visit counts=8\n",
      "Action  Down : Q-value=-0.466 - Visit counts=5\n",
      "Action  Left : Q-value=-0.259 - Visit counts=9\n",
      "Action  Right : Q-value=0.092 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.072 - Visit counts=26\n",
      "Action  Up : Q-value=0.122 - Visit counts=26\n",
      "Action  Down : Q-value=-0.113 - Visit counts=12\n",
      "Action  Left : Q-value=0.048 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.071 - Visit counts=31\n",
      "Action  Down : Q-value=0.062 - Visit counts=20\n",
      "Action  Left : Q-value=-0.002 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.113 - Visit counts=56\n",
      "Action  Down : Q-value=-0.423 - Visit counts=7\n",
      "Action  Left : Q-value=-0.137 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.094 - Visit counts=40\n",
      "Action  Down : Q-value=0.046 - Visit counts=33\n",
      "Action  Left : Q-value=0.079 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.075 - Visit counts=42\n",
      "Action  Down : Q-value=-0.004 - Visit counts=24\n",
      "Action  Left : Q-value=-0.075 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.025 - Visit counts=35\n",
      "Action  Down : Q-value=-0.004 - Visit counts=28\n",
      "Action  Left : Q-value=-0.003 - Visit counts=28\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.199 - Visit counts=15\n",
      "Action  Down : Q-value=0.061 - Visit counts=35\n",
      "Action  Left : Q-value=0.054 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.096 - Visit counts=23\n",
      "Action  Up : Q-value=0.149 - Visit counts=30\n",
      "Action  Down : Q-value=-0.052 - Visit counts=14\n",
      "Action  Left : Q-value=0.015 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.112 - Visit counts=12\n",
      "Action  Down : Q-value=0.022 - Visit counts=26\n",
      "Action  Left : Q-value=0.100 - Visit counts=41\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.105 - Visit counts=30\n",
      "Action  Down : Q-value=-0.178 - Visit counts=13\n",
      "Action  Left : Q-value=-0.153 - Visit counts=11\n",
      "Action  Right : Q-value=0.142 - Visit counts=36\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.125 - Visit counts=46\n",
      "Action  Down : Q-value=-0.143 - Visit counts=14\n",
      "Action  Left : Q-value=0.010 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.003 - Visit counts=24\n",
      "Action  Down : Q-value=0.100 - Visit counts=41\n",
      "Action  Left : Q-value=0.062 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.086 - Visit counts=33\n",
      "Action  Up : Q-value=0.044 - Visit counts=28\n",
      "Action  Down : Q-value=-0.181 - Visit counts=13\n",
      "Action  Left : Q-value=-0.085 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.195 - Visit counts=41\n",
      "Action  Up : Q-value=0.096 - Visit counts=23\n",
      "Action  Down : Q-value=-0.988 - Visit counts=2\n",
      "Action  Left : Q-value=-0.003 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.112 - Visit counts=31\n",
      "Action  Up : Q-value=0.149 - Visit counts=30\n",
      "Action  Down : Q-value=-0.113 - Visit counts=12\n",
      "Action  Left : Q-value=0.015 - Visit counts=17\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.086 - Visit counts=16\n",
      "Action  Down : Q-value=0.008 - Visit counts=29\n",
      "Action  Left : Q-value=0.054 - Visit counts=34\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.147 - Visit counts=37\n",
      "Action  Down : Q-value=-0.080 - Visit counts=13\n",
      "Action  Left : Q-value=-0.259 - Visit counts=9\n",
      "Action  Right : Q-value=0.052 - Visit counts=24\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.098 - Visit counts=29\n",
      "Action  Down : Q-value=-0.152 - Visit counts=11\n",
      "Action  Left : Q-value=-0.239 - Visit counts=7\n",
      "Action  Right : Q-value=0.156 - Visit counts=39\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.165 - Visit counts=33\n",
      "Action  Down : Q-value=0.105 - Visit counts=24\n",
      "Action  Left : Q-value=0.154 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.100 - Visit counts=35\n",
      "Action  Down : Q-value=0.090 - Visit counts=28\n",
      "Action  Left : Q-value=-0.021 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.165 - Visit counts=33\n",
      "Action  Down : Q-value=0.048 - Visit counts=19\n",
      "Action  Left : Q-value=0.119 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.096 - Visit counts=23\n",
      "Action  Down : Q-value=0.098 - Visit counts=29\n",
      "Action  Left : Q-value=0.149 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.081 - Visit counts=27\n",
      "Action  Down : Q-value=0.075 - Visit counts=21\n",
      "Action  Left : Q-value=-0.465 - Visit counts=5\n",
      "Action  Right : Q-value=0.122 - Visit counts=26\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.149 - Visit counts=30\n",
      "Action  Down : Q-value=0.220 - Visit counts=38\n",
      "Action  Left : Q-value=-0.238 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.050 - Visit counts=21\n",
      "Action  Up : Q-value=0.019 - Visit counts=30\n",
      "Action  Down : Q-value=-0.180 - Visit counts=13\n",
      "Action  Left : Q-value=-0.018 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.151 - Visit counts=38\n",
      "Action  Down : Q-value=0.026 - Visit counts=22\n",
      "Action  Left : Q-value=-0.019 - Visit counts=19\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.157 - Visit counts=47\n",
      "Action  Down : Q-value=-0.040 - Visit counts=18\n",
      "Action  Left : Q-value=-0.033 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.145 - Visit counts=44\n",
      "Action  Down : Q-value=0.062 - Visit counts=30\n",
      "Action  Left : Q-value=-0.033 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.052 - Visit counts=29\n",
      "Action  Down : Q-value=0.029 - Visit counts=31\n",
      "Action  Left : Q-value=0.046 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.071 - Visit counts=31\n",
      "Action  Down : Q-value=0.070 - Visit counts=31\n",
      "Action  Left : Q-value=0.077 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.048 - Visit counts=19\n",
      "Action  Down : Q-value=-0.333 - Visit counts=6\n",
      "Action  Left : Q-value=-0.993 - Visit counts=2\n",
      "Action  Right : Q-value=0.186 - Visit counts=38\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.159 - Visit counts=40\n",
      "Action  Down : Q-value=0.072 - Visit counts=26\n",
      "Action  Left : Q-value=0.012 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.186 - Visit counts=38\n",
      "Action  Down : Q-value=0.072 - Visit counts=26\n",
      "Action  Left : Q-value=0.063 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.165 - Visit counts=33\n",
      "Action  Down : Q-value=0.165 - Visit counts=33\n",
      "Action  Left : Q-value=0.075 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=-0.003 - Visit counts=28\n",
      "Action  Down : Q-value=0.037 - Visit counts=32\n",
      "Action  Left : Q-value=-0.033 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.141 - Visit counts=36\n",
      "Action  Up : Q-value=0.082 - Visit counts=27\n",
      "Action  Down : Q-value=-0.152 - Visit counts=11\n",
      "Action  Left : Q-value=-0.426 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.123 - Visit counts=26\n",
      "Action  Up : Q-value=0.205 - Visit counts=44\n",
      "Action  Down : Q-value=-0.662 - Visit counts=4\n",
      "Action  Left : Q-value=-0.152 - Visit counts=11\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.165 - Visit counts=33\n",
      "Action  Down : Q-value=0.081 - Visit counts=27\n",
      "Action  Left : Q-value=0.165 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.093 - Visit counts=34\n",
      "Action  Down : Q-value=0.072 - Visit counts=26\n",
      "Action  Left : Q-value=0.027 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.033 - Visit counts=27\n",
      "Action  Down : Q-value=0.071 - Visit counts=31\n",
      "Action  Left : Q-value=0.063 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.062 - Visit counts=30\n",
      "Action  Up : Q-value=0.011 - Visit counts=25\n",
      "Action  Down : Q-value=-0.051 - Visit counts=21\n",
      "Action  Left : Q-value=-0.658 - Visit counts=4\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.179 - Visit counts=13\n",
      "Action  Up : Q-value=0.070 - Visit counts=31\n",
      "Action  Down : Q-value=-0.332 - Visit counts=8\n",
      "Action  Left : Q-value=0.081 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.167 - Visit counts=25\n",
      "Action  Up : Q-value=0.015 - Visit counts=17\n",
      "Action  Down : Q-value=-0.153 - Visit counts=11\n",
      "Action  Left : Q-value=-0.994 - Visit counts=2\n",
      "Action  Right : Q-value=0.074 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.117 - Visit counts=19\n",
      "Action  Up : Q-value=0.137 - Visit counts=21\n",
      "Action  Down : Q-value=-0.239 - Visit counts=7\n",
      "Action  Left : Q-value=-0.334 - Visit counts=6\n",
      "Action  Right : Q-value=0.075 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.238 - Visit counts=7\n",
      "Action  Down : Q-value=0.016 - Visit counts=17\n",
      "Action  Left : Q-value=-0.025 - Visit counts=15\n",
      "Action  Right : Q-value=0.197 - Visit counts=31\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.188 - Visit counts=29\n",
      "Action  Down : Q-value=0.192 - Visit counts=30\n",
      "Action  Left : Q-value=0.074 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.050 - Visit counts=14\n",
      "Action  Up : Q-value=0.048 - Visit counts=19\n",
      "Action  Down : Q-value=-0.200 - Visit counts=10\n",
      "Action  Left : Q-value=0.142 - Visit counts=36\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.073 - Visit counts=21\n",
      "Action  Up : Q-value=0.086 - Visit counts=22\n",
      "Action  Down : Q-value=-0.155 - Visit counts=11\n",
      "Action  Left : Q-value=-0.260 - Visit counts=9\n",
      "Action  Right : Q-value=0.085 - Visit counts=22\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.127 - Visit counts=20\n",
      "Action  Down : Q-value=-0.034 - Visit counts=11\n",
      "Action  Left : Q-value=0.048 - Visit counts=19\n",
      "Action  Right : Q-value=0.137 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.107 - Visit counts=36\n",
      "Action  Down : Q-value=0.119 - Visit counts=32\n",
      "Action  Left : Q-value=-0.990 - Visit counts=2\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.016 - Visit counts=17\n",
      "Action  Up : Q-value=0.215 - Visit counts=36\n",
      "Action  Down : Q-value=0.073 - Visit counts=21\n",
      "Action  Left : Q-value=-0.238 - Visit counts=7\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.186 - Visit counts=38\n",
      "Action  Down : Q-value=0.034 - Visit counts=18\n",
      "Action  Left : Q-value=0.098 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.072 - Visit counts=26\n",
      "Action  Down : Q-value=0.131 - Visit counts=34\n",
      "Action  Left : Q-value=0.081 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.063 - Visit counts=30\n",
      "Action  Up : Q-value=0.027 - Visit counts=22\n",
      "Action  Down : Q-value=-0.334 - Visit counts=8\n",
      "Action  Left : Q-value=-0.017 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.026 - Visit counts=22\n",
      "Action  Up : Q-value=0.062 - Visit counts=25\n",
      "Action  Down : Q-value=-0.021 - Visit counts=19\n",
      "Action  Left : Q-value=-0.181 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.082 - Visit counts=27\n",
      "Action  Down : Q-value=-0.040 - Visit counts=18\n",
      "Action  Left : Q-value=0.097 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.222 - Visit counts=12\n",
      "Action  Down : Q-value=-0.061 - Visit counts=17\n",
      "Action  Left : Q-value=-0.660 - Visit counts=4\n",
      "Action  Right : Q-value=0.120 - Visit counts=45\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.058 - Visit counts=24\n",
      "Action  Down : Q-value=0.077 - Visit counts=37\n",
      "Action  Left : Q-value=0.046 - Visit counts=33\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action  Stay : Q-value=0.062 - Visit counts=30\n",
      "Action  Up : Q-value=0.122 - Visit counts=39\n",
      "Action  Down : Q-value=-0.151 - Visit counts=11\n",
      "Action  Left : Q-value=-0.549 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.111 - Visit counts=37\n",
      "Action  Down : Q-value=-0.050 - Visit counts=21\n",
      "Action  Left : Q-value=0.063 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.136 - Visit counts=35\n",
      "Action  Down : Q-value=-0.199 - Visit counts=10\n",
      "Action  Left : Q-value=0.132 - Visit counts=41\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.042 - Visit counts=25\n",
      "Action  Down : Q-value=0.007 - Visit counts=29\n",
      "Action  Left : Q-value=0.018 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.403 - Visit counts=9\n",
      "Action  Down : Q-value=-0.058 - Visit counts=24\n",
      "Action  Left : Q-value=-0.238 - Visit counts=14\n",
      "Action  Right : Q-value=-0.003 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.184 - Visit counts=18\n",
      "Action  Down : Q-value=0.012 - Visit counts=42\n",
      "Action  Left : Q-value=-0.174 - Visit counts=21\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█?  @█\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.028 - Visit counts=31\n",
      "Action  Up : Q-value=-0.032 - Visit counts=22\n",
      "Action  Down : Q-value=-0.272 - Visit counts=11\n",
      "Action  Left : Q-value=0.032 - Visit counts=27\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.005 - Visit counts=20\n",
      "Action  Up : Q-value=0.026 - Visit counts=22\n",
      "Action  Down : Q-value=-0.261 - Visit counts=9\n",
      "Action  Left : Q-value=-0.114 - Visit counts=12\n",
      "Action  Right : Q-value=-0.079 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.151 - Visit counts=11\n",
      "Action  Down : Q-value=-0.152 - Visit counts=11\n",
      "Action  Left : Q-value=-0.005 - Visit counts=20\n",
      "Action  Right : Q-value=0.099 - Visit counts=29\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.141 - Visit counts=43\n",
      "Action  Down : Q-value=-0.021 - Visit counts=19\n",
      "Action  Left : Q-value=-0.003 - Visit counts=16\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.132 - Visit counts=41\n",
      "Action  Down : Q-value=0.022 - Visit counts=26\n",
      "Action  Left : Q-value=0.010 - Visit counts=25\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.122 - Visit counts=39\n",
      "Action  Down : Q-value=0.061 - Visit counts=25\n",
      "Action  Left : Q-value=0.073 - Visit counts=26\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.044 - Visit counts=28\n",
      "Action  Down : Q-value=0.062 - Visit counts=25\n",
      "Action  Left : Q-value=0.100 - Visit counts=35\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.070 - Visit counts=31\n",
      "Action  Down : Q-value=-0.061 - Visit counts=17\n",
      "Action  Left : Q-value=-0.658 - Visit counts=4\n",
      "Action  Right : Q-value=0.079 - Visit counts=32\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.086 - Visit counts=22\n",
      "Action  Down : Q-value=0.053 - Visit counts=29\n",
      "Action  Left : Q-value=0.062 - Visit counts=30\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.051 - Visit counts=24\n",
      "Action  Down : Q-value=0.051 - Visit counts=24\n",
      "Action  Left : Q-value=0.039 - Visit counts=23\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█y  @█\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.078 - Visit counts=26\n",
      "Action  Down : Q-value=-0.723 - Visit counts=5\n",
      "Action  Left : Q-value=0.043 - Visit counts=42\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.018 - Visit counts=23\n",
      "Action  Down : Q-value=0.022 - Visit counts=26\n",
      "Action  Left : Q-value=-0.002 - Visit counts=24\n",
      "Action  Right : Q-value=-0.112 - Visit counts=18\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█y   █\n",
      "█? @ █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.096 - Visit counts=23\n",
      "Action  Up : Q-value=0.098 - Visit counts=29\n",
      "Action  Down : Q-value=-0.990 - Visit counts=2\n",
      "Action  Left : Q-value=-0.257 - Visit counts=9\n",
      "Action  Right : Q-value=-0.113 - Visit counts=12\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█y @ █\n",
      "█?   █\n",
      "█    █\n",
      "█ n! █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      "Episode 200 - Total reward 2\n",
      "Values:  tensor([0.3319, 0.3319, 0.3319, 0.3333, 0.3320, 0.3344, 0.3344, 0.3344, 0.3344,\n",
      "        0.3344, 0.3318, 0.3318, 0.3344, 0.3344, 0.3344, 0.3344, 0.3344, 0.3344,\n",
      "        0.3344, 0.3338, 0.3338, 0.3320, 0.3324, 0.3312, 0.3316, 0.3316, 0.3316,\n",
      "        0.3316, 0.3316, 0.3316, 0.3312, 0.3316, 0.3285, 0.3316, 0.3316, 0.3312,\n",
      "        0.3312, 0.3312, 0.3316, 0.3285, 0.3285, 0.3316, 0.3316, 0.3316, 0.3316,\n",
      "        0.3285, 0.3316, 0.3312, 0.3316, 0.3316, 0.3316, 0.3316, 0.3316, 0.3285,\n",
      "        0.3316, 0.3316, 0.3316, 0.3316, 0.3312, 0.3312, 0.3316, 0.3316, 0.3316,\n",
      "        0.3312, 0.3312, 0.3324, 0.3324, 0.3285, 0.3316, 0.3312, 0.3324, 0.3285,\n",
      "        0.3316, 0.3312, 0.3316, 0.3316, 0.3312, 0.3312, 0.3316, 0.3285, 0.3316,\n",
      "        0.3312, 0.3316, 0.3316, 0.3316, 0.3285, 0.3316, 0.3312, 0.3324, 0.3285,\n",
      "        0.3316, 0.3316, 0.3316, 0.3316, 0.3285, 0.3316, 0.3316, 0.3316, 0.3285,\n",
      "        0.3324], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Targets:  tensor([0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.9763, 0.9792, 0.9821, 0.9851, 0.9881, 0.9910, 0.9940, 0.9970, 1.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n",
      "Loss: 0.1671\n",
      "Rollout time: 1.12 - Update time: 0.00\n"
     ]
    }
   ],
   "source": [
    "n_updates = 200\n",
    "total_rewards = []\n",
    "episode_lengths = []\n",
    "losses = []\n",
    "for i in range(n_updates):\n",
    "    t0 = time.time()\n",
    "    if (i+1)%50 == 0:\n",
    "        render=True\n",
    "    else:\n",
    "        render=False\n",
    "    total_reward, frame_lst, reward_lst, done_lst = play_rollout_value_net(value_net,\n",
    "                                                                        game_simulator,\n",
    "                                                                        episode_length,\n",
    "                                                                        ucb_C,\n",
    "                                                                        discount,\n",
    "                                                                        max_actions,\n",
    "                                                                        num_simulations,\n",
    "                                                                        render\n",
    "                                                                        )\n",
    "    t1 = time.time()\n",
    "    total_rewards.append(total_reward)\n",
    "    episode_lengths.append(len(reward_lst))\n",
    "    values, targets = batch_episode(frame_lst, reward_lst, done_lst, value_net, discount)\n",
    "    print(\"\\nEpisode %d - Total reward %d\"%(i+1, total_reward))\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"Values: \", values.squeeze())\n",
    "        print(\"Targets: \", targets)\n",
    "    loss = loss_fn(values.squeeze(), targets)\n",
    "    print(\"Loss: %.4f\"%loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    t2 = time.time()\n",
    "    rollout_time = (t1-t0)/60\n",
    "    update_time = (t2-t1)/60\n",
    "    print(\"Rollout time: %.2f - Update time: %.2f\"%(rollout_time, update_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the value network predictions and the MC return, the network gets stuck either by playing Stay or two complementary actions that as a result cancel each other (e.g. Left+Right).\n",
    "\n",
    "Second option tried: during the simulation/prediction of a leaf node, take at most n_steps of random rollouts and then bootstrap the end of the trajectory with the value network. This helps to mix the unbiased information from the true simulator and the low variance estimate of the value network.\n",
    "\n",
    "Still it doesn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9e7QsSVkn+ovMrKr9OK/uPtsGuqUbGERBVKAdQa8iNHrVwcdSl0tmcLwz3sV4nTsg6vUOa8YLel3jA8GrgsqjFUaQUZBBdOQ10M2jhcZu6IZuGvpBN/3u3uecPufsvWvvqsrMuH9EfhFfREZWZWVmVe2qzm+tvfbe9ciMzIz44he/7/d9IaSUaK211lpr7bFjwaIb0FprrbXW2nytdfyttdZaa48xax1/a6211tpjzFrH31prrbX2GLPW8bfWWmutPcYsWnQDytjJkyfl5ZdfvuhmtNZaa60tld1www2npJRb7utL4fgvv/xyXH/99YtuRmuttdbaUpkQ4mu+11uqp7XWWmvtMWat42+ttdZae4xZ6/hba6211h5j1jr+1lprrbXHmLWOv7XWWmvtMWYzc/xCiD8TQjwihLiZvfZaIcSXhRBfEEL8dyHEiVmdv7XWWmutNb/NEvG/DcAPOK99BMA3Sym/BcBtAF41w/O31lprrbXmsZk5finlJwCccV77sJQyzv79DIBLZ3X+1h47lqYSf339vYiTdNFNWSm790wfn7hte9HNaG0GtkiO/98C+EDRm0KIlwkhrhdCXL+93Xa+1ortC/efw6++5wu47q4zkz/cWmm76lN34ZV/deOim9HaDGwhjl8I8Z8AxADeWfQZKeWbpZRXSCmv2NrKZRy31pq2UYb0hy3ib9SGSdre0xW1uZdsEEL8LIAXA7hSttt/tdaApam0frfWjCWJbO/pitpcHb8Q4gcA/N8Ani+l7M/z3K2triUZfmh9VLOWSNne0xW1Wco53wXg0wCeJoS4TwjxcwDeAOAogI8IIW4UQvzprM7f2mPHaN2YtF6qUUtTqSfV1lbLZob4pZQv8bx81azO19pj11KN+Fsn1aTFaUv1rKq1mbutLb2Rb2odf7OmqJ72nq6itY6/taU3ck4t1dOsJYni+FsNxupZ6/hbW3qTLdUzE2uD5qtrreNvbemNpOZpKzlv1LRMtp1QV85ax9/a0pumeloH1ajFaUuhraq1jr+1pTdN9bQOqlFr1VKra63jb23pzah6FtuOVbMkbTn+VbXW8be29NZSPbOxlupZXWsdf2tLbxrxtw6qUWtrIK2utY6/taW3Vn0yG4vb+7qy1jr+GdoHb34IL3r9x2svlfvDGN/12x/DZ756uqGWHQ5727V34aff/Onax2kTuGZjRRTah295CFe+7pp245slttbxz9Du3N7FHY/sYhAntY7zaH+E+8/u445Hdhtq2eGwLz+0g1sf3Kl9HPL3LTBt1mgide/rHdu7uHN7Dwdx6/iX1VrHP0NLGgqOpSsaZBvEaSP8cRvcnY0V9d9V7Y+PJWsd/wxNy+FqAiNybKMVW1oP4qQR/li2VM9MrNDx6xVWe7+X1VrHP0NrConSQItXzLENRmkjKL11RLOxIqqnqZVsa4uz1vHP0BqjerKRt2rBtEGcNpIcZO5z/WO1ZiwpAC6yLd629NY6/hla0lDKOw20lUP8cdIIxy9bjn8mVgRcmurXrS3OWsc/Q2sqCKapnmS1Btowbqmew2yG6nEcf2q/39ryWev4Z2hNDRAd3F2xusODOIVsYKOPVsc/G9OIv5Dqae/3slrr+GdoTVU3pAG4aoh/kOnA6/pr+n5L9TRrhVRPQ2q11hZnreOfoTUV3CV/tmqIdjBSiW31749ffdJaPUsK7mtR0Le15bHW8c/QmgqCra6OnxB/MyuiVZsYF21F95UeV0v1LK+1jn+GljYkM1zV4G5Tjj9d0RXRoq2I4zdUT3u/l9Vaxz9Da1rHv3rB3WaonlRTEq0jatIKVT0t1bP0NjPHL4T4MyHEI0KIm9lrFwohPiKEuD37fcGszn8YrGkd/yoh2iSVGCXNJAK1Ov7ZWFFinNnqct4taq0pmyXifxuAH3Be+48APiqlfCqAj2b/r6y1Ov5iG7LKjnUpg3brxdlYEXAxWzK2N3xZbWaOX0r5CQBnnJd/FMDbs7/fDuDHZnX+w2Dkp+siURpoqxDcveWBc7j6y49Yparr1zJqOeemLU2lCeIWFGmbFtDceO9ZfOr2U000r7R96YHz+NiXH57rOZfB5s3xXyylfBAAst9fV/RBIcTLhBDXCyGu397enlsDm7Smtq7TtXpWwLFd9cm78J/fd7MO7AINBHdbVU/jxidjd2KuuuPZG6++A7/1gVvrN24Ke8snv4rXvP9Lcz3nMtihDe5KKd8spbxCSnnF1tbWoptTyZrW8a+C4x+lEnvDGIMRp3rqHbOlepo33mfd+1o1djVKUovim4ftD5O5n3MZbN6O/2EhxOMBIPv9yJzPP1drSv2wStU501SiP0wsqqepPIeWc27OLMdfQPVMO9EmqZw7eBnECeI2Cp2zeTv+9wP42ezvnwXwt3M+/1zNUD01j7NCwd0klRjGKfaGifVaHWt1/M2bRfU0tANXKuXc41SDONXqsdaMzVLO+S4AnwbwNCHEfUKInwPw2wC+TwhxO4Dvy/5fWWsa8a+Cjp+u5dH+MPdaVWuLhjVvqUX1NJPAlaRy7pPzIE5bQOCxaFYHllK+pOCtK2d1zsNmTWU4rlLwUjv+Pe74mzlm6/ibs3iM4zf3e7pjpinmjr4HcbISarim7dAGd1fBmioXTF9fhSUr3YszzPHXvT9tffjmjYMV129W3VI0kXLufPtglK6EKKJpax3/DK2o1sm0tlLB3exWNEn1VEWgrRXbOMRfh+qZd5yKqJ62nIdtreOfoRG4qUv1rNLWi+REzuyNcq9VNVNCYPnvz2GxZCzV4399kqULQPwk5VyF1XKT1jr+GVpzwV31exVkaeQszvabo3rajViaN+7Uc6qeihTmYhB/M4UAV81axz9Da47jJ6pn+Tuvj+OvL3dtqZ6mzaZ67PeqBtNJxz9P2oUyxFdBEdektY5/hlY1td01U6tn+T0bjb9mOX469vLfn8NilpyzaOvFaVU9C6AsyfGvAmhq0lrHP0PTVE9NsGFKNiw/avFx/LWpsBWSux4WGyvnrKiimvdOaXFiNPyrIIxo0lrHP0NLGgruaspoBVBL4uH46y79Wx1/88adc65IW8X7bWTJ83HCvBDgKggjmrTW8c/QNBJtiMpYBZ6S7gkfiE1tTdk6/uaM38sc1VPZ8c83VmU5/hUATU1a6/hnaEkb3M2Z71bUr17aBnebtrHB3YKduSaZjlXNCcDwQoCrAJqatELHn22TWPgzz0YuqzUV3OU6/mVPRPE5+Wmv6X9/+/X49b+7Rf/flHqqqu0OYnzbb3wYn7x9OfeN8JmduduQjn/OHD8v/b0KoKlJG1er5wYAEoAA8EQAj2Z/nwBwD4Anzbx1S27NIX52zFQiCkWt4y3SfM5iWirsrlO7NhWxYKrnbH+Is/0R7jnTX8j5Z2EzydxdJNXTIn7LChG/lPJJUsonA/gQgB+WUp6UUl4E4MUA3juvBi6zNaVi4N9f9iCV1/FPeU2q1K4ZyE1tal/VirYoXGYbV52zcq2e7JHNL7hrqJ4W8dtWhuP/dinlP9A/UsoPAHj+7Jq0OtY01QMs/767fqpnumO4pXalXlnValplq6prP8xm1+O336tcnXPOOv4W8RdbmbLMp4QQ/xnAO6Con5cCOD3TVq2IJQ0FHfn3lx258GsJA1GpRvtglFj3oamaSFWtKUrvMNmsirQB8+vDfMvFVUh+bNLKIP6XANgC8N+zn63stdYmWFPlgvnAWyWqZ6MTApieMhjEqaXSWLSOv6mV3WGycZm7dJlT78Clpbzzp3pWaVJuwsYifiFECOAPpZQvnVN7Vsq0Q2owuLvsS1Y+ANe7IXYG8VSqHiklBnFqI35yRAtyvIuOMczCrDwLF/FXrdVDO8nNK7g74oh/ucdN0zYW8UspEwBbQojunNqzUtZUPX7uGJee6nEcPzAdNz/MPhx7OP6FUT0Vde2H2crV6qlK9Swgc3fJx03TVobjvxvAtUKI9wPYoxellK+fVaNWxdKKXKhrHCUvO3Lht2K9AtVjim75VD0NNLCC6RjDCiF+/kzc+2qonumOOe98C0vVs+Qr5aatjON/IPsJABydbXNWy5quxw8sP1fJ78VGhvinoXpo+e7LLF3UvUkWvOKYhY2r1VMX8Y8WoupZnWfThE10/FLKX59HQ1bRmqIA+ABbdnUCd/KG6pkG8SsUx1c+csEce1OU3mGycTtwVZnopJRmQ6F5UT1t5m6hTXT8QogtAL8K4BkA1uh1KeULZ9iulbCm1CYWx7/kS9YkldjohugPE6x3VPebBowRivM5poWpelawVlA8huOvUhuJf3ZuwV1eq2fJKdKmrYyc850AvgxVouHXoTj/f5phm1bGmsrcXcSgmZUlqcTRNeXwCfFPgxwJxY08Ov6FJ3CtkOe3a/XY71VZ4VjUUUv1LNzKOP6LpJRXARhJKT8upfy3AJ4743YtvfGlbZM6/mXn+KUEjq51AADrHdX9pgvuKhQXe3T8iypg11T57cNkdnC3gOOfoi+mC1i1to6/2Mo4ftoq6UEhxL8QQjwLwKV1TiqEeKUQ4hYhxM1CiHcJIdYmf2u5jPez2lsL8lo9S75kTaTEkZ5C/BtdonrK35+hZyu9qrVjmrJV1PGP4/h1baKKiH9+On5eq2e5x03TVsbx/6YQ4jiAXwbwKwDeCuCVVU8ohLgEwMsBXCGl/GYAIYCfrnq8w2pNLm0tqmfJkQunetY6FagecvwW4jfHXoStItVD10RlNaz3Kky0/LPzcsLDJMVmRie2wV3byjj+/ymlPCelvFlK+QIp5XOklO+ved4IwLoQIgKwASUXXSlLxyyV6xxr2ZGLonoI8VdR9RQj/kUBbqNPz7/3l9fdg69u7wIA/uaG+3Drg+cnHm8Yp/jDj96O/WEy8bOzMnomnVDkN2KpoOqxVq0zmCA/cds2PnGbvR/CYJRiI1tdHqaNWN77ufvwpQcm94NZWhnHf7MQ4lohxG8LIX4oQ/+VTUp5P4Dfg6rp/yCAc1LKD7ufE0K8TAhxvRDi+u3t5dvgYlaIf9m5ykRKXHJiHd/91JO44rILAEyr6iGOX+bUJYtD/MjakVe//Kf3fRHv/dz9AIDX/N0teOd1X5t4vJvuO4vXf+Q2XHfX4mohGscf5By8SVib/njAbMDLG66+A2+4+g7rtUFsEP9h2q/61X97C/7qn+5ZaBsmOn4p5T+DKsr2Raha/DcJIW6sekIhxAUAfhRKJfQEAJtCiFwtICnlm6WUV0gpr9ja2qp6uoXZuLK209oqlWxIUon1Toi/+LnvwDdfqjDENCsiS5ud2kHdhXH8BQlNqVSrEEKbcSLRH0xG8QcZN73IQD7dy24YFNbqmaZ9FtUzg+vyVXkdxImOIx0WilRKib1hvHAAN9HxCyEuBfBdAL4bwLMA3ALgr2qc80UA7pJSbkspR1CbunxnjeMdShu3kcXUx1oRHT856CBQO4iFQv2uQvXw7y1c1VPgCOPUpqXiNMXeMJ54PB3AXqTjTxjiL9iIZZp+zbvtLIK7SSpz92sQp1jrBIgCcWgo0kGcIpWLV+eVKdlwD5Ru/79IKX++gXPeA+C5QogNAPsArgRwfQPHPVQ2s+DuEiN+ug9B5vDDbAKoQvUAKilnrRM2Vv66qhVtxMKLkkkpMUok+iV4e1+S2ryNEHonEhaIkVJWU/VYK+DmnXAqZa49g1GKXhQiCsXCETbZ3kBN/It2/GU4/mcB+K8A/qUQ4tNCiP8qhPi5qieUUl4H4D0APgdFHwUA3lz1eIfVrI5eE4nak8jhQC5VjO4DOfzM/09H9XgqLnKufxGovyjYSc5mxGgIGvjjjMcxFmVJKiEEEAUBONaw+2L54/F7MzPE7xx3ECfodQJEQXBoMndp4l+04y9Tq+cmIcSdAO6EonteCuB7AFxV9aRSylcDeHXV7y+Dcf9cV+Znb714OJBLFaPL0Ig/+10lcxcw3DmfOKQ0E8q8rCiTleiSOEm1Ey+F+EeE+BfnrJJUIhQCQrgKNfOZaSZZe9/o5q/Lz/Gn6EUBojAvSV2UEdW36GS/MrV6rgfQA/CPAD4F4HuklJOlCY9xs5e2Dap6DglyqWKG6lH/E/KvkrnLj8fvTyIlAszX8xcFd8nZx4x/LsPx+ySr87ZESoSBQChEYbyqcnB3BteVyiLHH2aI/5A4/sGSIH4APyilXD495YKt6eBuNwowjNNDw1VWsTRH9VRA/B6qx3VGWV7Y3KyI6uF7zNKEXUbVQ5PbQjn+JHP8gSh09tNM2LPW8XuDu6ME3ShAJzw8wd3+cHk4/kAIcZUQ4gMAIIR4eh2O/7FiTdbXSSXQC9WjWmrHn409wbgY5VjKH8NXcZH7n0WsoJMCXbtW9aSpRpzTUD0L5fglUT3C4vJdWm2a45HNwgn7lDKc6jks4+awcPxlHP/bAHwISnMPALcB+MVZNWhVzEZG9Y4lM8QPLDnVQ4ifMTGBmJLq8ej4q6LQpqyohIHeeCQxNMT+KJk46A+FqieVCEOBMHA4/tT+zDTHI5uFpl4hfntsDBnVc3gc//Ig/pNSyr8GkAKAlDIGsLhc8iUxe7DUV/WQ4z8sXGUVc6keQAV6m6R6FlEorWiLTT4xcVXJ/mj88Dksqp5QCPV8Cu7vdFSP+XsW4KUwuHvIdPya419wcLeM498TQlwEQAKAEOK5AM7NtFUrYElFZOSzVCoHGQZiqRO4yDHmqZ5qwV26F/zriyiUVhTcNYjfjs30J0g6DeJf3LNOs+BuIEThimoqVc+MM3dTaXP8aSoxTIjqOTzB3cOC+MsEd38JwPsBPEUIcS2ALQA/OdNWrYA1ST9IKREEUMjlkCxZq5ir4weQOZbyxxgyxD8qCO7O24qKtOmMXRbcBYC9CTz/YeD444Lgrr1BS0U554x0/LwezzC7370oVMHdQwKYlkLVI4QIATw/+3kaAAHgK1mphdbGWJNUTyolAiHQCYOlrtVDtyEU3PHXSeDK6/gXMZ6Ka/Vkjj+1Ef+kJC6t6lmwnDMQQj0fK7jLPjOFL5319qEu4qfJsxcF3tLSi7LDgvjHUj1SygTAj0opYynlLVlp5tbpl7AmEb9F9RwSrrKKGarHvDY91ZOimymcfDr+RXD8RRuxWDp+5sQnKXs01bNAHjhJJaIwo3oKeP2qCVyzytzl56DJs9cJ0DlEmbt7S6TquVYI8QYhxHcLIZ5NPzNv2YzsYJQ0wgPHSWrRDq4lDSN+IZAtWasfq0599ybuG9/cg8zlkCfZIE6w0VNCfVKHpKlEJ5y+4FuaSl0Js47prRdzOn6TiMXrwU9K4pqk6klSacU6fCZlvWuj4G4YOAlcFQFNkwmN3uM7qh66h7pWTwOTTRN9heI7yxDc/U4AzwDwGwBel/383iwbNUv73tdeg7/8bP1a2K/90Ffw0rdeV/g+DZBOKBpA/GoQRkF1qufm+8/hma/5EO4+tTf1d6WUeP5rr65937yqnmkR/yjFZlZql1M9URBY5yhjf3vT/Xjeb320NhosqsdPz2qUpDbin5DENak6559+/E78yB9dO/YYH/vyI3jO//sR7BxUW6Dz4G6hqmeqSdb8PQv0nUq7VhNNjF0K7tacbG598Dye8eoP4Wunpx8/3AjxL3q3tjK1el4wj4bMw9JU4qHzB7j/7H7tY913dn/scfhGFnX7eZoqZByFovJOQg+dO0CcSty5vYvLT25O9d0klXj4/AAPnqt338hpWKoeITDNJQ3iVO/Za4K7QBQKYISpjnX/o/t4tD/C/ihBJyyDgfymqR7n3Dpz10GjkxH/eDrggbP7eGDCs3jg7D72hgnO7Y/05vbTGAV3g0AUFmmrksClpJWzQfz0OwoFhrH6vxsqOWddhdQDZ/eRpBIPnTvAZRdNN364Ece/aJFG9d6+hEadbzSGoilrcZJq5cC4c/l2MJrWDNVTHfFTR9veGUz9XXKwdTsr3S43uDttrZ7Nnr1lo0L809f9oeuq2x801VPA8buVIydRbpNq9SSpnNinePJYFSNBQSiKy49MS6sBKtg6kyJt0u6jtKroRqKRyUb3lZrHWabM3ZWxxOkUdSxO5NjjUN/ueHYwmtZ4cLdqh0lqOH6a4OqqTAzVY16bmuqJU2xmiJ/r+KNweqqHvl93MNPzdYOdto5/CsQ/oTonL/pWZK4DnNZiFtwtUk1NVY8/+2I3mk0Wrd6FzbnuThigE9YP7pq+Uu84/WWQc66a0c0eNrDUjFM5Fimareumy0z1mdbxS1FjIKvvbe9WQfzN6Mrp/ucSuKYJ7o5SvUk71/F3gukLvtV1jmRFwV2t6knkVBz/pMzdJM1vOpJrU/b+OAHCOEvSTM7pgA3+d5WNWHpROFOqJ9ZjXF13FDRTqydxjlvVDktZ5jJbL24IIX5NCPGW7P+nCiFePPumNW+a6mkC8bPCWz7Twd2oCcTPdPwLQPx0v+pnIGeIXziqnpKHlVJmVI8d3KW6MsB0A4ocUN3BbOrxu6+bIm2Wjr+mqidO8+UJ8m1Sv6v2daLP3DyL2lRPp3lppZRSr0RciqsbkQy6KaqnJuI/JMHdMlTPnwMYAHhe9v99AH5zZi2aoSUNPTx1DJUSXqRltoO7zVA9UVgH8as2nKqC+ONmOH76esCpnikSuOJUDfAjmuohigU6ODsNfUwTR21VTwHVw3X8/ByTEf/4FVaSphN3G0tq0lhxIhHozF3zelEy1yQziL95qocfTlMyMaN6gvpxhab6CiXvLUNw9ylSyt8FMAIAKeU+MOedLhqyRhH/BPqj6eBuIJCpExbI8dccPO6eu8B0VA85xA2PnLNTQc5JEj+a2KoanTKv489TPVEgSnD843lgOta4vtAE4tdF2gq0+1U5/qb5bR8VxTn+JnT8TfSVOEl1H14GxD8UQqzDFGl7CtQKYOmsrtLBfyz/wKIH221Ixx/U1PHXUfWYuvL1YxVA9QQucohHMlUPtYc058CU9WOapnpcjp+tMOm9Y+ud0pm74zj+ce+rz9RDqMXBXQI004GQVCP+sHGqh7fPpe8aC+4S4q8BfvojXmDw8Dv+VwP4IICvF0K8E8BHAfzqTFs1I2tS1WOkgJMR/2HQ8SdZI/aGidYSlzW6xrpIzYf4lWMp931yiJseHT9l7k6H+JumepzXOdWTnev4emdsrR6+P+84VQ8w/lrN6rbaM0uz4G4Y2MCFAE0UBNNRPdml9KLm6035ET8Br2Zq9ejj1pD+EsU3bX2qWViZBK6PCCE+B+C5UBTPK6SUp2beshlYo3LOdDwC4Bx/E1svNqXjB4BTO0M88aLygq7hBFqrrJEDsRx/UH4QDB3Hn6QmxlJJztlQzKeMjp8cxiTEz1cf43T8/Lf/M+p31WtLdHDXTrCjc0ZTqtVmqeP3lXzWVE9EgKkm1dOAjp8ovqNrncMr5/TU43kw+/1EIcQTpZSfm12zZmNm1q5/0yc5Db0kboDTlBK6RG5dVQ8AbO8e4IkXbZT+rlb11ERqND451RNOQ/WQ4+8axO/W/5nm9mgKq66OnxB4juM3feOAHP9ahAfGZHzzHcaKVT2TVVa1qZ4suJtX9ajf3SnzU7S8eRbBXQ/ijznVEwS1ixvS9+vQgpS4d2w9wtn+YmtdjoN9r8t+rwG4AsBNUIj/WwBcB+B/mW3TmjfqfHU5XYAhiyKqJztFEzr+VEp0g3qbRvPBNi3P37SOn/n9qRK4SN++1gn0pjTUpCpF2hqTc06ozgmYAl/H1zu445HdwmNZZacncPxlEH9VHT8Fd12aJNUrrOkS70xwt3kdv1XrPyUHbVbcqq0ZfRVU06Xo4G6NvkIU39FeB2d2h5WP04QVcvxSyhdkdXq+BuDZUsorpJTPAfAsAHfMq4FNWrOZu+MRgCnSVp/qSTKqp87eoUkDjr+2qoeoHnfrxdKOX51f7aOqlBraEZGqZxrH31B/KKJ6EsvxG6pnHMfPq26O0/GPe1+9V1POmarcCHdi1lRPMJ1azQR3m9fx+yp/6pINWXAXqBeYrbuCAoyG/9h6tBTB3W+UUn6R/pFS3gzg2+qcVAhxQgjxHiHEl4UQtwohnjf5W/Wt0eDuhGNZwd2GSjY0oeMPBLA9JdoYNqTj16oeUZHqoc01OoFOZqNba4K75dtD97J+cFf9dv2KD/EfW1Mcf5EGf2gh/vHxo3H9qq50OU1Jzukv0zBt1Vlq80x0/OwSzUqcqB5RSfHlWtwwx3/og7sAbhVCvBXAO6AknS8FcGvN8/4BgA9KKX9SCNEFUJ5wrmFNyjknJXRYwd2a84zMdPydoHq8IElTRIHAiY1uDcRfl+pRv/PB3XLf15trZLsqxUmaQ/xVMnfrxnz0ZutjEP/+MEEggKNrCu2p/WDD3LE41bNIHT8Fd92J2VA9AUZT7O9gHH+IJJWQUlqlO+pYEeIXQsV+qIBfnXFP361KnQFG1XO0Fy08uFsG8f8bALcAeAWAXwTwpey1SiaEOAbgewBcBQBSyqGU8mzV401jut5GE9U5J+n4dTCrOR1/GIpaS/cgENg62rMcf5yk+K0P3IpH94ZIU4nf+9BX8NC5A+u7jXP8Vubu9AlctI/qKDVUD5VsmIZ+aKr+yiQdPwAcxAmiMNB1hvqDBGf2hvjtD3zZ+p69mby0ns+48731k1/FVx7aYZ+pF4xMsuCuEO5GLOr3tBSmGQ/q4fO+dOO9Z/EXn/lapXaqNjGOX8dtJDphAJGVOlHvVX/OTRRpI8R/bL0zMfN61jbR8UspDwC8EcD/A+DXALwhe62qPRnANoA/F0J8XgjxViFErsC1EOJlQojrhRDXb29v1zidsWYzd2kSGR+Aa6RkQ6oKm3VrJKIkiUJwx9cjnN83ioI7tnfxpo9/FZ+4fRv3n93HG66+Ax/50kPWd5tC/L4Ermm2XqR2RCEls6XGEQWL1/G7t8dS9YwSdAKB9Y5y/PujBJ+8fYMTs4QAACAASURBVBt/+vE7cee2CfYSnbXZVciYns8n7zAKalfVI6XEb/6PW/H+m+5n586useJqRiN+l+NnVM90wV31u0eOn02K77nhXrz2g1+u1E51bD/ipy06owwU1AEuTcSDKM5Dk/8iUX+ZIm3fC+B2AG8A8McAbhNCfE+Nc0YAng3gT6SUzwKwB+A/uh+SUr45CyhfsbW1VeN0xmai4y9B9QD1UrRVZipqq3rCQKAXhRh4tOLD2KSTuzpzUkjMRMc/RZE2amuHVVzk1ANQTdXTVHB3nKpnf5QiYoHGOJF65Wkj/sw59CJV+jtz3D7JotmPwL4e9V69SS1JTa0efwKXmCoxUQd3O4T47T5Yp2/ZOn5z3RT3IaqnluPX1G79Y6x17MzzRVgZjv91AL5fSvkVABBCfAOAdwF4TsVz3gfgPikl7Vv4Hngc/yysKY5fSjmxWp8JghnuOahY4ohX56yzsUYUCPSiQJc+AMw9GcSpphn2HMc/0g6qGUrEdvzlJ0Wt2Q9FTtVTJXM3bqg/kOMpqtUDAAfDBFEg0MkQ7zAx1V19VA8hfl/ynFb1SJqQ85+hS6rj+EMhIIS/MNu0VA/n+AF7kiqzv8A4802KyvFniD9ogOppQPpLghCiuxYZ4C3D8XfI6QOAlPI2ANPv5Wa+/xCAe4UQT8teuhIqbjBzow5Sl9PlfbQY8avf3Qr6ct/5yPGPqwg6zhTiD9DrhI5yhDv+DPE7ckPN8TeUwMWl1NNQPTFDm1EYWDp+s+du+fZoSW7NmI9B3mMcf5wgCoXuD6Mk9cZOeCE6Vfo7P+m69f/dpCX+mcocf7ZCDEUR1VPN8ZPTG1mIP63llH2Zu8NYGscfNhDcddRCVSxJUyvYvEiqpwziv14IcRWAv8j+fymAG2qe9z8AeGem6PkqagSLpzHO09VRFXBnX7Spi4v468zuVLKBB8YI4ZY14vh7UeBVjgziRPPLOcTPqmDWMTfLFpiuSBs5Pxo8o0Sykg0VErgaov7ovrgrF1fOGQVMU574nbrm+Hsh+sPYOzm4On59HdZn6lM94Tiqp0KRNiFMLIaDCBWkr55gVcjxZ+NF02sL1vHHiYmb8LYuwso4/v8DwL8H8HKozN1PQHH9lU1KeSNUNvBcjRfTSlKpncW0xgdhEQLgG7EA9R6y1IjfoMVpNwc3HH/gKEdU+wcjQ/XsO0XcmuL4yUHmi7RVQfzK8RjEXyG425SO36Fe3NcBZBu6Cx2LGCVmz2buBOkZrHcjxOmBN7DuIn0qpcFRc1PBXSEEpIQGShzQTKugCoXwxmJ45ctekJe4TjKvjp9x/KFnspnWmqjVM8qURkvh+KWUAwCvB/B6IcSFAC7NXls6Sxxe0SOjLmV8gE1K4NIOqYZvMfX4M6cRS6A73TGSNEUUZsFdTz2YQZxqysNF/LHH+VQx7fhzqp5y3+crhihQCid9nyusrJqK+dCzda/DCu4OUxxfj7QzGsbSW/VUUz2djOOn5Dmn7/Lv8S0ezbU1FNwVJjEuFFy0UP65AVmMK7DBCz8Xtb9XYTNYW8dvrpvGS6cJVU8DICHOxiABn8Ou6rlGCHEsc/o3QskwXz/7pjVvvIPU4fktxD+mZIMQxvHX0fKnNGhYYHBa04i/E3jrwQzihKl6ZsPxU7N55q6YIrhrED/VLZKWyoSfo4yNGgjYAUzOmQvucgonQScMtMRwIsffC62du8YifmcCUG1Sv6tWpSSETgtLN44RBdNlpFMmsA60WmOo3orSXj3QM5V6vDQS3G2AFoxTtYpqwifUtTJ8wXEp5XkAPw7gz7N6PS+abbNmY3xg1gnScAdYxPEnrMgVUDO4mypKpOtBS2UtSQ3HP0xSEyBMDOInp7M3cDn+PDKtYgbxm9dcDnnSNdB3omw7PVOyoUJZZmebvqpWRPVYHH8W3PVz/MzxZ4qr9U6IlDl+m+NPrfMZVU8+uFvl2mgP2zCjegAWxyBqbcrig0maPTdPHzZVUqs9B18toVGc6vHSRHC3iZINcbYKCQ4B1VPG8UdCiMcD+CkAfz/j9szUfCijio3KUD2Zsw4qcM+uEdXDnca0plU9Gb/lygSHTM7pIv6m6vH7OH5XNTLO+PaFeR3/9Jm7jen4WexIepyQOoe6/+YZ+qWagzhFLzK1iHwB4CLEP0ryk0NVkACYUuD8GjXVE0yfuRvwFfCsEL/F8duIv1atnqaCu+HhUPWUcfy/AeBDAO6QUv6TEOLJUAldS2e8o9ZdsunjFAV3pUQQGFqjnpzT6PiB+ogfMOoRS8c/KkD8Dev4bapHlI5/JKmqvxIEeR1/leWzGcx1KSzzff6YXUfWCQS6UV7OaVFCmeOncsg+p1jM8XPEr35XobHoHoZZPX5+LprYOpGYquwAqYQiNvHx99RrFRF/gaonJ+es0X+boAVH6XIFd98N4N3s/68C+IlZNmpWZlM09aVdwPjM3ZAh/ro6fl5zpKhMxDjjHD8ADJIEQIepekpw/LURv/ptB3fLr4aIIwWyImFJXR1/Qxy/5fglwixRL0lUnIcuL0f1eKqeDuIUvU5WdjrN00FpaiqSjuP4m0L8AQvu8vfofkup4jQTjylV/zNyTlvHz489dXt9Ov7E6Pg7muNfLOInHf9hCO6O24HrV6WUvyuE+CNkG61zk1K+fKYtm4ElDSH+URmOP6eKqP6QqTonR4vTGlXnJKrHi/iLMneb4vhTonrMa9Psh0qoEcjKV6SezN2Sx5JSoomAHZDnmLOMfMSpxFoUYj/j7Wnjbzqnl+OPEwfx25OuFcB11DyWqif7s8pqhq/MNNWj4xjIrsWssMpkpMts1RqFxcHdqisva+Jlz5TGi67V00Dmbp1KrqPk8AR3xyF+Kr18/TwaMg+zg7v1Az3q72Kqh3OkdRF/GNSjeuJEtYeSWgjd+1Q9w1hlUtIgbXrPXZ7AJabU8RPSDLMS1bo655Q8bhllVllzEb95PcVaJ9COnz/DIef4LR2/onqibJtNd9L1ncsgfk9wt8K10WEsdKrjGDTRmvvdKSGL5glhqq351UnVBKvUg/htjv9wFGmLszYdhuBuoeOXUv5d9vvtgC6nLKWUO0XfOezGH3w9rm56qqd+cFdYgcFpLUklep3AcPwZuvdx/ADQHyU45kw0jal6csHd8tdAc0YnUJvSmDLB091nX3CxqvH2uxy/KsilqqFGAZNzxqm3rv5gpOr0h0EAKY3KR6N6XurAcfh2cDejPCqoeuh4YZDvv5rqCQ3VU8ZI7NDxoG9fHsI0xoeg1vHHnOOvn7nbxJ67KndoSYK7QogrhBBfBPAFADcLIW4SQlQt0LZQayq4m1gTiP/hkfbeBHfHH3PnYIQv3ncOdzyymwuYJWmW7u5B/Of6o1LXYlQ9BYifZe4CZtMIfr4klZXqBJEZqsdO4CqP0s0qJArdIm22nPNglGDnoHhD6yJl1s7BSO+WVdZ8Gnv6e43B4U4o0PEEd+Psvt7xyC5O7w3Q6wSanqDVAjm0Mnp+YDrE3x/G+OJ953DbwzuQUlorM+q/wzjFuf7I1OqZkq6gFbBOQrRWzXkaaxrzqnpSmUP8TWzE4rufp3cHpcYFlWzgiL9Kf2vCyqh6/gzAL0gpL5dSXgZVvuHPZ9us2ZhVH6QWx18e8bsJMEX2H971efzwGz6FF73+47jxXntfGirZ0HUcv5QSV77+GryjxCYWRtVjc/zkIAZxYqHDPRbg5bRYHZSiE7gsqme66pz0XSrSRuPNUGrq/9/94FfwM1d9tvBYRX3hX731Orzuw1/xfWVsu8hSxwnRREtt5pP3UE+oKT53z1m86PUfx+fvOYvj6x19PeT4x3L8Hic/jWLpVe/9In74DZ/C9//+J/CJ2085wV31mT+/9m686Pc/bvImpixFQs/OZNHmg7uN6vgTo+PnpbCrWtH9fPj8Ab7jv3wUn77zdKljREFgKf1eetVn8Xsfmq6/NWFlHP+OlPKT9I+U8lMAlpLuaUzOWWICoYSVoGRw92un+7j4WA8A8Gjf3hNX6/id4G6SSpzaHeL+R/cnt9lV9cS2Q+EJXICN+IeeZXkVM1SPeW0aHT9NXgA0B15UlvnhnQM8cr54vyCLrmMT24PnDvDIlFtT8va7TqjHED8P7A0TO3BLO2z92oufjt/+8W/Rn9sf2kjfh/hTz6RAf5bp53fzvrc3NPseMD76njN9bO8MNDig9pWWczq5KG6RNtXWBhE/o3p8k03VcySptM53Zm+IOJV4eGfy3lQjj47/4Qr9rQkr4/g/K4R4kxDie4UQzxdC/DGAa4QQzxZCPHvWDWzSLIqmTnC3BMevdfwl+bztnQGedHLT2zaX4x8m9kBxVTg+M6oem+ohCoGretQxGeL31FWpYlShUbhUzxTBXY34gwBxwmkJe8ObUZwW0nCAy/EzHf0omRoZWs7YCTSuccSfZcLSTmo8cEt96jufchEed3xNX+fBGMRPk4xPxz+NNv4U73tsJdJhdWWINuMKJffax1lKiN9TdsQXnJ7G/IjflGzQ56yRoT0qAHvTqH3i1AnuSjX515mQqlqZkkjflv1+tfP6d0LJPF/YaItmaE0pOax8gDFbL1o6/jHObX+YYHcQ4wkn1r1tS6XSvvPAIGAGj6u797Y5NTtwAT6OX6l6SHfeL3D8tXZKyu4Jt+kSuAzi74QquJsryyyNwxv3jItyOgbx+O/5jCY0Ke1ifErVY7auiBgCHcW2nHOkUbZZ0QCc488miRLF2vhrk5ydlBLbuwN8x5MvBGDft24YIJXq/LvZHg37w0Qn0QHlOX4VmPcXaaubJ2KperJS3UOm6unWEEWY49rtpdgNrRzLBH1JWccRP9+QZ55WJoHrBfNoyDwsbcrxZ8dZ6xTvgUvVCLWOf0ynPrWrlnpPOO53/FL6SzbQbzfT1tue1M3czRwKr9UzSnHBRhdn9obWMX1ZllUslXZgF5g+gctw/EVlmU2bxz1jeo8/QyklBnE6tQNKskDiME5zTqjLED85vU6kzjlkk69R0gTW7/2hi/jzkk1/dc5y9Mn5gxjDODV9LzaJZbTxDwDsHijH3x8mCITp12Vj/RTcdcELb3dTqh66Hy7HXyuul0qsdQIcjGxHPU3ZD9pHgwd3R0m9TWiqWhlVz8VCiKuEEB/I/n+6EOLnZt+05q25BC713Y1uVEz1pOWLtBHH9/gTa962EVpyC1zR71KIP6EduIp0/CkO4gQnNjq5Y9o7dlW/b0R/cQvFFEXaElvHHzO+NRTC2sZxOAnxZ59Tz9B8h79X1pJUGpWLg7o5r89pKs7xJ6m06hDx3+VUPXkdfFnd+Xau75n8gk4UMKpH9YeDUaLvtduecWaCux5VT90ibQ7HT9dM56IgdV0l30ZX4WR/EL0M4s8Hd0dJvW0nq1oZjv9tULV6npD9fxuAX5xVg2ZpTWm3aZCud8Lxqh5PAozPaPAR1eNy06l0SjY4vGI5jt9R9Xg2+t49iHHBhir0zxE/dyi1EH8B1eMWNysyjvjJ0dL9F9kkm1pUT7H81HqGsYlzqPempXoMj+zyzbwipaEeBGK2xOd7znJnBRiO31+WwX6N92nO/4+7t/m+lzLHaYDLzoAQf4wgwNRJSImEE6cyqywtlazYtxLnnvMVCxlfvUxrUiqAsU70jo/jL+FPKLgb6r4rrUl/nlbG8Z+UUv41gBQApJQxgPkLTxuwooBe1eOsd8PxOn4r5b34eET1XHLCLLfJaNAGAjk5p+b4ByU5/lDkEri4Izl/MNKO3+b4Taev00mT7J5wCx2KZuz3s40sAMOX02okENk2jszxU9t9RpOZeoaZ4x+R458e8ftqBVFtIaoVo+MTEQV3TYCdJhtzfQUcvye4O07xA4x3Sj6akXP89LzoPlemelJbzknn4PerahHARE+aSunFVyxk3TConK1P92+9G1r/A6YflQkcx5nAgvq8GYOHkOoBsCeEuAhZvR4hxHMBnJtpq2ZkTTl+CuhsdMPC6pwmRT37fwLqEgK4+Fie6qEmW4Gx2KV6Kqh6RsbpkO0cxDi6pnaJ4sccxanu9HURv7un6jSUgYX4Q9shBdnqyqh6xlMdNHg3umbVRgNx2iqOiZSaT3adbxgECJ3JqhMGVgyCl2ZwaaE8x+9B/B6euWxfJ8R/8bFeto8xR/wBnMdlqJ4S/ZobBfZF1o9dulL9XS+42w0DzZur/03jabKtYjEb70VtLnPsJJWIwoBJdRPrGPO0MqqeXwLwfgBPEUJcC2ALwE/OtFUzsiSV6GZLvqaonjPDofcziTTOCBgf3N3eHeDCja53Kcm172pjjIocf+Y0hVD1elyOXx1HFQjb6EaW4x8mKY6td3Kfn9ZSaSdvAZiqpAVX9dBxCN3R/THB3fHcq161dUKkMlNYeOivMpamRjrIaRVCeJGL+LM+qM+XmFiFzkwuoHoST3C3SNVj+voYx787QCcUOL7e0RMSKdU4LUHWHyZ2v55Gx59NFmrbTM9EVlPH343sPQwiFlCiSa2KxayvADa6n0Y2O0pULCgoeLbztDKqns8JIZ4P4GlQm61/RUpZnAt/iC2RUu9AVU/Ta2iC0flxwV2jYhn3cLd3Btg62jMo1sPVBoHQPP/QQRnTcPwA0MsUKL529aIAm90QewNbzrnWqb+ZBSXxcJumeqmt6rGD1CILOGoZo0OH5Y7FniGgrnGgV1JTUj3S3FubbwbC0KzUiL7phoSsjfOjVYaZ2NT1HTj0k63cgf6++l/qTdFV8lhg6fJ9tr0zwMkjPY3Eh7FD9fgcv2AUXcn+kKYSnY6Rsw498ZSqlAd3/AkrZd2JmuH44xzVwxF/+T5DhQ+pzx84K/d5WqmtjTNe/5YZt2XmppZa9lKz6nEAogn8D9xQPeWCu1tHe7llMGA4VJpAKPkHMDkENFh5MMs1qtUDINt3N8/xq/dCbPQM4ifJJHX6WqqeNM/xT1ObPEkluhnqouAuORDKmUhLcvwj9gwBNUFUCe5Kqerj+xKaiF7Twd2AUz22jp9nywJmAhgm9gTtHp//BoxkMJEqLrNzEE/k+LeOqqzdbkaH0DPuMI6f7GCUlBYtcOPxnS6jXZrYFU9TPZFK6qNxwakedc5qx6f+sOHj+B1F2DgbpXZw16Xx5mllOP6VMS4pa0LHv96JCh84dfQyVM+pXYW6ADXYfBmYBLz4xMCd8CSeX127+rsXhUbV4wwGRfWEOnNXr246DXD8Uo6heiZ/34f4qX0UcNRqFq18moD4O5H+3GA0/UCkj3Y11ZNvr6Z6mLpnMEot3j7W16G+694nU7qYOX4nc1ddl9QqFJ1kNGZ1u70zwBbre6OE6fijIKfC6g+TbHU1WbTALU15fIY5/iQ/kU1rNFxcjt9W9YjKctERG+9AgZyzBINAq26t2CLwdRh1/KtkJnuw+uwPcB1/sZyTOvokxC+l1IgfQK5tbiljPmj4ucfx/OQINOIv4PjpvY1uqGv10MRGTqRe5q4ngSv7twxlQAgaYIhYUz3IKA5Y7S56PkbHb1CcuSflB6Lm5kvo+Lmqh0/UlHTUCc3m5pHj+H2I383cBVRgmv4lem5ScPfkEbvv2SUb7M/vZ8Fddy/eSZZII+XlfbzR4G4UenX85pwVJ5bE7iu+2lWTjk1jMArM1otaRXaYOP5JdXiklJ+rc2IhRAi1ycv9UsoX1zlWWaOBmNbg+wCFUoRQTrJQ1SMdVU/Bw90ZxBjEqYW6eNvoa4INGkPxmGOOy951nVM3CkzmruPkelGIzW6Eh7ICZ3R9TSF+N4FrmtR/SnkHDHo2ck51r91yxMUcf563NVTPNIjfpmjcWj1hELBS0kbHzydq0vHzYGQe8efRva84W8wCxToYOSbAfXpvyECHsILBPo6f2jZ9Apd51nQeX9urmM3x84nL5firHX/EpL+AP+t40qTFS3Lk6jAdMlXP68a8J1G/Rs8roHb5OlbzOKWNyih0UW5pVmRxKtEJgkwi5n9oxGdPUj+QnE7zrFkdFzKu4wdsfrQs4qfBRR2u1wmLEX8nsDh+V8Ncp5OmspjjL63q0bVs7OAubWxvOP7xA1Lr+Duc459eXmc05HlVDwENt/5OJwysgDzRExHjpPnf/Dx8oqbHb60CklT/3+vkOWluj/aHSFJprzZjW87pTkAArFo9ZRE/T96j87jXUze428tKddP10taLAOn4awZ3PfdzknpMH4OC9zy4OzqEOn4p5QvG/NRy+kKISwH8CwBvrXOcaU0PxBqaXoCi82rj6GGSejMjDeInCsB/rFOZ49fLbadtNKa5ft3X2cYhfhqcWtUTmeBukkprs2xX1aNr2kQG8f/JNXfis3edAQC88eo78DNXXYeXv+vzhRtKfOCLD+K/ffYereXmVibBjYzTVW4iUJBpxJNsSe3K7NJU4lXv/SJ+5qrr8Jt//yVLx0+fM0vvfGPe9/n78b7P359vk1MWmh6JoddEbpXSCQMr6S7Ogruc3gmdpZG/Hn9+hRKnpmKpTx58tj/Eq977BewNYp285QZ3DTrN6/hV26bfS5rLOW3wYrf9pnvP4vUfuc367p9ccyc+89XievdUJI/qN5GDtxB/VCzouO3hHfzWP9wKKSXuPdPHa95/izcPwqfj51RPkkq85v234J7Tff3+F+47i9//yG36c1EgdF4HJectQsdfiuMXQnyzEOKnhBD/mn5qnvf/A/CryLKBC875MiHE9UKI67e3t2ueTlmSKsSvNL3Vb7bOyBxTmpb47EnB3W1n8BVx/GICx78/mgLxOxz/BqsZ34tCrHVC7cRp2cxVPW+8+g78/RceAAC88zNfw3VfPYP33/QAbnvYv03D2z99N972j3frXcm4acqgpJyTo2bAoCbKkk5TuzgbOYFTuwO867P34Lq7zuCtn7pLU0Rm+S4LA94A8I7PfA1/du1dudfpubp9gdNrJnPXqHr6bJJMsnotEXNUZTh+umcWak6kbr/m+BnSveFrj+Jdn70Xn7/nrF5t5jh+7ThFboUGUF2k8YDGNa7o4n3c3b/6Azc/hD/86O0WiPjja+7Ae264r/DYBCjCQFTi+D98y0N40ye+ivP7MT725Ufwtn+8W1OddHzA9BVfSelRIvHQ+QO87R/vxtVfeUS//8GbH8IffPR2/QwiNmke6CTKQ+j4hRCvBvBH2c8LAPwugB+pekIhxIsBPCKlvGHc56SUb5ZSXiGlvGJra6vq6SyjDlJH0wsgW5YHWifsm0TSTEUzKbjrUj05jj+n6jGDhnOWYzl+Jyu0F4Usc1dio2cYv24U6NR3ulYAurhbkkoM4sRqwzc87ggAk/7v2qndoapcmSKHIMuonvR1MGUIZSDT4OEJXJbjz45LE+wzLzkOANgdqFSUNR/V40H8oyTVqzO3TUCe6tGTbchr9VCMReTUP3GSaokqUKzq8ZVlsCSRaar72pqH4x+wiTDf90zmLgWai6ke9fdUwV3fqtXa20LqZ8D70ihJC/sWHZsAXRWOn2jN/ijWf/vyC9Y8KygeS6K4mZX1nr1Pr0WMPjOI/xBRPcx+EsCVAB6SUv4bAN8KoFfjnN8F4EeEEHcD+G8AXiiEeEeN45U2chzdmlSP4W7zm0rozzi1eopm9e2dAaJA4ESWGdsN7SUpfc2gJTZo+I5ZZTj+0K/j3+xyxB/Yq4o4TxvwcgOjJMXjszov2wU7CW3vDDCI0wm1esogfqPqIf56nyF+IZBtbsEcYXaPqG2XXbgBADi3rxz/ho5djA/uDhNVtz63H7JL9Tj1c5SOP4/4rWNkqp6xiN+T6epV+niCu/x+0HPf3vE5/oA5ftUWL+IPypUbt6/RVfXkJ7I4Nc+A96VRIgv7FrVBIX6l46fn12X3szsG8ZNT3hskehz51EYbY4O7pu1unSvA7GfAg7tV5MNNWRnHvy+lTAHEQohjAB4B8OSqJ5RSvkpKeamU8nIAPw3gY1LKl1Y93jRGOvK6Ov5Rouqvdx2e2T1XmdR2ktMRBRKFLsfvQ/zTcfyuqodTPUma6nKz6r1QrypoQwvAOBET9DVteMLxNX0trg3iBOf2RxjECaRPx6/vT2HzresIHMRP7dE6fpfqSRzHf5Haacp1/KNEWvI618HThEffIyPAajZ7hz4GoLh6nY3rBKbJiJ7gzt6lxFzppkK39nvqbxPc9SFUusbt3QFO7Q6w3gn1xG9qCEmv46c+ELANhsr6LF6nqVsg50xS8wzoeVG8ZpzjT1LoDU4sHT8L7o5L2txjlUdpHHHFXOw6fg4sWFlmGlN7nm1LaTIIg8BL9ZTdwrIpK+P4rxdCnADwFgA3APgcgOJdrA+xGW6+LsefBXfHbPBQth7/qd0BTh7t6v/V7kzjOX63ZAMwCfGrzxmahCVwpVJ3aECtBrqMwnJLG+x5HP+RtQjH1iLv4Dy1q2oZDUapzqPgNk15X87xG6pHtUdoqkda5Tho4BHVc9lFCvGf31f3iyflDBNeijrv+IH85EYIn9C6Wz+H6/h15m7koPnMuVmqngKOX28ykpUn4O8BsLaj9Dp+hqh5xjhgnCPfvYpP1BdkezXw2NU0mbs+xG/vX22oHnpe9LnTe8PC1QUFd8NQZKqeAo6/QNUzCfEbBVjWV5yYCpDFiDTVwxB/ds7dAW1ZKbLyK2a1Stc+TytTq+cXsj//VAjxQQDHpJRfaOLkUsprAFzTxLHKGCHGThjopVcVIz25dvyecq9a1TMB0W7vmsxJQHXQHRaopXHlk8LRBBAFYmy9Hi/iZwWiep1Ab17eyzh+wKBcwDgRUqMMY6lrqXfCAFtHe9rJcyNeXFE9eSRL/5aSczIdf47qCYRO4PIt00/tDHGkF+lAJiF3HrAjtEn3hcW8DWW0O8BTLz6qXydn5Fbn5JOtLsMQGsTLjapzltHxJ6SxjwKQr4gdSaQb3OUTIad6JCROHuGgIyvqFqf6eqgZvSjQsSCu4y9P9TAdfxQwR8xHYgAAIABJREFUHT93onmqh1NCj/aHuOhInmUmCtdw/HbcxZzT31bKUu8P4xywUe2yg7t8vHP1mEb8Ho6fVhV6IyEhrAD2vAO8ZYK7H6W/pZR3Sym/wF9bJiOecRzfV8biNNU6fsDP8aek6qEg2BiOnzhWIM9FmiJt2fuRLefshgE2e9HYmvyTVD1hEGgETVQPHd8t2cAHBpf9bR3teRE/vTZMUiRpitD2Z1PJAnkxNGov1TsJswQu6XL8DPGfPNLVgzdP9ZiBy79HRk4jh/id4K5bI9+q1eNsyEKvJanaerFjIf78ZwDzLHsW4jftGXkQP1+9cCqlqO+NklT3bXLWm71I33MKpPPrnWSKZjXX40X8qbSCz3Q9ZNsFAV5aTbiqntIcf4bG94aJATYexN/NArOFwV3i+AfFHD/1hSAQNuKfs5a/0PELIdaEEBcCOCmEuEAIcWH2cznMblxLZVatnoqbMgCqs0ahGMvxU22ccaqeNJU4tTu0Bp8bfzC1ejxyzlg5i81uWBLx2yUbtNZcGARNwV0AVmXHdQfx80mhEwqcPNLzDkz+2sEorU318LIT6pjE8ZuNWPwc/wG2jvaw2VPXcX5/ZO8By1Q9QD7AW5rqcWoFcR1/6AnurnVCrePnKJ//rT5j0zqUpapes/tLPrhbrOrJ9b2YVh+E+NXvjW6oKUAeuyqfucuonsCMPytYnRi6xEX8/DXXKH6gOX4mRyUbV5ZZI/5BzGpU2RQUHc+NFdjB3cQ6HpDfLImX7+ArzHln746jev4d1BaLT4Di9cnOA3jjLBs1K0uloiXqJnCNMgXGOI6fqJ5xA+Ts/ghJKjX9ACCXDUxfs3X8prN1siX4/hjHz50QYGd06r14CfF3AuYMjRP1cfycS52E+AHFpR5ds7vcNMW+SE0FQG8hua85fnWvuZwPMDTH9s4AT3vcUWxmgezzByOdzAfYOn4gj8D4yoFbnuoxbQWQJfq59fjtgKlS9aSFqp51lldhNhIPvMFdSiQCxlM9p/cULbd1ZE2/R0lOXNVDznqzaxA/1/FPk7lrqJ58EuJaJ/Cqenjbixy/0fGbevxC2JPnuDGvOf5hYsQLnn2maczz/mWCuyYw7ZNz7jE5J6DuYZ/HlOYs6RyXufsHUsonAfgVKeWT2M+3SinfMMc2NmaxRvzCS8+UPk6mwJgU3LW3XswPEFdOB9i1yoF8yQbe8YYZv77Jqmn6zMfxA8oJkDO1qJ4s+MjT9zXHP6SlsK2X3jraw+4gzk1AfLDS7k3cyNdNciA8E1adU2TtMYg/DJQ+fmRRNhnHvzvE1pGepnbO7Y+scgo+jp8b3YdTO3YcoxDxc1WPu+cuqxO/3g21BJFPCCGfHLphTtXTi8Li4G5O1cPlnHZftYUFZuMWg+7Vexu9UE+2Vr8uCVTd4K7m+FkMiSePucFdoDhPhEBWlCXw0bgQrK9xpZprWtUziHMZ67yNUZCniTXHH3M5ZzHHT7kaocN5zlvSWUbV8yYhxMuFEO/Jfv5PIURn5i2bgVHFzPocv6PjLxHc9VE92vEfsXlWHvBydfxdix9VST8b3UjzlP72ZoHG0HX8CmmqvXgN1UOU0ChJczp+kqqpScGgT7oGd3Dy/2mjbm5l1SHu5CWEmqy4jj8QyPhyGwGTpFRRPaTiUau2MlQPjxu4iL8Mx+8GdznVoxC/VKvIoBjxS6n6r96U3QruMsfP5Jz0TH1yTjK371HsxpVzcsQfBGbCnorqseSc9qS1FoUZ4mfBZydeM57qgcXxuwF0WpH5HKwP8fsqcJKSb+SRevIEQIvjzz6rg7vOSso9zrysjOP/YwDPyX7T338yy0bNyuLUZO7W23pRLYWpCFQRx2/pnX2If1elhec5fk71eDh+tnNPx6mf77M84lcOYRCnBvF3DHfu5fi7NuIfZeoPQC3dT2bX8IgzOF2qp7BI2wQHwjNhyXpRYFXnJI7fWoonqVYbnTzSQy8K2OrJXrUVBXfHOR+j4y9W9ehaPUHe8a9ljp9WkWQux0/3QG/uEgiD+BOzsxmv1dON1GTIr8Vd6fr6npJzUlxC/V7vhpoipC08gfJUj2SKLouuZJUv48TQbQejFLuDuBTHn0jo4K6v4B2dE8iPVSmlreoZ5Dl+omGiIMjV/OGUlU/VQ/eb5Jx0P11127yDu+PKMkfZzlvfLqX8VvbWx4QQN82+ac2bHdytifjL6PgDM3jGIf6TuQCbR9VDziriHL9CZryaZlF7qR2AKb8wGCWa/upFSrEQWRMa4/i9qh5G9WTI0R2c27sDrHUCHIxSHIzyjr8sZeBOXuo6QuBADVSRIT4+IQFq4HFKTQiBzW6EnUGMiNEwvB4/v2d0rfp6CoK7bgKXT9VjNlu30TxVybR1/PaqgI5JzysUttJnrROiP0wsqicM8pz0IE7086B7QtYJgyyJKtF7LBvEH+r2U7Kcut6SiN+heigIzameOFP1UPtO7Q6tthepenhwl3T8bna0Jb027FYmclB/94eJt4yCHdwNvKuBVBqFmZ25m1qvuZVa9XEOEeKnJK1ECPEUelEI8WQAkzd5PYSmM3ejehw/qR7KBHcBZIM0f5xTu0P0ogBHWa0ct23UKQOHH6WsWs3xj5Fz+lQ9gI34u5EJ8I6Tc/YZIuIc/9cd9VM92zsDXHrBhv5OUebuJMqAc+ZkPcaVU8AxSd2SDTIXS9noGeRKgddhXEz10D04uhbhzN7AWy9HO35HdsnPQfwupyEI6SqUal7nt4kXyEuyvhcEZrWRpKm+Fzy4q6XLjJoYjFJccmJd/28LC0zcpKudPLJ7xuWck6vOusazrjtspWwFdxNV74b6y/bOQE/iRwsSBOnYHPEPY5mjeoqk13zcnO2PLOEEPz4AL03MAcIuHxvOfrq7jo7fBUDzLs08zvFTy34FwNVCiGuEENcA+BiAX551w2ZhOrgb1K3Vk6pln2dzdDLS8QOKE/UhIzdzEsjrjbWck3T8jKtUXGbG8U+D+BnVY3T8Rq5n0JHh8WmVwDlQzvFfuNmFEDYi3huooleXXmAcTZ7qye7XlBy/ug7uKBWtlkgXraW58sOk7KEsSpL6DUYmqMkHIjmLJxxfRyqBM3smwGs2YrFjFXyyzSP+PMev9ngw1yaEyfj1If4oMLEgQvz0N3dUrpplEKc4sdHFkV6Eo2uR/h5gJqT+MMll7m52TXBXUT3Z9Zfg+FM2EfHzWAqkKNSrLuov2zsD3ccuObHuTRAE7OAujYuOQ/UUSa/5uOErCi6wIBqmE6pn6aOBAGD3wEwifUcWujdwEL8b3D1EiH9LCPFLAL4NwJugHP7fQZVueNYc2ta4Ef3SCQOkcjzKlFLixnvP4tN3ntYD/fzBCI/uDbWOnztI1xKWsBIKgfse7ePTd57Gp+88rWd/V0cNINc2X8kGwKClThhgs6c4fq5YOLU70J2NuOCcqmdkq3pcxD9MUj0AuqHixvlevJzqicIAF212rcFDDtd2/PZ94htPP3TuAEXmlp1Q12GclhDq2NLD8dNkdNGmuteEoLkjplIFR1jw1xxD/f34E3ZNontO93ObpPt0/LpWj0a8RTp+ezjS512nHoWBnuQA1VcKHb+jOx/ECXqRUmH5+h6gnrGbwLXRjfTkTwoqQDnFe8/0Mc5oMjQJXEYurPevzrYxtR3/gW7744+v4czeEKNstcvPaRR0AaRU11hI9SQUQ1D9jcfGePVV34bqtMr3KX4AYJcdiyhRGj97DsefC+4eIsQfAjgC4ChULEBk/0fZa0tnOribLTWHY3j+6+46gx9747V4yVs+g//r3Sqk8Wvvuxk//44bMEqVwy2qzmkSo9R5jq138A9ffAgvectn8JK3fAa/84EvA1CO8aLNrvVdt4O6qh7OVY7ijOPvRpDSFH0CgJe+9Tr8zgfVefI6fqbqSVKEgcDXMUfg6vgVtaBQZp+repxEmYs2ezjNHD85yEtObOjXijZbf8snv4of+sNPosj8HL+N+GmfBZ65OUxSnN4d4NhapNE8If6I3Y+DkUL8vFonGf39hIwi2d4d4N4zfXzv712Na257JLsHNtXDdfwnj/Sw2Q21rp6j0bVOoHX8LkrViL8b6GMaxG+Cu1Rqg9rKHW0vCq3SAINY0UKXX7SBJ2UF68joGvqDRLelGyrxwOOOr9lUT9Yf33/jA3jh667B2b4fjfN7wYO7QJ7qIX79ccfWIITKNaCx9bisAuyZvSE+cfspPP+1V+OBs/vq+IT4szbvDRJLMuueEwD+7Nq78P2//3GN0rthYK1Wfc+fQGNRvoeF+B1Z6J7m+O0JVR/nECVwPSil/I25tWQORoifkOIwTjX6c43TA/T3Q+cOcPfpPc0n0kBwJxDqrKSCePfPPw/3nlGd9Jf/+kY8mg2SQZxivWs/Aq4rX+uEGsWbwJh5f5ikONbt6GzU/jDW13Nqd4C7Tu0BsJ0Q4Ff1vPL7nqYdBE2MVDtFJ+4Ewto1SHP82ftr3dCafGggWYi/gOO/c3sPZ/aG2e5meTziTl7qOmxOfL0b4WCU6Alpo6fog91BgqNrRoFMHD/djws3uzi9N8AgTnB8fc26Z+pa1fGIG6dS2qkEHjh7YB1LB3elae9PPOdSfO/Tvk7fd5pYqUR4nEoEicwt/7WixqPqCVjcSNVbMltjamolCLJrM055MErRi0L8zk98S+4e875FbVzrhPjoLz8fJ4/08NZP3pXda6PquXN7D6NE4szeECc2urljqntS0IfjFHGmSOqGZgP6tU6ItWzCoud+4aZ6fnuDGA+d29eU2xNOKPotYOqpR3YO9ERhrs2WXt99ag/nD2Lcn00eW0d7+m/A0fFnY4QkxDvMwfN+wut/uTV/TD1+f3B33rV6xjl+Mea9pTRCBjyBCfCnJBCyfcLxNf0Q+8MEp3aHuGCjg04o9GDjQUH1f+b4s/NcesGGDlgdW+/o9wejxHJegEnuIedlEL/67aIl4vipfRdRG0aG4ogdtMyvP04lwlBgsxdpjTtfyQziRF8n76wuDUTH5fdi20v1OM4t+//0nqnp43P87uSlzmcm7UAIHeQm9LTZjTDKVjU0OdLr6nrUebaO9HBqZ4hBbD43YgORnMXjs/LTp3YH+l7sHoyse+CWUSB64HHHTYasqc/PygwgzZVrps+R408Z4g8DO5C8Rv0mq+1P93braM+iRdTzDHB8I9/vOUrmVAntt8ABADlZem7jYkyceuLnGSXZPgTZhvTkOHudMNszwqwITqx39XmINqFxpCrhmv754LkDPPOSE8612Rw/jY27T6l7c9Jx/DxuRyo+dQ9CnIrNRMqR+jiOf1Jwt07MsYqNo3qunFsr5mSkLOCqliKjpdnW0Z5etu0NYySpQjcRK3OQQ/yO4+fGNzrnaJqM859ASY5fl1MwHW/A9Otc3geYgUelkl30wameIUf8DiIl9N+xHL+5F6d2BggELKfnFmkjX0fhCTfBiGySqicITJCbViIbGW+8N0ysPQeIziHkuXVU1RlSjl99ji/1tfPZ6GCjG1qbmBD6G8fxu8ZpFCozEHueg+b4uw7H7wR3kzQ1HD+TcwaBvWIF/H3OtMvv+Mk0xx+Y6pz03MapyogNydGVidT7V0eByVjvhUFWQdbkkRxfN4ifnCrfN5pPRjsHcWH8wi298bUzalW85VT9dAUCpMzimxgBTnB3EOv7QsBRZ9k7tOihDe5KKc/MsyHzMJOsZKiOIiMEs3W0pxE/6XRTCd1ZA5E/jkH8eRpJJR2ZwI/7GRr8muOnQUyIP+KO3+j4ARNAkllNepIe5hG/OucBIf6gaPKxqR7XMdH96DA0xB339u4AF272sNExTreI6iErktkW6vjZvdnshegPYz3INrohhkmK/iC29hwg507o6+SRHh48t48klQXBXZavkNUkIsdBjt9QPXlVj2s0sXaiAGFguHt3pUPXaja6TzXiDzKqCVBOw6iRpHXuk0d6OJ1RaAA5fj+9ye+tu2cAb3cg8s+R7yHsGqe9ALt/0YRnreQ6QbZnRKLvPa1Q+sMkFzilnd14+3m5aX5O6l9UeuNr2cboW0ftz7sBXIP4AwvoxanUK7Kdg5GmuwiEuTV4dAKXcGm8w4P4V87iLPpvUz1+6w9jRIHA8fWudnAc1VAtEL6pCRlVGOTBRzKOigdxmvsMDWDqoJrq0Rpw4keVc+9kwTdqMx2Xvnt6b6BpB3e/Wrou16HzevzECfPvm3vkIH4HDZFqyQ3CcnOPWYz4faoee9m80Y2QSoW8oiyWMw7xRwzxU2yCJgXOuVo1iY4ox39KI/6RdQ9onLsSWm70WbUNX6CzTd3gLs+apWMmWd1+SlaithJt5AZ3t472IJkEdTDKBz51u9jrrg4eMBNtyIK7ZONKhuSDuyaWQKvWyFnJ9SKzNwAAvTXp3tDUg7KonkBYAKYY8cusKm6G+Mnxj0H8dlVYe7zHqREE7A5ivVlNf6gUcy5178pkN9iznac9phx/mtoFycZSPYMEG90QR3qh5rM5j8kVMoPReI6fG6FiVUxqHNVDzttB/CEhOyqmJXTncblPQDnfHOJ3NPmuc+owCos4YfV9u619R6ngUj3k+LkTcR2G6xeLJuNxOn6dXZrx82f7I6W6igTiRKI/jG2Ov2ereriT2OzmEVhu34HdPOKna/TV6nGN7q9yeOp9KfP319Xxx4nh+AMhWAKXUbXEqR3c1RnVuyaGUkT18OfkW6mYWj0iN4GPKxmSD+6aOJbJprdjN2pcmVwRQtL9QaIB2IAhfi6bBXyOX+hznt0f6TFBEwDf4OXoWuRsvZiyVa093uNE6ok5lUooACCLNeX9iyvt5RTdPO0x5fhNcDejegrQJYDMWUQaKZ7dH1qzMiXbuM4OmED1ZKhYK3+cz1ip5fDX6gFcjp+Cuzb3Cajs4FxwLbQd/ziO31X1cNvzUT2u48/2E9Y0wQSqp2gy9iFound0CHKQ5/aHOr1+lKgJmyN++hxRK5bj91E9LIhNnDlx/KTd1lQPC7i67SWzOX7zfpGqZ40lcCVpqoO7vGRDFJjERDe4C6hnQVnNRVSPvWNVvt18IxZ3Xhi3EZDpf/Z5aOexThbr4OehvkTj5IRG0rHut+SA3Uq4AHQmuXttPK+DbL0TaooPUPGEnKqH+njHkXMmhupR7TRBaB9t6er49a5ecw7uTtx6cZVMB3e1jr14earogVAjRbezRBrljqF6vIhfdRyadPJ64wyZZHDOV7IBUFRPnC37SZ64pweEg/h1Akqg2x4Fgm0A7VI9Nse/7lH1AGawE4Ll/KeU9iYztHR3/WCO6pmI+BkdkUP8qjuf2x+hG1GdGon+INZIXn3OCe4eyTt+X3C3EylN/tn+SLeHno+mejTiz1Q9bjQbjONniF9dm/1ZulbS/1MQmGgNXqqZyj8nDPEHAawaSjp46qEg+f3gbeTGKT935VZmI6DAlXMmtA+Bg/izPSEsjl9TPYleXeQQPzvGySMFjj81JTwCoVD6Zs+M825W9LAwuJuNdyklhBAW1QMAR3oqX2RvGOcSO0kSCph+0VI9czC39vzY4O4gxkY30jr7vOPniL+A6vFy/Krj0Hdyck4nG9hF/FyWNkxSdCJhEL+zBKZ2u2Uf6LxEDfnUJIEwpYonIX6fnPP8foxhkuogG92Lolo9ZIUc/xgdv8uXEtVDzqM/SnQAXH3OkXMydEjIjw9Ed98BAJaWWx1rGsRPwV3hIH67LxAFYuIHRtUTBrz2f5q9FljZsFEQ6Hr7SrVUDEh4u9y/yegZ0qY33PplqB43uJvp+EmlpM8T2XJOGrMEVvoOpZmkdv2gTij0REHGxxXRO0/ZOgJA9QfqE5vdMJedy0tKayVf9n7CqB56f7Mboj9IcklZfGLKB3dbxz8Tk1IFWgLBqJ6xcs4M8euEKDszkQZ6N5OdcZuk6hmMksI4gFHt2FwxjbO8jj8wBdR00MsOsMYetNzrhNjPNnUPPYM8yrJeB6O00GkXcfxSylzJab6JBzeX+il6Ji5dpa6BHJH6n9D62f2RRpE7BzGkhBfx0zOkOkMAvJm7vn0HXKPJUidwjVH1dAoQfy5zN3P6dM0xU/VEQaDbmGS5GJ3QDu4GgXJqR3qqwNm4fgkU6/jJrB24XBBQIrib1/HLbNIKnPwMI+ckKkgIFcvaG3DEb6ieMGAxmyN2/SvALgxHIO6bHn8MgHrm9Nw3upFeKZLR3g38HtC9HKWpRfX0okDLil36psP6Qi64e4h0/CtlPNjGa9UUmcvxU2ehTuvuX8ttHLIiJKM/0yng+CeUbDgYpUil+l9p2MOcqgdQwStX1UNtK0L8AHRVRy7/yyH+QWLVbellG4aMEqnr8nOqh18HmUsZFFE942r18H1hAeAcBXfDAOf62abqPsTPNka5MONmfYjflXPqe+TkEfDy22V0/J0wsCZd97NUIoCej0b8YRbcJTknk0Ty4C5f0ZzaHWqAUg7x+zh+ux9wR14G8fviVKbulRPczeScSrlGzzdyOH6b6gk9Kzj32kZJiu3dAXpRgCedVCUr+Djf7IVZ37eVOzqORVLw7Nyxg/iJKuoP4xzHzye3nGKrDe7OxgwKKkv1+Dn+yy5UGbh2QNOhekbjEL+ieg4KBiHnPwG29aITGHNrf2x0o0KOn/qfq4gp4vipHUT10DLZVZn0h7HlLLhMlu4XBdm6mpaxz5NX9Uyh42fBRsBk5FLJgU5kaqdbiF87ftMYXblzjI4/CoW1dwL1BcCUhc7V6vHcWyGUo+u6iN+j6rERP3H8Rv9P56Kqs/QZahOgNO3bOwcMbJTg+AviU6r96n+a9C85sT4hc1f95lQMkMk5s/wFK7jbCTSgUsq1rI/3QsXxO5QmBXd9Ki1zbUTRSK0202W62Tjf6EZZBU5Hx0/gJjR9XEqZ6fgNqOhFITZ6aiy6Jc35NergbvbdeW/E8thx/Gy5WY7qibHJuD+Sw12WFbbSS79Osapn3OAhjriQ43cQv1vStu+Ued3shYzjV4Pw4mNKepikavPpwEHLumKg8Dn+wCRwOVTPJksY63odv8kaps28TbKVn+q5+JgahEUcv5fqcXX8TLJJHD+ZpeOn4C47FgUDjY7ftMNkXQZWYtBlrMhZIJCh8MkcPx1L6fjHq3q6oeG/k8TU6iH9P92bKFtxxElqBXcB6KSziVRPSY6f+ksgBE4eUSWey5RsCJw+rBC/2j6041I9jOOntmx2I/QHcY7SNIifJrq84+fnPLU7wMkjxvHzcb7Zy3P8REfxe0B1rgBYwV3D8cdamWdKoeQRP303WXXEL4T4eiHE1UKIW4UQtwghXjGP81pUTwlVT3+QYKPHEb/irC+7SKE8HuxxSzaMpXq04x9l/0+iepySDRlXqQOr2fHWO2Euo1HVMB94ywH0Ogbx+5QntOGLSuCy6a2ja0Y+ypN++IS6vTNANwxwbF19lpCSyw2TE6ECaBNVPZb6g+Sc2YTEnHvHoQ98tXp8CT+6Vo+nLHM3VDJDChxSX6DrUhuBUHvz1BQ3oqIsVY/jbNVWfw7iT0ytnoRNMmFW78YN7gLQSWe1g7sequdktoH92JIN44K7LGZB1iU55yjRHD8AD8fPYhwTEb/Q58wh/p6N+F2O383cBRRAiX2OvxPo1TetNmm8+KgeUmzx2lDzsEUg/hjAL0spvwnAcwH8eyHE02d9Ul4vhGb/InRJ+3ByJEAIlgZ7kXYdmKTqUa+dJ8TvfEYHd3M6fvU+DRA3sLrZi3Ic/6UXbOBsf4T+MMk5oF4U6InC55y6kdl/1R3wxIPvDRO4MjxAxU62dwY4eaSrnbKLFskIBVIRu0k6fh/VQz6KB9l42WzAHpwbjpwTMM5iLQoVcrYSuFLr81tHezjai3SyDl2XCu46iN+zmtLtixwdfxHHz/bzTWWG7rPdxqgEeJSh5jhNTZ9hiP/8QYzz+/5Vpm6TFdz1cfxG1QOoZ7d1tJdx2mWCu/Z5qFYPv0aAJthAV6A1q9oIZ/rDXF0n2noxHOP4w0BtHkPB3a2jPR2o3+xGWItCCKEowa67r27Kg7umMKMpKc0Rf6hLh9D7NF58wd1uplaad3B37jp+KeWDAB7M/t4RQtwK4BIAX5rleWOmq6aEoiInM4hV4HSjZ6L9p1yqhwd33cxd0uiPQU3n9wnx+zn+8wcjvPxdn8c3Pl5tfeAuk4mm4WiI6CPj+BWKfmTnIKcu6UaBpoa82aWhwH6Wdq4Rf2g7/v4gxgXM+XGqZ3vX3mTG5ePJqFnU1smqnjy1ZHY6E9oJdaPAQtAW1aMTuJjjP2JiEWog2hy/YEHsrSM9pKm04gZBQPVzDP0SiPwKh6wbCnQcpJvX8QtN4QC2qofuQyrtImUUAFbfV58h6oOqT7qCArLJOn57og2Euhf9UYK9gs1Y3nbtXfjLz96TfT7P8VNyFF27ClIHWbkNtf8v7+N2wTlG9Qij4/cpr1RcRZV+PtMfYuuIzfEHgcB6J8R6N4Icxg7Hn1oJm+rcqe4jLtWz0Y2wN0hyjp9P8nz1Q0F5bm//x7vxvhvvBwD82oufjmc/8QLv/a1qC03gEkJcDrWb13We914G4GUA8MQnPrH2uRJHWeCjaMgIvWx0FN/XjQKczdQh3375BXjpc5+I5z5ZFUD2cfzDJNEd2DVCvucLqB4acDfddw5/d9MDeLR/0mr3Zi/Um1QAjOPvRnj4vKKjaEAQin7w3IEH8Ye6sJZbpE0dN8DuYGS1WSN+onpGCS62OH5D9ZzfH+kNu/l7riM80ovw757/ZPzYsy7BH19z55SIPx83IMevOH5G9TDHH4UBXnHlU/Gib7pYv/b9z7gY95/dx+OOraETBo6qx0gKAeBfP+8ynN0f5QYy3wB95yDW3K7PXvY9T8aTt45YSNmlV37q278e5/qjvKono3oAQ+v5MnepeeTg7ns0c/xFiJ/1g47nM0IIvPJF34DTsbaRAAAgAElEQVQXfuPXAQB+4QX/DM/6+hP4m8/dV1ir59033Icze0P8wDMeh2dectw6DxUa5GOF2kZ9bncQWxvo0Djk156kql89/fHH8ZJ//kQ87ykXwWfdMMBD5w8gpSrDvNYJ8Yorn6qv5+VXPhXP+voTePcN91kTP49z0e9hXEz1bHYdxL+WFxPw8hWdIMipev72xvtx96k9fPMlxwtXjXVsYY5fCHEEwN8A+EUp5Xn3fSnlmwG8GQCuuOKK2gSYGxx0C4pxI76SJIAb3VCXJ97oRvjNH3um/qw/c7e4HoqmegqW3TT4aXchquNNzz7KpIf0Pk0UGz0TrKUVB6Hoh88d5Dn+KNBLZj/iN3JPcrD0OdocXkrbWXGZbH8Y6/r1/D23Ewsh8Kof/Cb9mWKO3yPndHT8ACF7U7JBv96zJ9hXft83WP9fdtEmXvMjz9DncDN3OQL+wWc+HgDwP77woH6NtO00X2zvDHJlA7j9b9/1JADA//zSw/o1N9byvz7jcQCA+x5VaJpv0UiTNd2vMFSIdxinmcrFUDLk+DXiL+ibtP9wnMrcJET2ihc9Vf/9889/CgDgAzc/VFirZ3tngCu/8WL8zk+ajV/4Psd6/2o9LkOrjXtsNzD3GZpihErHv94N8Vs//kwUWScUeDDb3pNWBbwf0PW878YHLCkmJXOqdhlwQywCxWrUTmhK1bM/SvQ4POoJ7pqii4T4bR/SHyb450+6EG/6mSsKr6eOLUTVI4ToQDn9d0op3zuPc+Ycv8dhkxEKI5Sof3sQXFHmbtFyWlM9BzaaJss5/oO85PLkkZ5+nyseqD4+Xdcl5Ph3Bl6On8zL8YdsYwwnQ5YQDGDXdOGKB1XkLsq9V0R90Hkm1eMfV6QN4LX2bY6fI/5J1gmFFWzzVc4EbEcUBFkJgOx7KsZR7PjJ+B4HRYFgreqZgPijMMAopdr++cA1TSBFfROwK4eWNcVpJ9aez4C6F6f3hoXySsXxZ1SPi/gjU/GSUz3crHr8JVBxJwz0uPG1iazryDn3homm9axNjFiuBk9i3OyqfBaiXjXi98RzqEKrm+VLMcZZ2SJUPQLAVQBulVK+fl7npQk1ZFRPkeMn9LKhI/32b27ERfIyvrzMQf7zjqondFU9qn1E25Dz5c5t62hPv09LcoX4bTknKWV8m61wislfQVIYyalTnZMXtOLOtRsaNORWxCzK3LXa1CmejP1yTrseP2AmZ9LxA2pFsFagXfcZz4oFYEkKufGBSaWKiVJ0YxzF5zKNL0LZOR1/aEom0P0Ks1LNSRbc5ewdbTI/ieoBzKrDx/EX2UY3QpLKXMLSo31VINCtjQ+ofj6MU71/tbs7XFePE0P1uKW1LR3/GEBhzhnocTNuNdZxErj6w1iv/nmBR57Yx8UeunTIvqJjabxEnuBuJwp0xjW3/iAp3Ba2CVsE4v8uAD8D4IVCiBuznx+a9Uljp2hW1xOUJSO+kgY2PXTfDKzr57OOMm6XI83x7/tVPSKTpdE8Qoif9+utoz39Puf4afP0QZbtuNYx0kN39yx+3qJ6Mprjd1U9a37Hz2Wybg18NzDos3FUj1tojh+TD3qO+Lvs3rgp/OPMDbYNYz/1wYGAUo2Y4O6pnXKOf5yqx3090dSICYYOOOLPgtKJg/i7UYATG51c9rnPdLLeFI6f0LDL82/rfavXct8hrTzJU939oOnZ7g5GbFVr7vcFG107c7cU4jfjatxqrBMZBD6MVRxCI36Pjj8Khb6nJOcEoOMRR8bIOTusuB63veH4GFFdW4Sq51NYwH6+btr4OHSpEX/2sOmhuxwjYC/9aIbmm5fkP29TPT5kpYKLahBR2V/hIH7+Wd7WfsYt0nm2jvZwbn+UU/XwialIx+9mF5Oz4Yjfl8C1N1C7YLlqB2AC4h+zCtOI30n0cY9Jk3MnEoUUwSTzqXp8zpIGpsj49DBQmbv7wwQ7g/z2f/5zsYBqEeIPHcTPlD7D2MQ+yJmSoojbVlZVFBiP+P//9q402pKqOn+77vTmHl8P0DR0003TrRjBFlGm1laRljggCMQIy2jAAYeYFkGIC5M/EhfGtaIJcSBOoJIYlGSRgEGEuByYwqTMBMLQ0E130/2mvu8OJz/O2VWnTp2a3rvDy7vnW+ut917dunVOnTq1a9e39/n2TKgevwLcdDjLa5ch22G2w4HokrZIzQ+iqj4eqGmSDdq8WzRY8guy6CJqSeBzG6oUE71pXsMihPBTpAOOP7jfa5ozEqJ6uC6EytzzPX49uOt7/Coorxn+RlPgQK2Ze97mQVc4/m7AXEmZ5F3yxR70g7vxHr/uATD04iWR/fVX2IJnfUXVb7pAljn4XH911vP4Ael16VQT72vL6mHYsnrCBt2exw8YC6rUfntZHyeU7ZCB6jFKN+pIzuoJ9hvwBdi8yNhkRbHgRfL4bYaQHQL2NqVWT5D6m4njt6T4RfpjzeoJB3d1rR6bIQxpDCV48xyzyUP1BPUgDI8/yfAXKai56+lUCT8Awmsy9HYA5fFrwd0sb3R8nLQHcll70Prxvgq/iUSDuyytAbBWj5IHnwwbfn2luK5bZCYT+PZnPnH83UI0uJvA8ftUj/L4Da5fB0+EzFSP5vHHVkKybDc5fkYgYCWPOzFdD7XPr9m2rB5GXB6/vy9z/JpYVrBf1OPfOym5Td3gxkk7h/qUkGmVlNVj8/hZj1/2N7/Hb2r1WKkedX788CaSRsgUqEtrixHnZcdp9QBhj7+g3lRsdZT5IVQpeolGUlcOzYqgAlw4syd4ANo4fulVc3BXT7rgfjL0zDXG4sHA8DeUOmcaSoYzFLuftnqejXC/Nq8AQK8OpovM8QIuIOD4By15/EFwN1hxzeA3GRvD0Cr0jOGPBnfjvcvgYgfLuIH4rB4gLDWQVNA64PhrCWJZFsOvTRrWvwG0m6IcePzTWlYRp61FPP4Ujr9oMeh6Kiy3G3ozUMfcq9YY6AY3KJoSaSp0LqkVuDSjxW3rhkxflcu1C/Ia/lLBC3Gu0w07x++Lb7HHr0Ta2ODFSTjrCBsD+3ywZfXww44DqsUCi7Q1fZliHaZKahz0QvBZEdR8jnr8fSUv9Iaot1NTwV3TYzb7afP4F/aXUK01fLn1LBw/e/JpD+RAUkJEnECukVCtN/w5omeQVXSPX1E9QTpncE56OmepQCFtqAkjq7Ad6BnD78v6FgLjFZ/Hr564WnEGAFZeUOciGVmyepoiuniLYTX8RnDX3Hcw5PE3NI9f7ps7q8dC9eirlQNp4agh3jNRU32KevyJ6ZwpWT0ehb/PK7BDVE8puMlKxkMxK0x1xlq9aaU+Ch6hrxTILhQ8Gdw1lUnT2rL9rYPPr94IatTyvuy8cAWuekOgIUTkIeIb/oRUTmCm6Zws2hf2+FkawfaGoccjigVPC+7yHNOoHuMBPlAuoE9l9fDzOWtWD5D+QPZrTjeaWrwvrMDJiRSAvH/0B9agYfj9Gs/6tdY9fi+cTOCvI3Ic/+xhFnyuJHiXU7VG6IY2g7w6fH1uneqp2YOBQHhCx4tlRSdxLNXjp3MGwmk61RPP8Sd7/Pqq17Lp8RcLoYLhDC7pyFTPQN50zoRMKzM3Xf+O3v+grKLO8c8+uBtnlAfLRd8wE0mRtl1jVRAhpOWT1BYjjl7hTK+qZmgiHr8Z3DU9/qFsHr//QI95+7Ah1uMfr8Ya2VKBpCxzQ4SkK8yVu3LfMGU3UC4GxpcduhZy/MzF1xrNIMOvEn57rdYbfkCWC+bIfhf8eS8rwZGv5WNN51SB7VqI4+c2ncc/a5iFMWxSC4yJanjxxIAR5NURT/Wk8/dxDweeRLrHqBvMhf2liLb5oHbzmVk9QJRG0G+spCpR+jnquda+Z2icQ6Xo+VSPzeNPukGTZDTiMjcqpTBnzdeoPBuP37MFd+3XaqBS0Dx+6WDsGq9i8UA5U0pkwWIM7PtR4N1rWTD+yl3m+Dm4a4zz0oxUD3ugWTxohp9YYKF64gLcpUKwWK9o5MGb/Sz7D/BgJb35pp3H408Luuscv93jl30PPP5wcJofUGMH5OKziu8kWYK7xeBNjWFmFbYDPWP4G8IM7hbi8/inGyFv1Qzy6rAVdZFUi/2iFbQc7LjXbn4g6LK/+n3seeR78kHgy8jqKYW9G9OolLWFY2aOPwBDbjns8ffFcPx8TkFwN6xhwn2PQ9Jqar0Yhvmd8AIujeO35H9nQSSPP4bjl8cu+uPCevy7MubwA4bHn+BlFz0K5ezbV+4mZPX4Hn/yWOhB8awIPH471RPXzuR0+KEF6Omc0awenerhOTmVoDAbbTMjx1/Ug7tRj58TQ+qGx++RvA6s9Ml9t5Uu9Uu3quCuPt8mDaq5HegZw+8HdzNl9RgefznJ4w9y9xm6qJMNAY+Z7PGHC33Y0/PsHL+e1aM8fnMBV2pWjx60DWv1VIoFK8fPx+X85X6DF5XnYT1l1U5yVo/tAVUpeoZkA+fxewE3nPOVuVTwLHn89o4PlAuBOqgSaXsp46pdwDAGCbx6wSPNuw/OOZTHr1YcJ6VzJs1LILwKNSt4zPW6u7VGE3sna7HjINeJsMJsWPLA7Kf55jZYKQaGn4UG25DOOV0XGt9ucvyBZINcLCgVRUlpNrECbElt19sHAgdIirSFtXqcx99CmDVbWe/b1BcBlMdvKc5t9fgtRV2mE6ge+Z3o66yOkpKpPUhJLuj9ZrAHZ94Uk9ONUFbRksFKqC6u2W/bsYEwxx94/HpwN8rxA9KT4yHVPe1M6ZwJD2NbMRluL+TxW7R68nr8Bc8I7iZQPYOVokb1BB5/lhx+/g4j3fCHvXtAl2yQ25pCznUznXPxYBkeZeP4k1b2xvWtrxSuu7ubK7DFGn7yvemi54WcCsCoBlYMxrev5CmPP/yWkSu4mzWrR/P4TQXOaT2+oOIr+j3Fzka5EJR6jdfqCVM9LJfuOP4WIBLc1QqDA8Czeyf9BRfmcukkj98v6hLJ4483Nlk8/iVDZT8NDIh6ykt9w69eGVWmzUQ1nNVT8AiLB8szzurh11cA0JfVxxl+/ZxML0keLzvV8/TuCV/TKJ7jLxjpnAHHb6a6ZkXJoHpq9QSOP+Txy37moXqypHPK/byQdx/V6gnLOJjdlfOgkkr1mOJ2WTFYLvqe6jN7JnHjA1K5NInj52tbsoq0RTl+bmewXPSNrE/1ZHhJ4fNi7aK0/djwl40xYQeFbUdJVUrT++w7INr2YpzHXzCCu7Xow6bV6BnDHwnuGkHZc6++A1fc9DAAqY+jD/pBC/tBFIie6TBX7vKrdqLHb0lZ07FipA/rlg2F4gxmStz65UNYZqTKDVaKGDtQj2QVrVs2FDFEaVk9wat3ISTvO9Inl7szD2p6h8FrbdhzHB2uoOhRokHk4C6/hZ151a/x1Z8/DkCVF7Q8NA5a0Bc65vIR2c6ykQpG+kuoFD1fpTQril44j7+WIFO8atGA375HhH1TkmrLksMv25LnpBd6iduPvVtZpD0874peQClMVOvWsVq/bAgrRqK6OTpWjPRhWco+NvDcA4ALr70Hf/lvsq7SYRpdqWPJYNnPV184UEZfycNwXxErlJQ3UTB/9LFftagfBy/qj1I9GTz+FSN9OHTJQOobTWD4hRJoC9+n7KA0NP2vFSMVrFgQzDOfclQc/uhwJZSssWy4gr6Sh5H+UkSrZ7IqK+alvZ3NBl0txNJJmHU/9aDskBB4Zs+kb9hfGq/i2DWL/e+uWzaEuy97izU9L+D4w5LIyRx/MtXzV+96JRpNgZ8prXbbnP7A8Wtw1mvDBWqWDJaxe6IaySr61nmvzU31sGHX93vPMatwyqYVKBcDATQbxw9EvewVC/pwx6VvxqKBEuJgPkR3jlXxjJISbihVShNffu+rQ/+vXNDvt0NE+M0lW7EwoU0bInn8jWaI+tLxmVM2hORAdir1x7wef1r6ZMEjvLhfrg9YMlT20zW5nwWPsEQF/HfG1AL45nmbUw3kdu188mDJUNmnd557eQrbjlqBi045EocttRv+z719I87cfAhKBQ9HLB8CEeGXF70pJADIjoBu+K/50+NQKhB+9fhuAEEmURaq5yNbDscHTjgsdT+O59QaUl7cXEgl41jToZW720/ZEFp9q6cVA8B/fvrkkDP5lo3L8euLt2JBf0mlc4azegbKhVzCgnnRM4a/EQnuBvn3+6ZqqDXkK/p03R6UisvJZkPH+dS+4U+iekpRo6qD8355wZiNHikVPCzoD39/dDgoqq1nDNlXHOtaPfEcv/lmsEAZ0ViqpxTPq6fltZsVvIBA78VUnGTYFtXp7SzKkEtvIpLHn0D16PVWPSK/MlpWjp/PKYnf589fVEVElg5V/MContXDbe7cXw0VwWFk4Yz7UhZ4xWF0qIKnd0+i3mhi98Q01i0bjjX6gLzWr1QVuRgLjAd0pVjAGOoh54JXAUeyerKs3C16meIXpmSDSblUStF0zkqxAH14g7Ri2a8F/eFz8zzy52bRCO5OWh42rUbvUD1mcNcoDA5IT3/3RHadFUB7c6ix4W+Etid9J41v5YufxIvrGB3uU4Y/Obhs9i8pjz9tdXGU41cpdzMITOn0G0v66oY/y+t8KxAVaROZJAxCsho5Pf60cyt4hDEV9BsdrmjB3YDq4DanG82OjRVjdLiCXeNV7JmQxdCznn8STG3+0Gfq/s3j8WdFOI+/EZnLweKxwOM3YXr8SbDl8bdTpwfoIcMfCe5qVA8bl90T03jBKM2WBuYi2dPnB8BsqB4GX/ysiyhHhyrYsU/WFM1j+G33jJleF/d5JI+/OLNMGv271VpwTfh3XWm6dAIlLwjuCiELjGS5gfX7P28ef9rx9XNfMlSOpHMWC+H4STcM/97JaTyf8/5JAt9DtrExs3paWZeW25tuCExW65G5zMq+PEdsNF2/Jh2Spb1QHr+RVdgO9IzhjwZ3A1qBvUshgEdfHAMQrHTMAl3iORPVk5LVw8jr8S8dLmdqHwjn5sdpqcj97H1kHrRk5LfzOc2kepAuf8EGf2Ja1u/tuMev5gvfkHEcvw6+TkWPsLA/W1zB8whE9swqHZyeuXCghEoxWC2sp3guGiiHFpN1EkuHKhACeOQFWT67NR5/eP1I+DOD6mnh3GBnplZvRgoKcdshrZ4kjz/Dm6KkFrU8fq3Gb7vQM4Y/NrirUT0A8Pvn1cTN4bHoaYiZqB6f4082jgMJHL8Nep/TFuqk5dWzYY97gPjcdKTAC+sazY7qYYVLAHhpbDo2j78d4ELgQBA8zeK5Md0gg6/Z+6qrUybtAwSxg2ghFs9P3dX37xTY0PP9k0WgLg2+1IGN6mGPv9YGqkcL7to5/kJYltnStp/Vk6FfXCuZManV+G0XesbwR4K7WgbJLs3IPLRDevx5PBbW7uDjyW1JHn82qkev8JQFep/Tji2FvpLK/WWjeuKCu7Pj+Juhh/Gu8QMd9vgDqqdWD1ZnpoG7l9fb1SULkvYBojLbenDX9nmnwOfM90/W4HYSfI7fRvVE8vjbxPFXG5GFmwHVI2MptjdmfSFhGiIev1bjt13oIcNvrtwN0wq8/aEd+zHcV8yV3aBLDWTj+LNRPbk9/pDhT+4/EYUoAxNpHD+/EZhSBrPh+MsGx8992zVWVVo9nZmunMfP/D6QTabYNM552kvN6uFjGzLb+gIu/fOOG/4hNvz7U0sbZkWlFG88eZ75HH8LpwZz9tMNganpKO1SKXpoCikQF+c4seHORPWoFddN5WxMOY+/dfArcFGw0hVQGSRjVRw+KlPPxqr13DeujepJKl0XaHen8PBFT63SzNaPENWTYcKVi16sgmRg2HN6/JzVMyOqhx/GMqtn3egQAGDX+HRHPX428vWmyEf1UNj4ZgXr7KTtox87EtzttuFX7WatNZwFNlXL4LOwFHQrYxpM9VTrDUzWokaY74mJaj1BvC9YyJjanjoGv2U6jr+F4EH1fI8/7F2uXjzo5wjnCewCCGf1ZFrAFZ+mpoOIMFCO98pNLB4s+7RQGsfP/Uj3+POlc/I55dXA5/4AwVvYhhXD8Eh5/M14TfxWgwOp9cbMDH9emkPX3ondp2Dn+PV0Tv3zTgd3+0oFX2KkFRk9AKwrd83P2hHc5fbGDtQhRJS25HtivFqPvW66dEga+KFdb3KB9yi91Gr0jOHn4K4pBlWtN/HS+DRGhyu+1HFej0UvIBIEd2fP8QMySJp1BV+x4GHJIMs1p0+cSsmLfVXNms4Zp9UzE4+lT4u7vDRexYoFfVg8WMausWpXPP5asxkY/gzXyvTKs0LX2YnfxwsdO5rVE/6808Fdve1We/w2B4nHbKoNwV0eu5cnuZJclOMH5NtG3Djn4fj5WtYaklqsN4Xz+FuF6Mpd5THUGtgzUcXoUDmYuLmpnsDjn/aDuxk4/gxeuRQBy94Xv6h2Jo8/A8cfl87J4nBxefwz8vjld/aMV3Gg1sTSoTKWDlWUx9/ZrB4AaDQEpuv50znzGj7p8WfL6jENP8cgWM7Cp4K6YPiXttzwJxvPSlHT9G/hGw6RrOWwb4prRxsefymgeuLiTrpWTxp4n0ZTBBW/HMffGkSCu+ri7Xh5Ck210nCmHkulWPANfjWL4bcUm4jDQKWQ67U9a1Ft3ifOmJazUj1mcJflJmaw9J/7/OzeKQDBNXlpvLMePxvhkMefJ6snp+NQKGTx+O1ZO34VK+Pzbnr8/OY8WyRx/ICca+2gerhN3+O3iLQBMvsmlurx1TmzpHMqqiem4lc70BXDT0RvI6JHiOhxIrq4E236Hj+FPVXdyLC3nNvjt2b1tIbqGSgX8xn+jJWWuP1Yj78YfjOKfp5M9cxES5z77F+ToT6Mhjz+zkxXP7ibk+P3efbcHn+6FLKfxz+saigbNXcDmsleY7kT4LnXMo+/FJ/OCSiPv6b0+Fsc0ygVPd/w91uyegBZeCa+TkO0rkBsWx47GiLQ/59vHD8RFQB8DcCpADYBOIeINrW7Xfb4+RWYC4M/+7JUfxwdrsx44upUTyu1egD5ypdnTufz+AuxxjRrcDdK9QTl8fKCb/TQNVEaMPUO6s/owd3pHIafZpHVkyWP36NASz7w+MNUx+iQFGfrdHAXaAfHn071tM/j97BvKo7jD4K7ce3moXr4GPVG06/41W6Rtm6ocx4L4HEhxJMAQEQ/BPBOAL9vdUN/e8tjuOG+5wFIAbaINHHRw8PagpPgVTU/1bNj3wG85cu3YffEdOoSfN+TyeLxaxWesoDPIcuxKyUvlgv2DXtcHr8vy9w6j58fIsE1kXGX6XoTz+6dwtGrF+U+5kzAHv/7r/6tVl4vSx6/HK/hnOcuV+6m5/EvHgwkGfj3WLUOosChGekvKr3+7nn8rVi8BSSv3JWfF/D0HukktDKPH5Bzccc++eZprkng+3f3eNVPpjAxmMPwM9Vz7tV3+POtFesgEtts69HtOBjAM9r/zwJ4nbkTEZ0P4HwAWL16tflxJowOV7B+ucwFX798CEeuGAl9fuGb1uOB517GsuE+HLJoAIObirhg9wQ2rhzO1c7pxxyM/VM1CAisB7Bh+UhiJs5J60fxkS2HY92yodRjv/+4Q33qIwu2HbUS+6ZqVlleEx84fo0vf2xipK+I7W89AtuOWmH9fOvG5dg1Po3lI+Gb/HVrF+P8k9biVasWWL+XBM8jfHLrejy2cwyHLB7A4sEy3rppBR54bh9qjSbOeM2q3MecCY5buwTvPvpg/+3t+HVLsOmgkZRvyXoFG5YP59ZR/+gb16Vq+5x97Gq8Yd1S//+hShEXnLQWz+ydxBHLg/lKRLjstI34g1ULc/WhFdi6cRkuOHktNq5MH6ss2HbUSgCBFLOJD564Br94ZCeGKsWWtcm44OS1+M2Tu7FwoIz1y8L24BUHjeC9m1dhvFrHlg3LrN8f6S/iM6dswKmvtN8/Oo5buwSnH30wDmjz7aiD898/eUC2mrNtbZDoTACnCCE+pP5/P4BjhRAfj/vO5s2bxV133dWpLjo4ODjMCxDR3UKIzeb2bgR3nwVwiPb/KgDPd6EfDg4ODj2Jbhj+OwGsJ6I1RFQGcDaAG7rQDwcHB4eeRMc5fiFEnYguBHATgAKAq4UQv+t0PxwcHBx6FV2puSuEuBHAjd1o28HBwaHX0TMrdx0cHBwcJJzhd3BwcOgxOMPv4ODg0GNwht/BwcGhx9DxBVwzARHtAvD0DL++FMBLLexOqzBX+wXM3b65fuXDXO0XMHf7Nt/6dagQYtTc+P/C8M8GRHSXbeVatzFX+wXM3b65fuXDXO0XMHf71iv9clSPg4ODQ4/BGX4HBweHHkMvGP6vd7sDMZir/QLmbt9cv/JhrvYLmLt964l+zXuO38HBwcEhjF7w+B0cHBwcNDjD7+Dg4NBjmNeGvxtF3WP6cQgR3UpEDxHR74jok2r75UT0HBHdq362daFvTxHRA6r9u9S2xUT0MyJ6TP3uTM3DoE8btDG5l4j2E9GnujVeRHQ1Ee0koge1bbFjRESXqDn3CBGd0uF+fYmIHiai+4noeiJaqLYfRkRT2thd1eF+xV67Lo/Xj7Q+PUVE96rtnRyvOPvQvjkmhJiXP5CSz08AWAugDOA+AJu61JeVAI5Rfw8DeBSy0PzlALZ3eZyeArDU2PbXAC5Wf18M4IouX8cXABzarfECcBKAYwA8mDZG6rreB6ACYI2ag4UO9uutAIrq7yu0fh2m79eF8bJeu26Pl/H5lQA+34XxirMPbZtj89nj94u6CyGmAXBR945DCLFDCHGP+nsMwEOQtYfnKt4J4Dvq7+8AeFcX+7IVwBNCiJmu3J41hBC3A9hjbI4bo3cC+KEQoiqE+B8Aj0POxY70SwhxsxCirv79DWSFu44iZrzi0NXxYpAslPxeAD9oR9tJSLAPbZtj89nw24q6d93YEtFhAI4G8Fu16UL1Wn51pykVBQHgZiK6W6toKNoAAAd2SURBVBW4B4DlQogdgJyUAOwVpTuDsxG+Gbs9Xoy4MZpL8+5PAPy79v8aIvpvIrqNiE7sQn9s126ujNeJAF4UQjymbev4eBn2oW1zbD4bfrJs62ruKhENAfgxgE8JIfYD+HsAhwN4NYAdkK+ancbxQohjAJwK4GNEdFIX+mAFydKc7wDwT2rTXBivNMyJeUdElwKoA7hGbdoBYLUQ4mgAnwZwLRGNdLBLcdduTowXgHMQdjA6Pl4W+xC7q2VbrjGbz4Z/ThV1J6IS5EW9RgjxLwAghHhRCNEQQjQBfANtesVNghDiefV7J4DrVR9eJKKVqt8rAezsdL8UTgVwjxDiRdXHro+Xhrgx6vq8I6LzAJwG4H1CkcKKFtit/r4bkhc+olN9Srh2c2G8igBOB/Aj3tbp8bLZB7Rxjs1nwz9niror/vBbAB4SQnxZ275S2+3dAB40v9vmfg0S0TD/DRkYfBBynM5Tu50H4Ked7JeGkBfW7fEyEDdGNwA4m4gqRLQGwHoAd3SqU0T0NgCfBfAOIcSktn2UiArq77WqX092sF9x166r46XwZgAPCyGe5Q2dHK84+4B2zrFORK279QNgG2SE/AkAl3axHydAvordD+Be9bMNwPcAPKC23wBgZYf7tRYyO+A+AL/jMQKwBMAtAB5Tvxd3YcwGAOwGsEDb1pXxgnz47ABQg/S2Ppg0RgAuVXPuEQCndrhfj0PyvzzPrlL7vkdd4/sA3APgDzvcr9hr183xUtu/DeDDxr6dHK84+9C2OeYkGxwcHBx6DPOZ6nFwcHBwsMAZfgcHB4cegzP8Dg4ODj0GZ/gdHBwcegzO8Ds4ODj0GJzhd5gxiEgQ0ZXa/9uJ6PIWHfvbRHRGK46V0s6ZShXx1hYc63PG/7/K+f0PE9G5s+1HxrY+l76Xw3yFM/wOs0EVwOlEtLTbHdHBC28y4oMAPiqEeGMLmg4ZUyHEG/J8WQhxlRDiu7PpgFqFmgXO8PcwnOF3mA3qkLVA/8z8wPTYiWhc/d6iRK+uI6JHieiLRPQ+IrqDZF2Aw7XDvJmI/kvtd5r6foGk5vydSvDrAu24txLRtZALhcz+nKOO/yARXaG2fR5y8cxVRPQlY39S7TyovneW1s7tJLXuf09EVxGRR0RfBNBPUrv9mpmcM0nN+u1EdBCF6xE0iOhQtZr0x+rc7ySi47XvfZ2IbgbwXeM8Vqr+3qvO5cSYvv6x6s+9RPQP2qrVcSK6kojuIaJbiGhUbf+EOv/7ieiHGeaKw1xCu1ajuZ/5/wNgHMAIpKb/AgDbAVyuPvs2gDP0fdXvLQBehtQgrwB4DsAX1GefBPAV7fv/AemcrIdcadkH4HwAl6l9KgDugtQk3wJgAsAaSz8PAvC/AEYBFAH8HMC71Ge/ALDZ8p33APgZZD2A5er7K1U7ByBXPRfUPmfo5ziLc74chmY9gI8BuE79fS2AE9TfqyGX+PP37gbQbzmPP0ewIrsAYNjsK4CNAP4VQEn9/3cAzlV/C0jNHwD4PICvqr+fB1BRfy/s9lx0P/l+sr4WOjhYIYTYT0TfBfAJAFMZv3anUHKzRPQEgJvV9gcA6JTLdUKKej1GRE8COBJST+hV2tvEAsgHwzSAO4TUJzfxWgC/EELsUm1eA1mU4ycJfTwBwA+EEA1Isazb1HH2q3aeVMf6gdr3n1t0zj6UR/8hSMlgQGrKbJLSLgCAEVJaSwBuEELYxv9OAFeTFAH7iRDiXss+WwG8BsCd6tj9CATBmgjEy74PgAXE7gdwDRH9BMnj6DAH4Qy/QyvwFUg9k3/UttWhqESS1qSsfVbV/m5q/zcRnpOmnoiAlKT9uBDiJv0DItoC6fHbYJOxTUPSd2z9SkPWc5aNS1Gzb0GKrY2rzR6A15sGXhlr67kLIW4nKbX9dgDfI6IviWgcgQB8RwhxSYbz4HN9O+TD8x0A/oKIXiGCAjAOcxyO43eYNYQQewBcBxkoZTwF6UUCsmJQaQaHPlPx54dDUiuPALgJwEeUBwsiOoKksmgSfgvgZCJaqrjrcwDclvKd2wGcpWIKo5BGjhUQjyWp+uoBOAvAL9X2GvdrNlDHuA7AZ4UQj2of3QzgQm2/V2c41qEAdgohvgH5IDnG0tdbAJxBRMvUdxar7wHSRvDb1R8B+KU670OEELcCuAjAQgBD+c/UoVtwHr9Dq3AlNKMEqbn+UyK6A9KwxHnjSXgE0kAvh1RPPEBE34Ssh3qPepPYhZTSkEKIHUR0CYBbIb3bG4UQaVLT1wN4PaQ6owBwkRDiBSI6EsCvAXwRwFGQD4jr1Xe+DuB+IrpHCPG+3Gcb4A2QtNIXiOgLats2SDrta0R0P+S9ezuAD6ccawuAzxBRDTImw+miob4S0WWQldg8SPXKjwF4GvK6vYKI7gawD/JBVwDwfSJaADmefyOEeHkW5+vQYTh1TgeHHFCU0nYhxGnd7ksnQETjQgjnzc8zOKrHwcHBocfgPH4HBweHHoPz+B0cHBx6DM7wOzg4OPQYnOF3cHBw6DE4w+/g4ODQY3CG38HBwaHH8H9hFDJJIoBjawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(n_updates), total_rewards)\n",
    "plt.xlabel(\"Number of optimizer steps\")\n",
    "plt.ylabel(\"Total episode reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(n_updates), losses)\n",
    "plt.xlabel(\"Number of optimizer steps\")\n",
    "plt.ylabel(\"Value net MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(n_updates-10), losses[10:])\n",
    "plt.xlabel(\"Number of optimizer steps\")\n",
    "plt.ylabel(\"Value net MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the value network without the MCTS\n",
    "\n",
    "Idea:\n",
    "$$\n",
    "\\pi(s) =  \\arg \\underset{a}{\\max} r(s,a) + \\gamma V(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi:\n",
    "    def __init__(self,\n",
    "                 simulator,\n",
    "                 value_net,\n",
    "                 discount\n",
    "                ):\n",
    "        self.simulator = simulator\n",
    "        self.value_net = value_net\n",
    "        self.discount = discount\n",
    "        \n",
    "        \n",
    "    def get_action(self, valid_actions, simulator_state):\n",
    "        Q_values = []\n",
    "        for a in valid_actions:\n",
    "            self.simulator.load_state_dict(copy.deepcopy(simulator_state))\n",
    "            frame, valid_actions, reward, done = self.simulator.step(a)\n",
    "            with torch.no_grad():\n",
    "                value = self.value_net(frame).item()\n",
    "            Q = reward + self.discount*value\n",
    "            Q_values.append(Q)\n",
    "        print(\"Q values: \", Q_values)\n",
    "        best_id = np.argmax(Q_values)\n",
    "        print(\"Best id: \", best_id)\n",
    "        best_action = valid_actions[best_id]\n",
    "        print(\"best action: \", best_action)\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_value_policy(value_net, env, episode_length, discount, render = False):\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    \n",
    "    policy = Pi(env, value_net, discount)\n",
    "    frame, valid_actions = env.reset()\n",
    "    print(\"valid actions: \", valid_actions)\n",
    "    simulator_state = env.save_state_dict()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for i in range(episode_length):\n",
    "        a = policy.get_action(valid_actions, simulator_state)\n",
    "        print(\"Action selected from value policy: \", a, \"({})\".format(action_dict[a]))\n",
    "        # reset internal state of the env\n",
    "        env.load_state_dict(simulator_state)\n",
    "        #if render:\n",
    "        #    env.render()\n",
    "        frame, valid_actions, reward, done = env.step(a)\n",
    "        print(\"valid actions: \", valid_actions)\n",
    "        simulator_state = env.save_state_dict()\n",
    "        if render:\n",
    "            env.render()\n",
    "        print(\"Reward received: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid actions:  [0 1 2 3]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y  @█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Q values:  [0.3119818680882454, 0.31267251509428023, 0.3120264671742916, 0.3139860042333603]\n",
      "Best id:  3\n",
      "best action:  3\n",
      "Action selected from value policy:  3 (Left)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.3139860042333603, 0.3099972830414772, 0.31338185152411463, 0.31187977451086046, 0.3119818680882454]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█n!? █\n",
      "█    █\n",
      "█y @ █\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_value_policy(value_net, game_simulator, episode_length, discount, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
