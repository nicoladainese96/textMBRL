{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert(1, '../RTFM')\n",
    "# go to RTFM main folder and run 'pip install -e .', then rtfm, model and core should be available as packages\n",
    "from rtfm import featurizer as X\n",
    "from rtfm import tasks # needed to make rtfm visible as Gym env\n",
    "from core import environment # env wrapper\n",
    "\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scripts from muzero-general folder (not installed as a ackage -> not into the python path as a default)\n",
    "import sys\n",
    "sys.path.insert(1, '../muzero-general')\n",
    "import muzero, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test muzero components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical model\n",
    "\n",
    "We need to:\n",
    "- init the model from the game configurations\n",
    "- load the weights of the trained model from a checkpoint inside the muzero folder \n",
    "- init a new game to use as a test simulator\n",
    "- write an accuracy test suite for the dynamics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weights', 'optimizer_state', 'total_reward', 'muzero_reward', 'opponent_reward', 'episode_length', 'mean_value', 'training_step', 'lr', 'total_loss', 'value_loss', 'reward_loss', 'policy_loss', 'num_played_games', 'num_played_steps', 'num_reanalysed_games', 'terminate'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = \"2021-03-11--15-51-55/model.checkpoint\"\n",
    "rel_path = \"../muzero-general/results/rtfm_groups_simple_stationary/\"\n",
    "model_check = torch.load(rel_path+checkpoint)\n",
    "model_check.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Game instance and configurations for the current environment\n",
    "game_name=\"rtfm_groups_simple_stationary\"\n",
    "game_module = importlib.import_module(\"games.\" + game_name)\n",
    "game = game_module.Game()\n",
    "config = game_module.MuZeroConfig() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleMuZeroNLNetwork(\n",
       "  (representation_network): DataParallel(\n",
       "    (module): SimpleNLRepresentationNetwork(\n",
       "      (emb): Embedding(262, 8, padding_idx=0)\n",
       "    )\n",
       "  )\n",
       "  (dynamics_network): DataParallel(\n",
       "    (module): DynamicsNetwork_v2(\n",
       "      (conv): Conv2d(53, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1x1_reward): Conv2d(48, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=3200, out_features=128, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Linear(in_features=128, out_features=3, bias=True)\n",
       "        (3): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (prediction_network): DataParallel(\n",
       "    (module): PredictionNetwork(\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1x1_value): Conv2d(48, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv1x1_policy): Conv2d(48, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fc_value): Sequential(\n",
       "        (0): Linear(in_features=3200, out_features=128, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Linear(in_features=128, out_features=3, bias=True)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (fc_policy): Sequential(\n",
       "        (0): Linear(in_features=3200, out_features=128, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Linear(in_features=128, out_features=5, bias=True)\n",
       "        (3): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use config to init the correct model, then load model's weights and make sure it runs on cpu\n",
    "model = models.MuZeroNetwork(config)\n",
    "model.load_state_dict(model_check['weights']) # this works only if we don't change architecture params in the meanwhile\n",
    "#model = model.cpu()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "support_size = int((model.full_support_size-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.random.choice(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1step_true_dynamics(game, action, model):\n",
    "    simulator = copy.deepcopy(game) # avoid to change internal state of the game\n",
    "    observation, reward, done = simulator.step(action)\n",
    "    # render it as encoded state and support reward\n",
    "    observation = torch.tensor(observation).unsqueeze(0).to(device)\n",
    "    reward = torch.tensor([reward]).unsqueeze(0).to(device).float()\n",
    "    support_size = int((model.full_support_size-1)/2)\n",
    "    with torch.no_grad():\n",
    "        encoded_state = model.representation(observation) # represenation or representation_network?\n",
    "        reward_support = models.scalar_to_support(reward, support_size).view(1,-1)\n",
    "    return encoded_state, reward_support, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1step_learned_dynamics(observation, action, model):\n",
    "    # wrap obs and action in tensors and add batch size dimension\n",
    "    obs = torch.tensor(observation).unsqueeze(0).to(device)\n",
    "    action = torch.tensor([action]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # initial encoded state\n",
    "    with torch.no_grad():\n",
    "        encoded_state = model.representation(obs)\n",
    "\n",
    "        next_state, reward = model.dynamics(encoded_state, action)\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  4\n"
     ]
    }
   ],
   "source": [
    "# Prepare Game for tests\n",
    "observation = game.reset()\n",
    "action = random_policy()\n",
    "print(\"action: \", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DataParallel wrapper is giving me problems to load the model in the cpu and run it successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true_encoded_state, true_reward_support, done = get_1step_true_dynamics(game, action, model)\n",
    "pred_encoded_state, pred_support_reward = get_1step_learned_dynamics(observation, action, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  torch.Size([1, 48, 5, 5])\n",
      "Reward shape:  torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "assert true_encoded_state.shape == pred_encoded_state.shape, \"Mismatch in encoded states shapes\"\n",
    "assert true_reward_support.shape == pred_support_reward.shape, \"Mismatch in reward support shapes\"\n",
    "print(\"State shape: \", true_encoded_state.shape)\n",
    "print(\"Reward shape: \", true_reward_support.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2526]], device='cuda:0')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_reward = models.support_to_scalar(true_reward_support, support_size)\n",
    "true_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0058]], device='cuda:0')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_reward = models.support_to_scalar(pred_support_reward, support_size)\n",
    "pred_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1212, device='cuda:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(true_encoded_state,pred_encoded_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80.6629, device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_encoded_state.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(216.5389, device='cuda:0')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_encoded_state.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2468]], device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(true_reward-pred_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = game.reset()\n",
    "done = False\n",
    "\n",
    "action_history = []\n",
    "true_reward_history = []\n",
    "pred_reward_history = []\n",
    "reward_err_history = []\n",
    "true_state_history = []\n",
    "pred_state_history = []\n",
    "\n",
    "t = 1\n",
    "while not done or t==100:\n",
    "    action = random_policy()\n",
    "    action_history.append(action)\n",
    "    # compare dynamics\n",
    "    true_encoded_state, true_support_reward, done = get_1step_true_dynamics(game, action, model)\n",
    "    pred_encoded_state, pred_support_reward = get_1step_learned_dynamics(observation, action, model)\n",
    "    # compute scalar rewards\n",
    "    true_reward = models.support_to_scalar(true_support_reward, support_size)\n",
    "    pred_reward = models.support_to_scalar(pred_support_reward, support_size)\n",
    "    abs_err_reward = torch.abs(true_reward-pred_reward)\n",
    "    # store everything\n",
    "    true_reward_history.append(true_reward)\n",
    "    pred_reward_history.append(pred_reward)\n",
    "    reward_err_history.append(abs_err_reward)\n",
    "    true_state_history.append(true_encoded_state)\n",
    "    pred_state_history.append(pred_encoded_state)\n",
    "    # execute step\n",
    "    observation, reward, done = game.step(action)\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
