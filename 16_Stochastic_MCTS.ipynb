{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99352b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "# custom imports\n",
    "import utils\n",
    "import train\n",
    "import mcts\n",
    "from stochastic_mcts import StochasticPVMCTS\n",
    "from rtfm import featurizer as X\n",
    "import os\n",
    "from torch import multiprocessing as mp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d392645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check only if main logic of the training loop works\n",
    "ucb_C = 1.0\n",
    "discount = 0.99 \n",
    "episode_length = 32\n",
    "num_simulations = 100\n",
    "n_episodes = 100 #4000\n",
    "memory_size = 1024\n",
    "batch_size = 4 #32\n",
    "tau = 0.1 # new_trg_params = (1-tau)*old_trg_params + tau*value_net_params\n",
    "dir_noise = True\n",
    "dirichlet_alpha = 0.5 # no real reason to choose this value, except it's < 1\n",
    "exploration_fraction = 0.25\n",
    "temperature = 0.\n",
    "full_cross_entropy = False\n",
    "entropy_bonus = False\n",
    "entropy_weight = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d656f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = utils.Flags(env=\"rtfm:groups_simple-v0\")\n",
    "gym_env = utils.create_env(flags)\n",
    "featurizer = X.Render()\n",
    "game_simulator = mcts.FullTrueSimulator(gym_env, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6db93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_net = mcts.DiscreteSupportPVNet_v3(gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e755da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_rollout_pv_net_stochastic(\n",
    "    pv_net,\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    num_simulations,\n",
    "    dirichlet_alpha, \n",
    "    exploration_fraction,\n",
    "    temperature,\n",
    "    render=False,\n",
    "    debug_render=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plays a rolllout with a policy and value MCTS. \n",
    "    Starts building the tree from the sub-tree of the root's child node that has been selected at the previous step.\n",
    "    \n",
    "    If mode='simulate', it's identical to a policy MCTS with MC rollout evaluations, if mode='predict', the value network \n",
    "    is used to estimate the value of the leaf nodes (instead of a MC rollout).\n",
    "    \n",
    "    Samples the next action based on the Q-values of the root node's children and returns both the MCTS policy and the list of \n",
    "    sampled actions as possible targets with which to train the policy network.\n",
    "    \n",
    "    Formula used for MCTS policy (softmax of Q-values with temperature):\n",
    "    \n",
    "    p(a) = exp{Q(a)/T} / \\sum_b exp{Q(b)/T}\n",
    "\n",
    "    Note: the softmax function with T=0 is the argmax function.\n",
    "    \n",
    "    This function is also mixing a prior sampled from a Dirichlet distribution (with parameters dirichlet_alpha for each \n",
    "    possible action) to the prior of the root node's children, in order to increase exploration at the base of the tree \n",
    "    even in cases where the policy is almost deterministic. The mixture coefficient between the prior and the categorical \n",
    "    distribution sampled by the Dirichelt distribution is the exploration_fraction, such that:\n",
    "    \n",
    "    p(a) = (1-exploration_fraction) Prior(a) + exploration_fraction Dir(a)\n",
    "    \n",
    "    Version v2: same as v1, but it's not re-using the old sub-tree in the new mcts step. \n",
    "    This has be done if we want to use the deterministic PV-MCTS as a baseline for the stochastic environment.\n",
    "    As it is, this function it's not convinient to use in the deterministic setup (altough it can be useful to \n",
    "    study the search tree properties from scratch at every step).\n",
    "    \"\"\"\n",
    "    \n",
    "    A = len(env.env.action_space)\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    frame, valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    # variables used for training of value net\n",
    "    frame_lst = [frame]\n",
    "    reward_lst = []\n",
    "    done_lst = []\n",
    "    action_lst = []\n",
    "    probs_lst = []\n",
    "    \n",
    "    for i in range(episode_length):\n",
    "        tree = StochasticPVMCTS(\n",
    "            frame, \n",
    "            env, \n",
    "            valid_actions, \n",
    "            ucb_C, \n",
    "            discount, \n",
    "            pv_net,\n",
    "            root=new_root,\n",
    "            render=debug_render, \n",
    "            )\n",
    "        \n",
    "        root, info = tree.run(num_simulations,\n",
    "                              dir_noise=True, \n",
    "                              dirichlet_alpha=dirichlet_alpha, \n",
    "                              exploration_fraction=exploration_fraction\n",
    "                             )\n",
    "        \n",
    "        action, probs = root.softmax_Q(temperature)\n",
    "        action_lst.append(action)\n",
    "        probs_lst.append(probs)\n",
    "        \n",
    "        if render:\n",
    "            print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "\n",
    "        frame, valid_actions, reward, done = env.step(action)\n",
    "        \n",
    "        frame_lst.append(frame)\n",
    "        reward_lst.append(reward)\n",
    "        done_lst.append(done)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        total_reward += reward\n",
    "        \n",
    "        new_root = tree.get_subtree(action, frame)\n",
    "        if new_root is not None and render:\n",
    "            # amount of simulations that we are going to re-use in the next step:\n",
    "            print(\"new_root.visit_count: \", new_root.visit_count) \n",
    "        if done:\n",
    "            frame, valid_actions = env.reset()\n",
    "            if render:\n",
    "                print(\"Final reward: \", reward)\n",
    "                print(\"\\nNew episode begins.\")\n",
    "                env.render()\n",
    "            done = False\n",
    "            new_root = None\n",
    "\n",
    "\n",
    "    return total_reward, frame_lst, reward_lst, done_lst, action_lst, probs_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86dcd60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = play_rollout_pv_net_stochastic(\n",
    "    pv_net,\n",
    "    game_simulator,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    num_simulations,\n",
    "    dirichlet_alpha, \n",
    "    exploration_fraction,\n",
    "    temperature,\n",
    "    render=False,\n",
    "    debug_render=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069d6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward, frame_lst, reward_lst, done_lst, action_lst, probs_lst = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20255345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9a6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_x86",
   "language": "python",
   "name": "pytorch_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
