{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rtfm import featurizer as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "vprint = print if verbose else lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    def __init__(self, \n",
    "                 simulator,\n",
    "                 valid_actions,\n",
    "                 ucb_c,\n",
    "                 discount,\n",
    "                 max_actions,\n",
    "                 root=None,\n",
    "                 render=False):\n",
    "        \"\"\"\n",
    "        Monte Carlo Tree Search assuming deterministic dynamics.\n",
    "        \n",
    "        simulator: \n",
    "            wrapper of the environment that returns a scalar reward, a list of valid actions \n",
    "            and a 'done' boolean flag when presented with an action\n",
    "        valid_moves:\n",
    "            list of valid moves for the root node\n",
    "        ucb_c:\n",
    "            Constantused in the UCB1 formula for trees\n",
    "            UCB(s,a) = Q(s,a) + ucb_c*sqrt(log N(s,a)/(\\sum_b N(s,b)))\n",
    "        discount:\n",
    "            discoung factor gamma of the MDP\n",
    "        max_actions:\n",
    "            number of actions to be taken at most from the root node to the end of a rollout\n",
    "        root: \n",
    "            might be the child of an old root node; use it to keep all the cached computations \n",
    "            from previous searches with a different root node. \n",
    "        \"\"\"\n",
    "        self.simulator = simulator\n",
    "        self.original_dict = simulator.save_state_dict()\n",
    "        self.valid_actions = valid_actions\n",
    "        self.action_space = len(valid_actions)\n",
    "        self.ucb_c = ucb_c\n",
    "        self.discount = discount\n",
    "        self.max_actions = max_actions\n",
    "        self.root = root\n",
    "        self.render = render\n",
    "    \n",
    "    def get_subtree(self, action):\n",
    "        \"\"\"\n",
    "        Returns the subtree whose root node is the current root's child corresponding to\n",
    "        the given action.\n",
    "        \"\"\"\n",
    "        return self.root.children[action]\n",
    "    \n",
    "    def run(self, num_simulations):\n",
    "        \"\"\"\n",
    "        Runs num_simulations searches starting from the root node corresponding to the internal\n",
    "        state of the simulator given during initialization.\n",
    "        Returns the root node and an extra_info dictionary\n",
    "        \"\"\"\n",
    "        if self.root is None:\n",
    "            self.root = Node()\n",
    "            self.root.expand(\n",
    "                self.valid_actions,\n",
    "                0, # reward to get to root\n",
    "                False, # terminal node\n",
    "                self.simulator # state of the simulator at the root node \n",
    "            )\n",
    "            # not sure about this\n",
    "            self.root.visit_count += 1\n",
    "        \n",
    "        max_tree_depth = 0\n",
    "        root = self.root\n",
    "        for n in range(num_simulations):\n",
    "            ### Start of a simulation/search ###\n",
    "            vprint(\"\\nSimulation %d started.\"%(n+1))\n",
    "            node = root\n",
    "            # make sure that the simulator internal state is reset to the original one\n",
    "            self.simulator.load_state_dict(root.simulator_dict)\n",
    "            search_path = [node]\n",
    "            current_tree_depth = 0\n",
    "            if self.render:\n",
    "                node.render(self.simulator)\n",
    "            ### Selection phase until leaf node is reached ###\n",
    "            while node.expanded or (current_tree_depth<self.max_actions):\n",
    "                current_tree_depth += 1\n",
    "                action, node = self.select(node)\n",
    "                if self.render and node.expanded:\n",
    "                    node.render(self.simulator)\n",
    "                vprint(\"Current tree depth: \", current_tree_depth)\n",
    "                vprint(\"Action selected: \", action)\n",
    "                vprint(\"Child node terminal: \", node.terminal)\n",
    "                vprint(\"Child node expanded: \", node.expanded)\n",
    "                if node.expanded or node.terminal:\n",
    "                    search_path.append(node)\n",
    "                    if node.terminal:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            ### Expansion of leaf node (if not terminal)###\n",
    "            vprint(\"Expansion phase started\")\n",
    "            if not node.terminal:\n",
    "                parent = search_path[-1] # last expanded node on the search path\n",
    "                node = self.expand(node, parent, action)\n",
    "                if self.render:\n",
    "                    node.render(self.simulator)\n",
    "                search_path.append(node)\n",
    "            \n",
    "            ### Simulation phase for self.max_actions - current_tree_depth steps ###\n",
    "            vprint(\"Simulation  phase started\")\n",
    "            value = self.simulate(node, current_tree_depth)\n",
    "            vprint(\"Simulated value: \", value)\n",
    "            \n",
    "            ### Backpropagation of the leaf node value along the seach_path ###\n",
    "            vprint(\"Backpropagation phase started\")\n",
    "            self.backprop(search_path, value)\n",
    "        \n",
    "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "            vprint(\"Simulation %d done.\"%(n+1))\n",
    "        extra_info = {\n",
    "            \"max_tree_depth\": max_tree_depth\n",
    "        }\n",
    "        # just a check to see if root works as a shallow copy of self.root\n",
    "        assert root.visit_count == self.root.visit_count, \"self.root not updated during search\"\n",
    "        \n",
    "        # make sure that the simulator internal state is reset to the original one\n",
    "        self.simulator.load_state_dict(root.simulator_dict)\n",
    "        return root, extra_info\n",
    "        \n",
    "    def select(self, node):\n",
    "        \"\"\"\n",
    "        Use UCT formula on the input node; return the action selected and the corresponding child node \n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        ucb_values = []\n",
    "        for action, child in node.children.items():\n",
    "            actions.append(action)\n",
    "            ucb_values.append(self.ucb_score(node, child))\n",
    "        actions = np.array(actions)\n",
    "        vprint(\"actions: \", actions)\n",
    "        \n",
    "        ucb_values = np.array(ucb_values)\n",
    "        vprint(\"ucb_values: \", ucb_values)\n",
    "        \n",
    "        max_U = ucb_values.max()\n",
    "        vprint(\"max_U: \", max_U)\n",
    "        \n",
    "        mask = (ucb_values==max_U)\n",
    "        vprint(\"mask: \", mask)\n",
    "        \n",
    "        best_actions = actions[mask]\n",
    "        vprint(\"best_actions: \", best_actions)\n",
    "        \n",
    "        action = np.random.choice(best_actions)\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, eps=1e-3):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus.\n",
    "        \"\"\"\n",
    "        exploration_term = self.ucb_c*np.sqrt(np.log(parent.visit_count)/(child.visit_count+eps))\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_term = child.reward + self.discount*child.value() \n",
    "        else:\n",
    "            value_term = 0\n",
    "\n",
    "        return value_term + exploration_term\n",
    "    \n",
    "    def expand(self, node, parent, action):\n",
    "        \"\"\"\n",
    "        Expand the node obtained by taking the given action from the parent node \n",
    "        \"\"\"\n",
    "        simulator = parent.get_simulator(self.simulator) # get a deepcopy of the simulator with the parent's state stored\n",
    "        valid_actions, reward, done = simulator.step(action) # this also updates the simulator's internal state\n",
    "        vprint(\"reward: \", reward)\n",
    "        vprint(\"done: \", done)\n",
    "        node.expand(valid_actions, reward, done, simulator)\n",
    "        return node\n",
    "    \n",
    "    def simulate(self, node, current_depth):\n",
    "        \"\"\"\n",
    "        Simulate a rollout with a random policy starting from the input node\n",
    "        until the end of the episode or self.max_actions are reached \n",
    "        (also considering the current depth of the input node from the root)\n",
    "        \"\"\"\n",
    "        if not node.terminal:\n",
    "            simulator = node.get_simulator(self.simulator)\n",
    "            valid_actions = node.valid_actions\n",
    "            steps = self.max_actions - current_depth\n",
    "            cum_discounted_reward = 0\n",
    "            for i in range(steps):\n",
    "                action = np.random.choice(valid_actions)\n",
    "                valid_actions, reward, done = simulator.step(action)\n",
    "                cum_discounted_reward += (self.discount**i)*reward\n",
    "                if done:\n",
    "                    break\n",
    "        else:\n",
    "            cum_discounted_reward = 0\n",
    "        return cum_discounted_reward\n",
    "            \n",
    "    def backprop(self, search_path, value):\n",
    "        \"\"\"\n",
    "        Update the value sum and visit count of all nodes along the search path.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            value = node.reward + self.discount*value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.reward = 0\n",
    "        self.simulator = None\n",
    "        self.expanded = False\n",
    "        self.terminal = False\n",
    "        self.simulator_dict = None\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, valid_actions, reward, done, simulator):\n",
    "        self.expanded = True\n",
    "        vprint(\"Valid actions as child: \", valid_actions)\n",
    "        vprint(\"Terminal node: \", done)\n",
    "        self.reward = reward\n",
    "        self.terminal = done\n",
    "        self.valid_actions = valid_actions\n",
    "        if not done:\n",
    "            for action in valid_actions:\n",
    "                self.children[action] = Node()\n",
    "        self.simulator_dict = simulator.save_state_dict()\n",
    "        \n",
    "    def get_simulator(self, simulator):\n",
    "        if self.simulator_dict is not None:\n",
    "            # load a deepcoy of the simulator_dict, so that the internal variable remains unchanged\n",
    "            simulator.load_state_dict(copy.deepcopy(self.simulator_dict)) \n",
    "            return simulator\n",
    "        else:\n",
    "            print(\"Trying to load simulator_dict, but it was never instantiated.\")\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def best_action(self, discount):\n",
    "        \"\"\"\n",
    "        Look among the children and take the one with higher Q-value. \n",
    "        Exclude children with 0 visits.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        Qvalues = []\n",
    "        for action, child in self.children.items():\n",
    "            actions.append(action)\n",
    "            Qvalues.append(child.reward + discount*child.value())\n",
    "        actions = np.array(actions)\n",
    "        Qvalues = np.array(Qvalues)\n",
    "        max_Q = Qvalues.max()\n",
    "        mask = (Qvalues==max_Q)\n",
    "        best_actions = actions[mask]\n",
    "        return np.random.choice(best_actions)\n",
    "    \n",
    "    def render(self, simulator):\n",
    "        if self.simulator_dict is not None:\n",
    "            simulator.load_state_dict(self.simulator_dict)\n",
    "            simulator.render()\n",
    "        else:\n",
    "            raise Exception(\"Node simulator not initialized yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueSimulator():\n",
    "    \"\"\"\n",
    "    Returns only valid actions, reward and done signal from env.step() - no state is returned\n",
    "    \"\"\"\n",
    "    def __init__(self, env, featurizer=None):\n",
    "        self.env = env\n",
    "        self.action_space = len(gym_env.action_space)\n",
    "        self.featurizer = featurizer\n",
    "        \n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        valid_moves = frame['valid'].numpy().astype(bool) # boolean mask of shape (action_space)\n",
    "        actions = np.arange(self.action_space)\n",
    "        valid_actions = actions[valid_moves]\n",
    "        return valid_actions\n",
    "    \n",
    "    def step(self, action, *args, **kwargs):\n",
    "        frame, reward, done, _ = self.env.step(int(action), *args, **kwargs)\n",
    "        valid_moves = frame['valid'].numpy().astype(bool) # boolean mask of shape (action_space)\n",
    "        actions = np.arange(self.action_space)\n",
    "        valid_actions = actions[valid_moves]\n",
    "        return valid_actions, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        self.featurizer.featurize(self.env)\n",
    "        \n",
    "    def save_state_dict(self):\n",
    "        return self.env.save_state_dict()\n",
    "        \n",
    "    def load_state_dict(self, d):\n",
    "        self.env.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters ###\n",
    "ucb_C = 1.0\n",
    "discount = 0.997\n",
    "episode_length = 50\n",
    "max_actions = 100\n",
    "num_simulations = 50\n",
    "\n",
    "flags = utils.Flags(env=\"rtfm:groups_simple_stationary-v0\")\n",
    "gym_env = utils.create_env(flags)\n",
    "#gym_env = utils.create_env(flags, featurizer=X.Concat([X.Text(), X.ValidMoves(), X.Render()]))\n",
    "featurizer = X.Render()\n",
    "game_simulator = TrueSimulator(gym_env, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "██████\n",
      "█   y█\n",
      "█  n!█\n",
      "█   @█\n",
      "█  ? █\n",
      "██████\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game_simulator.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_root_summary(root, discount):\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    \n",
    "    for action, child in root.children.items():\n",
    "        Q =  child.reward + discount*child.value()\n",
    "        visits = child.visit_count\n",
    "        print(\"Action \", action_dict[action], \": Q-value=%.3f - Visit counts=%d\"%(Q,visits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_v0(\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    render = True,\n",
    "    debug_render=False\n",
    "):\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    for i in range(episode_length):\n",
    "        mcts = MCTS(env, valid_actions, ucb_C, discount, max_actions, render=debug_render)\n",
    "        print(\"\\n\",\"-\"*40)\n",
    "        print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        show_root_summary(root, discount)\n",
    "        print(\"-\"*40)\n",
    "        print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        valid_actions, reward, done = env.step(action, verbose=False)\n",
    "        if render:\n",
    "            env.render()\n",
    "        print(\"Reward received: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_v1(\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    render = True,\n",
    "    debug_render=False\n",
    "):\n",
    "    \"\"\"\n",
    "    W.r.t. version 0 it re-uses the information cached in the child node selected \n",
    "    \"\"\"\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    for i in range(episode_length):\n",
    "        mcts = MCTS(env, valid_actions, ucb_C, discount, max_actions, render=debug_render, root=new_root)\n",
    "        print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        show_root_summary(root, discount)\n",
    "        print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        new_root = mcts.get_subtree(action)\n",
    "        valid_actions, reward, done = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        print(\"Reward received: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██████\n",
      "█    █\n",
      "█? y █\n",
      "█!@  █\n",
      "█n   █\n",
      "██████\n",
      "\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.976 - Visit counts=6\n",
      "Action  Up : Q-value=-0.756 - Visit counts=9\n",
      "Action  Down : Q-value=-0.933 - Visit counts=6\n",
      "Action  Left : Q-value=-1.000 - Visit counts=5\n",
      "Action  Right : Q-value=-0.527 - Visit counts=24\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█? y █\n",
      "█! @ █\n",
      "█n   █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.948 - Visit counts=5\n",
      "Action  Up : Q-value=-0.292 - Visit counts=29\n",
      "Action  Down : Q-value=-0.974 - Visit counts=4\n",
      "Action  Left : Q-value=-0.723 - Visit counts=8\n",
      "Action  Right : Q-value=-0.953 - Visit counts=4\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█? @ █\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.572 - Visit counts=11\n",
      "Action  Up : Q-value=-0.283 - Visit counts=26\n",
      "Action  Down : Q-value=-0.963 - Visit counts=4\n",
      "Action  Left : Q-value=-0.957 - Visit counts=4\n",
      "Action  Right : Q-value=-0.919 - Visit counts=5\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  1 (Up)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█?   █\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.367 - Visit counts=35\n",
      "Action  Down : Q-value=-0.968 - Visit counts=5\n",
      "Action  Left : Q-value=-0.969 - Visit counts=5\n",
      "Action  Right : Q-value=-0.914 - Visit counts=5\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█?   █\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.271 - Visit counts=20\n",
      "Action  Down : Q-value=-0.613 - Visit counts=7\n",
      "Action  Left : Q-value=-0.934 - Visit counts=3\n",
      "Action  Right : Q-value=-0.271 - Visit counts=20\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█   @█\n",
      "█?   █\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.732 - Visit counts=10\n",
      "Action  Down : Q-value=-0.381 - Visit counts=27\n",
      "Action  Left : Q-value=-0.637 - Visit counts=13\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█?  @█\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.538 - Visit counts=10\n",
      "Action  Up : Q-value=-0.727 - Visit counts=9\n",
      "Action  Down : Q-value=-0.573 - Visit counts=16\n",
      "Action  Left : Q-value=-0.555 - Visit counts=15\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  0 (Stay)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█?  @█\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.945 - Visit counts=4\n",
      "Action  Up : Q-value=-0.464 - Visit counts=12\n",
      "Action  Down : Q-value=-0.899 - Visit counts=4\n",
      "Action  Left : Q-value=-0.205 - Visit counts=30\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█? @ █\n",
      "█!   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.233 - Visit counts=16\n",
      "Action  Up : Q-value=-0.886 - Visit counts=3\n",
      "Action  Down : Q-value=-0.176 - Visit counts=19\n",
      "Action  Left : Q-value=-0.995 - Visit counts=3\n",
      "Action  Right : Q-value=-0.410 - Visit counts=9\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█?   █\n",
      "█! @ █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.291 - Visit counts=3\n",
      "Action  Up : Q-value=-0.892 - Visit counts=2\n",
      "Action  Down : Q-value=-0.184 - Visit counts=5\n",
      "Action  Left : Q-value=0.760 - Visit counts=38\n",
      "Action  Right : Q-value=-0.863 - Visit counts=2\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█?   █\n",
      "█!@  █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "\n",
      " ----------------------------------------\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.077 - Visit counts=4\n",
      "Action  Up : Q-value=-0.979 - Visit counts=1\n",
      "Action  Down : Q-value=-0.922 - Visit counts=1\n",
      "Action  Left : Q-value=1.000 - Visit counts=40\n",
      "Action  Right : Q-value=0.057 - Visit counts=4\n",
      "----------------------------------------\n",
      "Tree info:  {'max_tree_depth': 2}\n",
      "Action selected from MCTS:  3 (Left)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█?   █\n",
      "█@   █\n",
      "█n   █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "CPU times: user 12.2 s, sys: 60.9 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = play_episode_v0(\n",
    "    game_simulator,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    debug_render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██████\n",
      "█@ y █\n",
      "█  ! █\n",
      "█  n?█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=-0.890 - Visit counts=5\n",
      "Action  Down : Q-value=-0.358 - Visit counts=32\n",
      "Action  Right : Q-value=-0.083 - Visit counts=13\n",
      "Tree info:  {'max_tree_depth': 5}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█ @y █\n",
      "█  ! █\n",
      "█  n?█\n",
      "█    █\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.160 - Visit counts=12\n",
      "Action  Down : Q-value=-0.962 - Visit counts=2\n",
      "Action  Left : Q-value=-0.972 - Visit counts=2\n",
      "Action  Right : Q-value=0.775 - Visit counts=46\n",
      "Tree info:  {'max_tree_depth': 4}\n",
      "Action selected from MCTS:  4 (Right)\n",
      "\n",
      "██████\n",
      "█  @ █\n",
      "█  ! █\n",
      "█  n?█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Performing MCTS step\n",
      "Action  Stay : Q-value=0.608 - Visit counts=16\n",
      "Action  Down : Q-value=1.000 - Visit counts=72\n",
      "Action  Left : Q-value=-0.922 - Visit counts=1\n",
      "Action  Right : Q-value=0.319 - Visit counts=6\n",
      "Tree info:  {'max_tree_depth': 3}\n",
      "Action selected from MCTS:  2 (Down)\n",
      "\n",
      "██████\n",
      "█    █\n",
      "█  @ █\n",
      "█  n?█\n",
      "█    █\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  1\n",
      "Done:  True\n",
      "CPU times: user 1.68 s, sys: 4.03 ms, total: 1.68 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = play_episode_v1(\n",
    "    game_simulator,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    debug_render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing a value network\n",
    "\n",
    "**TODO**\n",
    "- define a **value target** with which to train the value net\n",
    "- choose on which data to train (whole trajectory? just one trajectory or many? on-policy or with experience replay?)\n",
    "- define training cycle\n",
    "\n",
    "**EXTRA**\n",
    "- use some muti-threaded application, like torch.multiprocessing, to run many episodes in parallel; adapt code from IMPALA\n",
    "\n",
    "**DONE**\n",
    "- **make the simulations faster** (function to set the state of the simulator instead of having to make a deepcopy every time?) \n",
    "- get the **state** to predict the value\n",
    "- define a **ValueNet** architecture that can process frame dictionaries with the batch dimension included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTrueSimulator():\n",
    "    \"\"\"\n",
    "    Returns the full state from the environment step.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, featurizer=None):\n",
    "        self.env = env\n",
    "        self.action_space = len(gym_env.action_space)\n",
    "        self.featurizer = featurizer\n",
    "        \n",
    "    def _process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Extracts from frame a valid_action numpy array containing integers from 0 to self.action_space-1\n",
    "        Adds batch dimension to all values stored inside frame dictionary\n",
    "        \"\"\"\n",
    "        # do this before batch dim is added\n",
    "        valid_moves = frame['valid'].numpy().astype(bool) # boolean mask of shape (action_space)\n",
    "        actions = np.arange(self.action_space)\n",
    "        valid_actions = actions[valid_moves]\n",
    "        \n",
    "        for k in frame.keys():\n",
    "            frame[k] = frame[k].unsqueeze(0)\n",
    "        \n",
    "        return frame, valid_actions\n",
    "    \n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        frame, valid_actions = self._process_frame(frame)\n",
    "        return frame, valid_actions\n",
    "    \n",
    "    def step(self, action, *args, **kwargs):\n",
    "        frame, reward, done, _ = self.env.step(int(action), *args, **kwargs)\n",
    "        frame, valid_actions = self._process_frame(frame)\n",
    "        return frame, valid_actions, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        self.featurizer.featurize(self.env)\n",
    "        \n",
    "    def save_state_dict(self):\n",
    "        return self.env.save_state_dict()\n",
    "        \n",
    "    def load_state_dict(self, d):\n",
    "        self.env.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedDynamicsValueNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 gym_env,\n",
    "                 emb_dim=10,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(gym_env.vocab), emb_dim)\n",
    "        \n",
    "        name_shape = gym_env.observation_space['name']\n",
    "        inv_shape = gym_env.observation_space['inv']\n",
    "        n_channels = (name_shape[2]*name_shape[3]+inv_shape[0])*emb_dim\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(gym_env.observation_space['name'][1])\n",
    "\n",
    "        self.value_mlp = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, frame):\n",
    "        x = self.embedding(frame['name'])\n",
    "        s = x.shape\n",
    "        B, W, H = s[:3]\n",
    "        x = x.reshape(*s[:3],-1).permute(0, 3, 1, 2)\n",
    "        inv = self.embedding(frame['inv'])\n",
    "        inv = inv.reshape(B,-1,1,1)\n",
    "        inv = inv.expand(B,-1,W,H)\n",
    "        z = torch.cat([x,inv], axis=1)\n",
    "        z_conv = self.conv_net(z)\n",
    "        z_flat = self.maxpool(z_conv).view(B,-1)\n",
    "        v = self.value_mlp(z_flat)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_simulator = FullTrueSimulator(gym_env, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, valid_actions = game_simulator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = FixedDynamicsValueNet(gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0268]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value_net(frame)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network and Value Target\n",
    "\n",
    "#### When and why the value network is used?\n",
    "As a default all our information about the value of a state comes from a single rollout of a random policy. \n",
    "In general this is highly inaccurate, since the probability of executing the optimal sequence of decisions from a given state is exponentially decreasing with the number of steps $t$ taken during the rollout (e.g. $~|A|^{-t}$). In our specific case it's more a proxy signal for how likely it is that a good, neutral or bad outcome is obtained from the current state; for instance if we are near the goal state (which is an absorbing state), there is a relevant probability of getting there, especially if many simulations are directed to the node from which the rollout starts.\n",
    "\n",
    "The idea about the value network is that it should learn to predict given some policy what is the EXPECTED value if we made infinite rollouts from there following that policy. This in general will be a BIASED estimate but with LOWER VARIANCE than a Monte Carlo rollout. \n",
    "\n",
    "#### On which states is trained?\n",
    "The value network is trained to predict the value of the root nodes, i.e. the states actually visited along a trajectory, which are also the states on which we possess better informations thanks to the MCTS. \n",
    "\n",
    "#### On which target is trained?\n",
    "This is the key question, since it determines of which policy are we estimating the value. If we were to estimate the value of the rollout policy, this would be guaranteed to be smaller or at best equal to the value of the MCTS policy (that is, the policy that runs $n$ simulations arranged in a tree and selects the action corresponding to the highest Q-value, or the most visited child). \n",
    "\n",
    "I think that the most sensible way to train the network is to use the formula:\n",
    "$$\n",
    "V_{trg}^{\\pi_{mcts}}(s_t) = \\sum_{k=0}^{n-1}\\gamma^k r_{t+k} + \\gamma^n \\hat{V}(s_t+n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.frame = None\n",
    "\n",
    "    def expand(self, frame, valid_actions, reward, done, simulator):\n",
    "        self.expanded = True\n",
    "        vprint(\"Valid actions as child: \", valid_actions)\n",
    "        vprint(\"Terminal node: \", done)\n",
    "        self.frame = frame\n",
    "        self.reward = reward\n",
    "        self.terminal = done\n",
    "        self.valid_actions = valid_actions\n",
    "        if not done:\n",
    "            for action in valid_actions:\n",
    "                self.children[action] = ValueNode()\n",
    "        self.simulator_dict = simulator.save_state_dict()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueMCTS(MCTS):\n",
    "    def __init__(self, \n",
    "                 root_frame,\n",
    "                 simulator,\n",
    "                 valid_actions,\n",
    "                 ucb_c,\n",
    "                 discount,\n",
    "                 max_actions,\n",
    "                 value_net,\n",
    "                 root=None,\n",
    "                 render=False):\n",
    "        \n",
    "        super().__init__(simulator,\n",
    "                     valid_actions,\n",
    "                     ucb_c,\n",
    "                     discount,\n",
    "                     max_actions,\n",
    "                     root,\n",
    "                     render)\n",
    "        self.value_net = value_net\n",
    "        self.root_frame = root_frame\n",
    "        \n",
    "    def run(self, num_simulations):\n",
    "        \"\"\"\n",
    "        Runs num_simulations searches starting from the root node corresponding to the internal\n",
    "        state of the simulator given during initialization.\n",
    "        Returns the root node and an extra_info dictionary\n",
    "        \"\"\"\n",
    "        if self.root is None:\n",
    "            self.root = ValueNode()\n",
    "            self.root.expand(\n",
    "                self.root_frame,\n",
    "                self.valid_actions,\n",
    "                0, # reward to get to root\n",
    "                False, # terminal node\n",
    "                self.simulator # state of the simulator at the root node \n",
    "            )\n",
    "            # not sure about this\n",
    "            self.root.visit_count += 1\n",
    "        \n",
    "        max_tree_depth = 0\n",
    "        root = self.root\n",
    "        for n in range(num_simulations):\n",
    "            ### Start of a simulation/search ###\n",
    "            vprint(\"\\nSimulation %d started.\"%(n+1))\n",
    "            node = root\n",
    "            # make sure that the simulator internal state is reset to the original one\n",
    "            self.simulator.load_state_dict(root.simulator_dict)\n",
    "            search_path = [node]\n",
    "            current_tree_depth = 0\n",
    "            if self.render:\n",
    "                node.render(self.simulator)\n",
    "            ### Selection phase until leaf node is reached ###\n",
    "            while node.expanded or (current_tree_depth<self.max_actions):\n",
    "                current_tree_depth += 1\n",
    "                action, node = self.select(node)\n",
    "                if self.render and node.expanded:\n",
    "                    node.render(self.simulator)\n",
    "                vprint(\"Current tree depth: \", current_tree_depth)\n",
    "                vprint(\"Action selected: \", action)\n",
    "                vprint(\"Child node terminal: \", node.terminal)\n",
    "                vprint(\"Child node expanded: \", node.expanded)\n",
    "                if node.expanded or node.terminal:\n",
    "                    search_path.append(node)\n",
    "                    if node.terminal:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            ### Expansion of leaf node (if not terminal)###\n",
    "            vprint(\"Expansion phase started\")\n",
    "            if not node.terminal:\n",
    "                parent = search_path[-1] # last expanded node on the search path\n",
    "                node = self.expand(node, parent, action)\n",
    "                if self.render:\n",
    "                    node.render(self.simulator)\n",
    "                search_path.append(node)\n",
    "            \n",
    "            ### Simulation phase for self.max_actions - current_tree_depth steps ###\n",
    "            vprint(\"Value prediction phase started\")\n",
    "            value = self.predict(node)\n",
    "            vprint(\"Predicted value: \", value)\n",
    "            \n",
    "            ### Backpropagation of the leaf node value along the seach_path ###\n",
    "            vprint(\"Backpropagation phase started\")\n",
    "            self.backprop(search_path, value)\n",
    "        \n",
    "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "            vprint(\"Simulation %d done.\"%(n+1))\n",
    "        extra_info = {\n",
    "            \"max_tree_depth\": max_tree_depth\n",
    "        }\n",
    "        # just a check to see if root works as a shallow copy of self.root\n",
    "        assert root.visit_count == self.root.visit_count, \"self.root not updated during search\"\n",
    "        \n",
    "        # make sure that the simulator internal state is reset to the original one\n",
    "        self.simulator.load_state_dict(root.simulator_dict)\n",
    "        return root, extra_info\n",
    "    \n",
    "    def expand(self, node, parent, action):\n",
    "        \"\"\"\n",
    "        Expand the node obtained by taking the given action from the parent node \n",
    "        \"\"\"\n",
    "        simulator = parent.get_simulator(self.simulator) # get a deepcopy of the simulator with the parent's state stored\n",
    "        frame, valid_actions, reward, done = simulator.step(action) # this also updates the simulator's internal state\n",
    "        vprint(\"reward: \", reward)\n",
    "        vprint(\"done: \", done)\n",
    "        node.expand(frame, valid_actions, reward, done, simulator)\n",
    "        return node\n",
    "    \n",
    "    def predict(self, node):\n",
    "        with torch.no_grad():\n",
    "            value = self.value_net(node.frame)\n",
    "        return value.item()\n",
    "    \n",
    "    def simulate_and_predict(self, node, current_depth, n_steps=5):\n",
    "        \"\"\"\n",
    "        Simulate a rollout with a random policy starting from the input node\n",
    "        until the end of the episode or self.max_actions are reached \n",
    "        (also considering the current depth of the input node from the root)\n",
    "        or at most n_steps before calling the value_net to approximate the rest of the trajectory.\n",
    "        \"\"\"\n",
    "        if not node.terminal:\n",
    "            simulator = node.get_simulator(self.simulator)\n",
    "            valid_actions = node.valid_actions\n",
    "            steps = min(self.max_actions - current_depth, n_steps)\n",
    "            cum_discounted_reward = 0\n",
    "            for i in range(steps):\n",
    "                action = np.random.choice(valid_actions)\n",
    "                frame, valid_actions, reward, done = simulator.step(action)\n",
    "                cum_discounted_reward += (self.discount**i)*reward\n",
    "                if done:\n",
    "                    break\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    bootstrap_value = self.value_net(frame).item()\n",
    "                cum_discounted_reward += (self.discount**(i+1))*bootstrap_value\n",
    "        else:\n",
    "            cum_discounted_reward = 0\n",
    "        return cum_discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_value_net(\n",
    "    value_net,\n",
    "    env,\n",
    "    episode_length,\n",
    "    ucb_C,\n",
    "    discount,\n",
    "    max_actions,\n",
    "    num_simulations,\n",
    "    render = False,\n",
    "    debug_render=False\n",
    "):\n",
    "    \"\"\"\n",
    "    W.r.t. version 0 it re-uses the information cached in the child node selected \n",
    "    \"\"\"\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    frame, valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    # variables used for training of value net\n",
    "    frame_lst = [frame]\n",
    "    reward_lst = []\n",
    "    done_lst = []\n",
    "    for i in range(episode_length):\n",
    "        mcts = ValueMCTS(frame, \n",
    "                         env, \n",
    "                         valid_actions, \n",
    "                         ucb_C, \n",
    "                         discount, \n",
    "                         max_actions, \n",
    "                         value_net,\n",
    "                         render=debug_render, \n",
    "                         root=new_root\n",
    "                        )\n",
    "        #print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        #show_root_summary(root, discount)\n",
    "        #print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        #print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        new_root = mcts.get_subtree(action)\n",
    "        frame, valid_actions, reward, done = env.step(action)\n",
    "        \n",
    "        frame_lst.append(frame)\n",
    "        reward_lst.append(reward)\n",
    "        done_lst.append(done)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        #print(\"Reward received: \", reward)\n",
    "        #print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, frame_lst, reward_lst, done_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_rollout_value_net(\n",
    "        value_net,\n",
    "        env,\n",
    "        episode_length,\n",
    "        ucb_C,\n",
    "        discount,\n",
    "        max_actions,\n",
    "        num_simulations,\n",
    "        render = False,\n",
    "        debug_render=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    W.r.t. version 0 it re-uses the information cached in the child node selected \n",
    "    \"\"\"\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    frame, valid_actions = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    new_root = None\n",
    "    # variables used for training of value net\n",
    "    frame_lst = [frame]\n",
    "    reward_lst = []\n",
    "    done_lst = []\n",
    "    for i in range(episode_length):\n",
    "        mcts = ValueMCTS(frame, \n",
    "                         env, \n",
    "                         valid_actions, \n",
    "                         ucb_C, \n",
    "                         discount, \n",
    "                         max_actions, \n",
    "                         value_net,\n",
    "                         render=debug_render, \n",
    "                         root=new_root\n",
    "                        )\n",
    "        #print(\"Performing MCTS step\")\n",
    "        root, info = mcts.run(num_simulations)\n",
    "        #show_root_summary(root, discount)\n",
    "        #print(\"Tree info: \", info)\n",
    "        action = root.best_action(discount)\n",
    "        #print(\"Action selected from MCTS: \", action, \"({})\".format(action_dict[action]))\n",
    "        new_root = mcts.get_subtree(action)\n",
    "        frame, valid_actions, reward, done = env.step(action)\n",
    "        \n",
    "        frame_lst.append(frame)\n",
    "        reward_lst.append(reward)\n",
    "        done_lst.append(done)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        #print(\"Reward received: \", reward)\n",
    "        #print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            frame, valid_actions = env.reset()\n",
    "            done = False\n",
    "            new_root = None\n",
    "            \n",
    "    return total_reward, frame_lst, reward_lst, done_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters ###\n",
    "ucb_C = 1.0\n",
    "discount = 0.997\n",
    "episode_length = 100\n",
    "max_actions = 100\n",
    "num_simulations = 50\n",
    "\n",
    "flags = utils.Flags(env=\"rtfm:groups_simple_stationary-v0\")\n",
    "gym_env = utils.create_env(flags)\n",
    "#gym_env = utils.create_env(flags, featurizer=X.Concat([X.Text(), X.ValidMoves(), X.Render()]))\n",
    "featurizer = X.Render()\n",
    "game_simulator = FullTrueSimulator(gym_env, featurizer)\n",
    "value_net = FixedDynamicsValueNet(gym_env)\n",
    "optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_reward, frame_lst, reward_lst, done_lst = play_rollout_value_net(value_net,\n",
    "                                                                        game_simulator,\n",
    "                                                                        episode_length,\n",
    "                                                                        ucb_C,\n",
    "                                                                        discount,\n",
    "                                                                        max_actions,\n",
    "                                                                        num_simulations,\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, discount, dones):\n",
    "    cum_disc_rewards = []\n",
    "    cum_r = 0\n",
    "    for i,r in enumerate(reversed(rewards)):\n",
    "        not_done = 1 - dones[-i]\n",
    "        cum_r = not_done*discount*cum_r + r\n",
    "        cum_disc_rewards.append (cum_r)\n",
    "    cum_disc_rewards = torch.tensor(cum_disc_rewards[::-1])\n",
    "    return cum_disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_episode(frame_lst, reward_lst, done_lst, value_net, discount):\n",
    "    \"\"\"\n",
    "    Single episode of varying length and no restarts in between. \n",
    "    batch_size is actually the temporal dimension.\n",
    "    Keep it simple just to see if the training procedure works.\n",
    "    \"\"\"\n",
    "    batch_size = len(reward_lst)\n",
    "    frames = {}\n",
    "    for k in frame_lst[0].keys():\n",
    "        k_value_lst = []\n",
    "        for b in range(batch_size):\n",
    "            k_value_lst.append(frame_lst[b][k])\n",
    "        k_value_lst = torch.cat(k_value_lst, axis=0)\n",
    "        frames[k] = k_value_lst\n",
    "    rewards = torch.tensor(reward_lst).float()\n",
    "    dones = torch.tensor(done_lst).float()\n",
    "    \n",
    "    values = value_net(frames) # leave out last state's value\n",
    "    targets = get_cumulative_rewards(rewards, discount, dones)\n",
    "    return values, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, targets = batch_episode(frame_lst, reward_lst, done_lst, value_net, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1 - Total reward 3\n",
      "Values:  tensor([[0.3245],\n",
      "        [0.3514],\n",
      "        [0.3487],\n",
      "        [0.3475],\n",
      "        [0.3475],\n",
      "        [0.3185],\n",
      "        [0.3463],\n",
      "        [0.3446],\n",
      "        [0.3462],\n",
      "        [0.3454],\n",
      "        [0.3247],\n",
      "        [0.3247],\n",
      "        [0.3453],\n",
      "        [0.3455],\n",
      "        [0.3466],\n",
      "        [0.3466],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3486],\n",
      "        [0.3486],\n",
      "        [0.3457],\n",
      "        [0.3449],\n",
      "        [0.3457]], grad_fn=<AddmmBackward>)\n",
      "Loss: 0.129\n",
      "\n",
      "Episode 2 - Total reward 0\n",
      "Values:  tensor([[0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1936],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1936],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1950],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947],\n",
      "        [0.1947]], grad_fn=<AddmmBackward>)\n",
      "Loss: 0.038\n",
      "\n",
      "Episode 3 - Total reward 0\n",
      "Values:  tensor([[0.0539],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680],\n",
      "        [0.0680]], grad_fn=<AddmmBackward>)\n",
      "Loss: 0.005\n",
      "\n",
      "Episode 4 - Total reward 5\n",
      "Values:  tensor([[-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0340],\n",
      "        [-0.0330],\n",
      "        [-0.0332],\n",
      "        [ 0.0256],\n",
      "        [ 0.0258],\n",
      "        [ 0.0259],\n",
      "        [-0.0337],\n",
      "        [ 0.0223],\n",
      "        [ 0.0219],\n",
      "        [ 0.0228],\n",
      "        [ 0.0226],\n",
      "        [-0.0336],\n",
      "        [ 0.0223],\n",
      "        [ 0.0230],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0353],\n",
      "        [-0.0364],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0349],\n",
      "        [-0.0349],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0352],\n",
      "        [-0.0351],\n",
      "        [ 0.0240],\n",
      "        [ 0.0233],\n",
      "        [ 0.0227],\n",
      "        [-0.0364],\n",
      "        [-0.0149],\n",
      "        [-0.0152],\n",
      "        [-0.0152],\n",
      "        [-0.0136],\n",
      "        [-0.0145],\n",
      "        [ 0.0232],\n",
      "        [ 0.0215],\n",
      "        [ 0.0259],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0226],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0226],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237],\n",
      "        [ 0.0237]], grad_fn=<AddmmBackward>)\n",
      "Loss: 0.049\n",
      "\n",
      "Episode 5 - Total reward 1\n",
      "Values:  tensor([[-0.0531],\n",
      "        [-0.0526],\n",
      "        [ 0.0011],\n",
      "        [ 0.0003],\n",
      "        [-0.0518],\n",
      "        [-0.0524],\n",
      "        [-0.0518],\n",
      "        [-0.0524],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0509],\n",
      "        [-0.0525],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0525],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0512],\n",
      "        [-0.0509],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0512],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0525],\n",
      "        [-0.0509],\n",
      "        [-0.0512]], grad_fn=<AddmmBackward>)\n",
      "Loss: 0.013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-07d91036d4bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                                         \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                                         \u001b[0mmax_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                                         \u001b[0mnum_simulations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                                                                         )\n\u001b[1;32m     14\u001b[0m     \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c39237e55cc3>\u001b[0m in \u001b[0;36mplay_rollout_value_net\u001b[0;34m(value_net, env, episode_length, ucb_C, discount, max_actions, num_simulations, render, debug_render)\u001b[0m\n\u001b[1;32m     42\u001b[0m                         )\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#print(\"Performing MCTS step\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#show_root_summary(root, discount)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#print(\"Tree info: \", info)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a4dfdb856769>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_simulations)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# last expanded node on the search path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a4dfdb856769>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, node, parent, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mvprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mvprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4dcb4c9cdfc0>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, frame, valid_actions, reward, done, simulator)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulator_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b004c01be8b0>\u001b[0m in \u001b[0;36msave_state_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Aalto_workspace/Research/RTFM/rtfm/tasks/groups.py\u001b[0m in \u001b[0;36msave_state_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'configs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/copyreg.py\u001b[0m in \u001b[0;36m__newobj__\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__newobj__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__newobj_ex__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_updates = 100\n",
    "total_rewards = []\n",
    "episode_lengths = []\n",
    "losses = []\n",
    "for i in range(n_updates):\n",
    "    total_reward, frame_lst, reward_lst, done_lst = play_rollout_value_net(value_net,\n",
    "                                                                        game_simulator,\n",
    "                                                                        episode_length,\n",
    "                                                                        ucb_C,\n",
    "                                                                        discount,\n",
    "                                                                        max_actions,\n",
    "                                                                        num_simulations,\n",
    "                                                                        )\n",
    "    total_rewards.append(total_reward)\n",
    "    episode_lengths.append(len(reward_lst))\n",
    "    values, targets = batch_episode(frame_lst, reward_lst, done_lst, value_net, discount)\n",
    "    print(\"\\nEpisode %d - Total reward %d\"%(i+1, total_reward))\n",
    "    print(\"Values: \", values)\n",
    "    loss = loss_fn(values.squeeze(), targets)\n",
    "    print(\"Loss: %.3f\"%loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the value network predictions and the MC return, the network gets stuck either by playing Stay or two complementary actions that as a result cancel each other (e.g. Left+Right).\n",
    "\n",
    "Second option tried: during the simulation/prediction of a leaf node, take at most n_steps of random rollouts and then bootstrap the end of the trajectory with the value network. This helps to mix the unbiased information from the true simulator and the low variance estimate of the value network.\n",
    "\n",
    "Still it doesn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(n_updates), total_rewards)\n",
    "plt.xlabel(\"Number of optimizer steps\")\n",
    "plt.ylabel(\"Total episode reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(n_updates), losses)\n",
    "plt.xlabel(\"Number of optimizer steps\")\n",
    "plt.ylabel(\"Value net MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the value network without the MCTS\n",
    "\n",
    "Idea:\n",
    "$$\n",
    "\\pi(s) =  \\arg \\underset{a}{\\max} r(s,a) + \\gamma V(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi:\n",
    "    def __init__(self,\n",
    "                 simulator,\n",
    "                 value_net,\n",
    "                 discount\n",
    "                ):\n",
    "        self.simulator = simulator\n",
    "        self.value_net = value_net\n",
    "        self.discount = discount\n",
    "        \n",
    "        \n",
    "    def get_action(self, valid_actions, simulator_state):\n",
    "        Q_values = []\n",
    "        for a in valid_actions:\n",
    "            self.simulator.load_state_dict(copy.deepcopy(simulator_state))\n",
    "            frame, valid_actions, reward, done = self.simulator.step(a)\n",
    "            with torch.no_grad():\n",
    "                value = self.value_net(frame).item()\n",
    "            Q = reward + self.discount*value\n",
    "            Q_values.append(Q)\n",
    "        print(\"Q values: \", Q_values)\n",
    "        best_id = np.argmax(Q_values)\n",
    "        print(\"Best id: \", best_id)\n",
    "        best_action = valid_actions[best_id]\n",
    "        print(\"best action: \", best_action)\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_value_policy(value_net, env, episode_length, discount, render = False):\n",
    "    action_dict = {\n",
    "        0:\"Stay\",\n",
    "        1:\"Up\",\n",
    "        2:\"Down\",\n",
    "        3:\"Left\",\n",
    "        4:\"Right\"\n",
    "    }\n",
    "    \n",
    "    policy = Pi(env, value_net, discount)\n",
    "    frame, valid_actions = env.reset()\n",
    "    print(\"valid actions: \", valid_actions)\n",
    "    simulator_state = env.save_state_dict()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for i in range(episode_length):\n",
    "        a = policy.get_action(valid_actions, simulator_state)\n",
    "        print(\"Action selected from value policy: \", a, \"({})\".format(action_dict[a]))\n",
    "        # reset internal state of the env\n",
    "        env.load_state_dict(simulator_state)\n",
    "        #if render:\n",
    "        #    env.render()\n",
    "        frame, valid_actions, reward, done = env.step(a)\n",
    "        print(\"valid actions: \", valid_actions)\n",
    "        simulator_state = env.save_state_dict()\n",
    "        if render:\n",
    "            env.render()\n",
    "        print(\"Reward received: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid actions:  [0 1 2 3]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ y @█\n",
      "█   n█\n",
      "██████\n",
      "\n",
      "Q values:  [0.6126198204755783, 0.612559324979782, 0.6020974074602127, 0.6132143164873123]\n",
      "Best id:  3\n",
      "best action:  3\n",
      "Action selected from value policy:  3 (Left)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ y@ █\n",
      "█   n█\n",
      "██████\n",
      "\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.6132143164873123, -0.38310425329208375, 0.6131241675019264, 0.7595323594808578, 0.6126198204755783]\n",
      "Best id:  3\n",
      "best action:  3\n",
      "Action selected from value policy:  3 (Left)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n",
      "Q values:  [0.7595323594808578, 0.7595030031204224, 0.7592893078327179, 0.7588241224288941, 0.7570907897949218]\n",
      "Best id:  0\n",
      "best action:  0\n",
      "Action selected from value policy:  0 (Stay)\n",
      "valid actions:  [0 1 2 3 4]\n",
      "\n",
      "██████\n",
      "█   !█\n",
      "█  ? █\n",
      "█ @  █\n",
      "█   n█\n",
      "██████\n",
      "blessed sword\n",
      "Reward received:  0\n",
      "Done:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_value_policy(value_net, game_simulator, episode_length, discount, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
